{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "############################################\n",
    "# 1. GLOBAL SETTINGS\n",
    "############################################\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "features = [\"Price\", \"Open\", \"High\", \"Low\"]\n",
    "WINDOW_SIZE = 35\n",
    "NUM_EPOCHS = 2000  # or 1000 if you prefer\n",
    "BEST_MODELS_DIR = \".\"\n",
    "\n",
    "############################################\n",
    "# 2. Basic Data Preprocessing\n",
    "############################################\n",
    "def load_and_scale_data(train_csv, complete_csv):\n",
    "    # Load partial CSV\n",
    "    df_trainval = pd.read_csv(train_csv)\n",
    "    df_trainval[\"Date\"] = pd.to_datetime(df_trainval[\"Date\"], errors=\"coerce\")\n",
    "    for col in [\"Vol.\", \"Change %\"]:\n",
    "        if col in df_trainval.columns:\n",
    "            df_trainval.drop(columns=[col], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    for c in features:\n",
    "        if c not in df_trainval.columns:\n",
    "            df_trainval[c] = np.nan\n",
    "        else:\n",
    "            df_trainval[c] = df_trainval[c].astype(str).str.replace(\",\", \"\", regex=True)\n",
    "            df_trainval[c] = pd.to_numeric(df_trainval[c], errors=\"coerce\")\n",
    "\n",
    "    df_trainval.sort_values(\"Date\", inplace=True)\n",
    "    df_trainval.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Basic cutoff\n",
    "    train_cutoff = pd.to_datetime(\"2024-07-01\")\n",
    "    val_cutoff   = pd.to_datetime(\"2024-11-30\")\n",
    "    test_cutoff  = pd.to_datetime(\"2024-12-01\")\n",
    "\n",
    "    df_train = df_trainval[df_trainval[\"Date\"] < train_cutoff].copy()\n",
    "    df_val   = df_trainval[(df_trainval[\"Date\"] >= train_cutoff) & (df_trainval[\"Date\"] <= val_cutoff)].copy()\n",
    "    df_test  = df_trainval[df_trainval[\"Date\"] >= test_cutoff].copy()\n",
    "\n",
    "    # Fit scaler on train only\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    df_train_nonan = df_train.dropna(subset=features)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df_train_nonan[features])\n",
    "\n",
    "    # transform\n",
    "    df_train_scaled = df_train.copy()\n",
    "    if not df_train_nonan.empty:\n",
    "        df_train_scaled.loc[df_train_nonan.index, features] = scaler.transform(df_train_nonan[features])\n",
    "\n",
    "    df_val_scaled = df_val.copy()\n",
    "    val_no_nan = df_val_scaled.dropna(subset=features)\n",
    "    if not val_no_nan.empty:\n",
    "        df_val_scaled.loc[val_no_nan.index, features] = scaler.transform(val_no_nan[features])\n",
    "\n",
    "    df_test_scaled = df_test.copy()\n",
    "    # fill missing with train set min\n",
    "    train_mins = df_train_nonan[features].min()\n",
    "    df_test_filled = df_test_scaled[features].fillna(train_mins)\n",
    "    df_test_scaled.loc[:, features] = scaler.transform(df_test_filled)\n",
    "\n",
    "    df_scaled = pd.concat([df_train_scaled, df_val_scaled, df_test_scaled], ignore_index=True)\n",
    "\n",
    "    # Load complete CSV for final rolling\n",
    "    df_complete = pd.read_csv(complete_csv)\n",
    "    df_complete[\"Date\"] = pd.to_datetime(df_complete[\"Date\"], errors=\"coerce\")\n",
    "    for c in [\"Vol.\", \"Change %\"]:\n",
    "        if c in df_complete.columns:\n",
    "            df_complete.drop(columns=[c], inplace=True)\n",
    "    for f in features:\n",
    "        df_complete[f] = df_complete[f].astype(str).str.replace(\",\", \"\", regex=True).astype(float)\n",
    "    df_complete.sort_values(\"Date\", inplace=True)\n",
    "    df_complete.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_scaled, df_complete, scaler\n",
    "\n",
    "############################################\n",
    "# 3. Utility: Sequence & Slices\n",
    "############################################\n",
    "def make_sequences(df_subset, window=35):\n",
    "    arr = df_subset[features].values\n",
    "    dts = df_subset[\"Date\"].values\n",
    "    X_list, y_list, date_list = [], [], []\n",
    "    for i in range(window, len(arr)):\n",
    "        X_window = arr[i - window : i]\n",
    "        y_target = arr[i]\n",
    "        X_list.append(X_window)\n",
    "        y_list.append(y_target)\n",
    "        date_list.append(dts[i])\n",
    "    return np.array(X_list), np.array(y_list), np.array(date_list)\n",
    "\n",
    "############################################\n",
    "# 4. Model Definitions\n",
    "############################################\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_type=\"LSTM\", window_width=35):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "\n",
    "        if model_type == \"CNN\":\n",
    "            self.conv1 = nn.Conv1d(4, 64, 3)\n",
    "            self.conv2 = nn.Conv1d(64, 128, 3)\n",
    "            with torch.no_grad():\n",
    "                dummy = torch.zeros(1, 4, window_width)\n",
    "                outdummy = self.conv2(F.relu(self.conv1(dummy)))\n",
    "                conv_output_size = outdummy.shape[1]*outdummy.shape[2]\n",
    "            self.fc = nn.Linear(conv_output_size, 4)\n",
    "\n",
    "        elif model_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(4, 128, num_layers=2, batch_first=True, dropout=0.05)\n",
    "            self.fc = nn.Linear(128, 4)\n",
    "\n",
    "        elif model_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(4, 128, num_layers=2, batch_first=True, dropout=0.05)\n",
    "            self.fc = nn.Linear(128, 4)\n",
    "\n",
    "        elif model_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(4, 128, num_layers=2, nonlinearity=\"relu\", batch_first=True, dropout=0.05)\n",
    "            self.fc = nn.Linear(128, 4)\n",
    "\n",
    "        elif model_type == \"EnhancedLSTM\":\n",
    "            self.rnn = nn.LSTM(4, 128, num_layers=3, batch_first=True, dropout=0.1)\n",
    "            self.bn  = nn.BatchNorm1d(128)\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.fc = nn.Linear(128, 4)\n",
    "\n",
    "        elif model_type == \"Transformer\":\n",
    "            self.input_linear = nn.Linear(4, 128)\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8, dropout=0.05)\n",
    "            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
    "            self.fc = nn.Linear(128,4)\n",
    "\n",
    "        elif model_type in [\"N-BEATS\",\"N-HITS\"]:\n",
    "            self.input_size= window_width*4\n",
    "            self.blocks= nn.ModuleList([nn.Sequential(\n",
    "                nn.Linear(self.input_size,128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128,128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128,4)\n",
    "            ) for _ in range(3)])\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid model_type: {model_type}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.model_type == \"CNN\":\n",
    "            x = x.permute(0,2,1)\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return self.fc(x)\n",
    "\n",
    "        elif self.model_type in [\"LSTM\",\"GRU\",\"RNN\"]:\n",
    "            out, _ = self.rnn(x)\n",
    "            out = out[:,-1,:]\n",
    "            return self.fc(out)\n",
    "\n",
    "        elif self.model_type == \"EnhancedLSTM\":\n",
    "            out, _= self.rnn(x)\n",
    "            out = out[:,-1,:]\n",
    "            out = self.bn(out)\n",
    "            out = self.dropout(out)\n",
    "            return self.fc(out)\n",
    "\n",
    "        elif self.model_type == \"Transformer\":\n",
    "            x = self.input_linear(x)\n",
    "            x = x.permute(1,0,2)\n",
    "            x = self.transformer_encoder(x)\n",
    "            x = x[-1,:,:]\n",
    "            return self.fc(x)\n",
    "\n",
    "        elif self.model_type in [\"N-BEATS\",\"N-HITS\"]:\n",
    "            xflat= x.reshape(x.size(0), -1)\n",
    "            forecast=0\n",
    "            for block in self.blocks:\n",
    "                forecast += block(xflat)\n",
    "            return forecast\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type\")\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=100, min_delta=0.0):\n",
    "        self.patience= patience\n",
    "        self.min_delta= min_delta\n",
    "        self.counter= 0\n",
    "        self.best_loss= None\n",
    "        self.early_stop= False\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss> self.best_loss - self.min_delta:\n",
    "            self.counter+=1\n",
    "            if self.counter>= self.patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                self.early_stop= True\n",
    "        else:\n",
    "            self.best_loss= val_loss\n",
    "            self.counter=0\n",
    "\n",
    "def inverse_transform_cols(arr4, scaler):\n",
    "    return scaler.inverse_transform(arr4)\n",
    "\n",
    "############################################\n",
    "# 5. Validation Loss\n",
    "############################################\n",
    "def half_blind_validation_loss(model, df_scaled, val_start, val_end, window=35, scaler=None):\n",
    "    \"\"\"\n",
    "    MSE over the range [val_start..val_end],\n",
    "    using the last 'window' days for each day,\n",
    "    not revealing future data.\n",
    "    \"\"\"\n",
    "    import torch.nn as nn\n",
    "    criterion= nn.MSELoss()\n",
    "    df_work= df_scaled.copy()\n",
    "    df_work.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "    val_dates= np.sort(df_work.loc[(df_work[\"Date\"]>= val_start)&(df_work[\"Date\"]<= val_end),\"Date\"].unique())\n",
    "    losses=[]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for d in val_dates:\n",
    "            idx= df_work.index[df_work[\"Date\"]== d]\n",
    "            if len(idx)==0:\n",
    "                continue\n",
    "            idx= idx[0]\n",
    "            st= idx-window\n",
    "            if st<0:\n",
    "                continue\n",
    "            window_df= df_work.iloc[st:idx][features].values\n",
    "            if np.isnan(window_df).any():\n",
    "                continue\n",
    "            X_ = torch.tensor(window_df, dtype=torch.float).unsqueeze(0).to(device)\n",
    "            pred_= model(X_).cpu()[0]\n",
    "            realvals= df_work.loc[idx, features].values.astype(float)\n",
    "            if np.isnan(realvals).any():\n",
    "                continue\n",
    "            lossval= criterion(pred_, torch.tensor(realvals).float())\n",
    "            losses.append(lossval.item())\n",
    "\n",
    "    if not losses:\n",
    "        return 999999.0\n",
    "    return np.mean(losses)\n",
    "\n",
    "############################################\n",
    "# 6. \"Train or Load\" PyTorch & sklearn\n",
    "############################################\n",
    "class SklearnWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model= model\n",
    "    def forward(self, x):\n",
    "        arr= x.cpu().numpy().reshape(1,-1)\n",
    "        pred= self.model.predict(arr)\n",
    "        return torch.from_numpy(pred).float().to(x.device)\n",
    "    def eval(self):\n",
    "        pass\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "def train_or_load_torch(model_type, window_size, df_scaled_in, train_cutoff_in, val_cutoff_in,\n",
    "                        model_save_name=\"best_model.pt\", num_epochs=1500, scaler=None):\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "\n",
    "    if os.path.exists(model_save_name):\n",
    "        print(f\"[PyTorch] Loading existing {model_type} from {model_save_name}\")\n",
    "        net= BaseModel(model_type, window_size).to(device)\n",
    "        net.load_state_dict(torch.load(model_save_name, map_location=device))\n",
    "        net.eval()\n",
    "        return net\n",
    "    else:\n",
    "        print(f\"[PyTorch] Training {model_type} from scratch => {model_save_name}\")\n",
    "        df_local= df_scaled_in.copy()\n",
    "        df_local.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "        df_train_local= df_local[df_local[\"Date\"]< train_cutoff_in].copy()\n",
    "        df_val_local=   df_local[(df_local[\"Date\"]>= train_cutoff_in)&(df_local[\"Date\"]<= val_cutoff_in)].copy()\n",
    "        # build sequences\n",
    "        df_train_nonan= df_train_local.dropna(subset=features)\n",
    "        if df_train_nonan.empty:\n",
    "            raise ValueError(\"No data in rolling window for PyTorch training\")\n",
    "        X_,y_,_= make_sequences(df_train_nonan, window_size)\n",
    "        X_t= torch.tensor(X_, dtype=torch.float)\n",
    "        y_t= torch.tensor(y_, dtype=torch.float)\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "        ds= TensorDataset(X_t,y_t)\n",
    "        dl= DataLoader(ds, batch_size=len(ds), shuffle=False)\n",
    "\n",
    "        net= BaseModel(model_type, window_size).to(device)\n",
    "        criterion= nn.MSELoss()\n",
    "        optimizer= optim.AdamW(net.parameters(), lr=0.002, weight_decay=0)\n",
    "\n",
    "        stopper= EarlyStopping(patience=100)\n",
    "        best_loss= 9999999.0\n",
    "        for ep in range(num_epochs):\n",
    "            net.train()\n",
    "            for Xb, yb in dl:\n",
    "                Xb, yb= Xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out= net(Xb)\n",
    "                loss= criterion(out,yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # val loss\n",
    "            val_loss= half_blind_validation_loss(net, df_local, train_cutoff_in, val_cutoff_in, window=window_size)\n",
    "            if ep>0 and ep%200==0:\n",
    "                print(f\"Epoch {ep}: val_loss={val_loss:.6f}\")\n",
    "\n",
    "            stopper(val_loss)\n",
    "            if val_loss< best_loss:\n",
    "                best_loss= val_loss\n",
    "                torch.save(net.state_dict(), model_save_name)\n",
    "            if stopper.early_stop:\n",
    "                print(f\"Early stop at epoch {ep}\")\n",
    "                break\n",
    "\n",
    "        net.load_state_dict(torch.load(model_save_name, map_location=device))\n",
    "        net.eval()\n",
    "        return net\n",
    "\n",
    "def train_or_load_sklearn(model_type, df_scaled_in, train_cutoff_in, val_cutoff_in,\n",
    "                          model_save_name=\"best_sklearn.pkl\", window_size=35, scaler=None):\n",
    "    if os.path.exists(model_save_name):\n",
    "        print(f\"[sklearn] Loading {model_type} => {model_save_name}\")\n",
    "        loaded= joblib.load(model_save_name)\n",
    "        return SklearnWrapper(loaded)\n",
    "    else:\n",
    "        print(f\"[sklearn] Training {model_type} => {model_save_name}\")\n",
    "        df_work= df_scaled_in.copy()\n",
    "        df_work.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "        df_train_local= df_work[df_work[\"Date\"]< train_cutoff_in].copy()\n",
    "        df_val_local=   df_work[(df_work[\"Date\"]>= train_cutoff_in)&(df_work[\"Date\"]<= val_cutoff_in)].copy()\n",
    "        df_train_nonan= df_train_local.dropna(subset=features)\n",
    "        if df_train_nonan.empty:\n",
    "            raise ValueError(\"No data in rolling window for sklearn train\")\n",
    "\n",
    "        X_, y_, _= make_sequences(df_train_nonan, window_size)\n",
    "        Xf= X_.reshape(X_.shape[0], window_size*4)\n",
    "\n",
    "        if model_type==\"SVM\":\n",
    "            base= SVR(kernel=\"rbf\",C=10, epsilon=0.01)\n",
    "        elif model_type==\"GPR\":\n",
    "            base= GaussianProcessRegressor(alpha=1e-2)\n",
    "        elif model_type==\"Boost\":\n",
    "            base= GradientBoostingRegressor(n_estimators=200, max_depth=3)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model_type for sklearn wrapper\")\n",
    "\n",
    "        from sklearn.multioutput import MultiOutputRegressor\n",
    "        mo= MultiOutputRegressor(base)\n",
    "        mo.fit(Xf,y_)\n",
    "        joblib.dump(mo, model_save_name)\n",
    "        return SklearnWrapper(mo)\n",
    "\n",
    "############################################\n",
    "# 7. Full-Blind Forecast with Noise\n",
    "############################################\n",
    "def get_latest_window(df_actual, current_date, window=35, scaler=None):\n",
    "    mask= df_actual[\"Date\"]< current_date\n",
    "    df_sub= df_actual.loc[mask].copy()\n",
    "    df_sub.sort_values(\"Date\", inplace=True)\n",
    "    if len(df_sub)< window:\n",
    "        return None\n",
    "    df_sub[features]= df_sub[features].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    arr= scaler.transform(df_sub[features].iloc[-window:].values)\n",
    "    return arr\n",
    "\n",
    "def forecast_n_days_from_date(model, df_actual, start_date, window=35, horizon=31,\n",
    "                              device=None, scaler=None, noise_std=0.1):\n",
    "    \"\"\"\n",
    "    Full-blind forecast, each day appended with random noise\n",
    "    to encourage fluctuations.\n",
    "    \"\"\"\n",
    "    def add_noise_4d(pred_unscaled, std=0.05):\n",
    "        # add random Gaussian noise to each dimension, clipped >0\n",
    "        noise= np.random.normal(0.0, std, size=pred_unscaled.shape)\n",
    "        return np.clip(pred_unscaled+ noise, a_min=0, a_max=None)\n",
    "\n",
    "    arr_window= get_latest_window(df_actual, start_date, window, scaler)\n",
    "    if arr_window is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rolling_buffer= np.copy(arr_window)\n",
    "    forecast_records=[]\n",
    "    current_date= pd.to_datetime(start_date)\n",
    "\n",
    "    for i in range(horizon):\n",
    "        X_input= torch.tensor(rolling_buffer,dtype=torch.float).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            pred_scaled= model(X_input).cpu().numpy()[0]\n",
    "\n",
    "        # invert scale\n",
    "        pred_unscaled= scaler.inverse_transform(pred_scaled.reshape(1,-1))[0]\n",
    "        # add noise => more fluctuation\n",
    "        pred_noisy= add_noise_4d(pred_unscaled, noise_std)\n",
    "\n",
    "        dayx= current_date+ pd.Timedelta(days=i)\n",
    "        forecast_records.append({\n",
    "            \"ForecastDate\": dayx,\n",
    "            \"Pred_Price_unscaled\": pred_noisy[0],\n",
    "            \"Pred_Open_unscaled\":  pred_noisy[1],\n",
    "            \"Pred_High_unscaled\":  pred_noisy[2],\n",
    "            \"Pred_Low_unscaled\":   pred_noisy[3]\n",
    "        })\n",
    "\n",
    "        # feed forward\n",
    "        pred_noisy_scaled= scaler.transform(pred_noisy.reshape(1,-1))[0]\n",
    "        rolling_buffer= np.vstack([rolling_buffer[1:], pred_noisy_scaled.reshape(1,-1)])\n",
    "\n",
    "    return pd.DataFrame(forecast_records)\n",
    "\n",
    "############################################\n",
    "# 8A. Daily Plot\n",
    "############################################\n",
    "def plot_daily_two_charts(day_i, horizon_days, daily_fcst_dict, df_actual):\n",
    "    df_sorted= df_actual.copy()\n",
    "    df_sorted.sort_values(\"Date\", inplace=True)\n",
    "    day_end= day_i+ pd.Timedelta(days= horizon_days -1)\n",
    "\n",
    "    mask= (df_sorted[\"Date\"]<= day_end)\n",
    "    df_plot= df_sorted.loc[mask].copy()\n",
    "\n",
    "    # 1) Joint Diagram\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(df_plot[\"Date\"], df_plot[\"Price\"], color=\"black\", label=\"Actual Price\")\n",
    "\n",
    "    color_map= {\n",
    "        \"LSTM\": \"red\",\n",
    "        \"GRU\": \"blue\",\n",
    "        \"RNN\": \"green\",\n",
    "        \"CNN\": \"orange\",\n",
    "        \"EnhancedLSTM\": \"magenta\",\n",
    "        \"Transformer\": \"cyan\",\n",
    "        \"N-BEATS\": \"brown\",\n",
    "        \"N-HITS\": \"pink\",\n",
    "        \"SVM\": \"olive\",\n",
    "        \"GPR\": \"teal\",\n",
    "        \"Boost\": \"purple\"\n",
    "    }\n",
    "    for mname, fcdf in daily_fcst_dict.items():\n",
    "        sub= fcdf[fcdf[\"ForecastDate\"]<= day_end].copy()\n",
    "        if not sub.empty:\n",
    "            plt.plot(\n",
    "                sub[\"ForecastDate\"],\n",
    "                sub[\"Pred_Price_unscaled\"],\n",
    "                color=color_map.get(mname,\"gray\"),\n",
    "                linestyle=\"--\",\n",
    "                label=f\"{mname} Forecast\"\n",
    "            )\n",
    "\n",
    "    plt.title(f\"Day {day_i.date()} - Joint Diagram (up to {day_end.date()})\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend(prop={\"size\":8})\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2) Zoom-In\n",
    "    plt.figure(figsize=(12,5))\n",
    "    maskz= (df_plot[\"Date\"]>= day_i)&(df_plot[\"Date\"]<= day_end)\n",
    "    df_zoom= df_plot.loc[maskz].copy()\n",
    "    if not df_zoom.empty:\n",
    "        plt.plot(df_zoom[\"Date\"], df_zoom[\"Price\"], color=\"black\", label=\"Actual Price\")\n",
    "    for mname, fcdf in daily_fcst_dict.items():\n",
    "        sub= fcdf[(fcdf[\"ForecastDate\"]>=day_i)&(fcdf[\"ForecastDate\"]<=day_end)]\n",
    "        if not sub.empty:\n",
    "            plt.plot(\n",
    "                sub[\"ForecastDate\"],\n",
    "                sub[\"Pred_Price_unscaled\"],\n",
    "                color=color_map.get(mname,\"gray\"),\n",
    "                linestyle=\"--\",\n",
    "                label=f\"{mname} Forecast\"\n",
    "            )\n",
    "\n",
    "    plt.title(f\"Day {day_i.date()} - Zoom-In Forecast ({day_i.date()} to {day_end.date()})\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend(prop={\"size\":8})\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "############################################\n",
    "# 8B. Rolling day-by-day\n",
    "############################################\n",
    "def rolling_train_validate_predict_moe(\n",
    "    df_full,\n",
    "    scaler_obj,\n",
    "    model_types,\n",
    "    start_train_cutoff=pd.to_datetime(\"2024-07-01\"),\n",
    "    start_val_cutoff=pd.to_datetime(\"2024-11-30\"),\n",
    "    start_pred=pd.to_datetime(\"2024-12-01\"),\n",
    "    end_pred=pd.to_datetime(\"2025-01-02\"),\n",
    "    horizon_days=30,\n",
    "    do_daily_plots=True\n",
    "):\n",
    "    df_sorted= df_full.copy()\n",
    "    df_sorted.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "    df_scaled_all= df_sorted.copy()\n",
    "    df_scaled_all[features]= df_scaled_all[features].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    df_scaled_all[features]= scaler_obj.transform(df_scaled_all[features])\n",
    "\n",
    "    date_range= pd.date_range(start_pred, end_pred, freq=\"D\")\n",
    "    all_records= []\n",
    "\n",
    "    for i,day_i in enumerate(date_range):\n",
    "        train_cutoff_i= start_train_cutoff + pd.Timedelta(days=i)\n",
    "        val_cutoff_i  = start_val_cutoff   + pd.Timedelta(days=i)\n",
    "\n",
    "        print(f\"\\n=== Rolling Day {day_i.date()} ===\")\n",
    "        print(f\"  Train cutoff={train_cutoff_i.date()}, Val cutoff={val_cutoff_i.date()}\")\n",
    "        print(f\"  Forecast next {horizon_days} days from {day_i.date()} with noise\")\n",
    "\n",
    "        # 1) Train or load each model\n",
    "        model_dict= {}\n",
    "        for mt in model_types:\n",
    "            out_file= f\"best_{mt}_{day_i.strftime('%Y-%m-%d')}_V1\"\n",
    "            if mt in [\"SVM\",\"GPR\",\"Boost\"]:\n",
    "                out_file += \".pkl\"\n",
    "                net= train_or_load_sklearn(\n",
    "                    mt, df_scaled_all, train_cutoff_i, val_cutoff_i,\n",
    "                    model_save_name=os.path.join(BEST_MODELS_DIR, out_file),\n",
    "                    window_size=WINDOW_SIZE\n",
    "                )\n",
    "            else:\n",
    "                out_file += \".pt\"\n",
    "                net= train_or_load_torch(\n",
    "                    mt, WINDOW_SIZE, df_scaled_all, train_cutoff_i,val_cutoff_i,\n",
    "                    model_save_name=os.path.join(BEST_MODELS_DIR, out_file),\n",
    "                    num_epochs=NUM_EPOCHS\n",
    "                )\n",
    "            model_dict[mt]= net\n",
    "\n",
    "        # 2) Forecast\n",
    "        daily_fcst_dict= {}\n",
    "        for mt, net in model_dict.items():\n",
    "            df_fc= forecast_n_days_from_date(\n",
    "                model= net,\n",
    "                df_actual= df_sorted,  # unscaled\n",
    "                start_date= day_i,\n",
    "                window= WINDOW_SIZE,\n",
    "                horizon= horizon_days,\n",
    "                device= device,\n",
    "                scaler= scaler_obj,\n",
    "                noise_std=0.1 # random noise => more fluctuations\n",
    "            )\n",
    "            if not df_fc.empty:\n",
    "                df_fc[\"BaseDate\"]= day_i\n",
    "                df_fc[\"Model\"]= mt\n",
    "                daily_fcst_dict[mt]= df_fc\n",
    "                all_records.append(df_fc)\n",
    "            else:\n",
    "                daily_fcst_dict[mt]= pd.DataFrame()\n",
    "\n",
    "        # see actual\n",
    "        row_actual= df_sorted[df_sorted[\"Date\"]== day_i]\n",
    "        if not row_actual.empty:\n",
    "            print(f\"  Actual price: {row_actual['Price'].values[0]:.4f}\")\n",
    "\n",
    "        # 3) daily plots\n",
    "        if do_daily_plots and daily_fcst_dict:\n",
    "            plot_daily_two_charts(day_i, horizon_days, daily_fcst_dict, df_sorted)\n",
    "\n",
    "    # Must do \"concat\" to combine DataFrames\n",
    "    df_all = pd.concat(all_records, ignore_index=True) if all_records else pd.DataFrame()\n",
    "    df_all.sort_values([\"Model\",\"BaseDate\",\"ForecastDate\"], inplace=True)\n",
    "    return df_all\n",
    "\n",
    "############################################\n",
    "# 9. Evaluate & Plot\n",
    "############################################\n",
    "def walk_forward_test(model, df_all_scaled, test_start, window=35, scaler=None):\n",
    "    dfw= df_all_scaled.copy()\n",
    "    dfw.sort_values(\"Date\", inplace=True)\n",
    "    dates_test= np.sort(dfw[dfw[\"Date\"]>=test_start][\"Date\"].unique())\n",
    "\n",
    "    model.eval()\n",
    "    preds=[]\n",
    "    with torch.no_grad():\n",
    "        for d in dates_test:\n",
    "            idx= dfw.index[dfw[\"Date\"]==d]\n",
    "            if len(idx)==0:\n",
    "                continue\n",
    "            idx= idx[0]\n",
    "            st= idx- window\n",
    "            if st<0:\n",
    "                continue\n",
    "            window_data= dfw.iloc[st:idx][features].values\n",
    "            if np.isnan(window_data).any():\n",
    "                continue\n",
    "            X_= torch.tensor(window_data, dtype=torch.float).unsqueeze(0).to(device)\n",
    "            out_scaled= model(X_).cpu().numpy()[0]\n",
    "            dfw.loc[idx, features]= out_scaled\n",
    "            preds.append((d,*out_scaled))\n",
    "\n",
    "    pred_df= pd.DataFrame(preds, columns=[\"Date\",\"Pred_Price_scaled\",\"Pred_Open_scaled\",\"Pred_High_scaled\",\"Pred_Low_scaled\"])\n",
    "    arr_s= pred_df[[\"Pred_Price_scaled\",\"Pred_Open_scaled\",\"Pred_High_scaled\",\"Pred_Low_scaled\"]].values\n",
    "    arr_un= scaler.inverse_transform(arr_s)\n",
    "    pred_df[\"Pred_Price_unscaled\"]= arr_un[:,0]\n",
    "    pred_df[\"Pred_Open_unscaled\"]=  arr_un[:,1]\n",
    "    pred_df[\"Pred_High_unscaled\"]= arr_un[:,2]\n",
    "    pred_df[\"Pred_Low_unscaled\"]=  arr_un[:,3]\n",
    "    return pred_df.sort_values(\"Date\")\n",
    "\n",
    "def evaluate_and_plot_all_models(results_dict, df_scaled, df_train, \n",
    "                                 train_cutoff, val_cutoff, test_cutoff, \n",
    "                                 csv_file_complete=\"McDonald’s Stock Price History_Complete.csv\"):\n",
    "    df_real= df_scaled.copy()\n",
    "    df_real.sort_values(\"Date\", inplace=True)\n",
    "    mask_tv= (df_real[\"Date\"]<test_cutoff)\n",
    "    df_tv= df_real[mask_tv].copy()\n",
    "    arr_s= df_tv[features].values\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    arr_un= scaler.inverse_transform(arr_s)\n",
    "    df_tv[\"Real_Price_unscaled\"]= arr_un[:,0]\n",
    "\n",
    "    plt.figure(figsize=(14,7))\n",
    "    plt.plot(df_tv[\"Date\"], df_tv[\"Real_Price_unscaled\"], color=\"black\", label=\"Actual(Train+Val)\")\n",
    "\n",
    "    merged_df= df_real.copy()\n",
    "    merged_df.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "    color_map= {\n",
    "        \"LSTM\": \"red\",\n",
    "        \"GRU\": \"blue\",\n",
    "        \"RNN\": \"green\",\n",
    "        \"CNN\": \"orange\",\n",
    "        \"EnhancedLSTM\": \"magenta\",\n",
    "        \"Transformer\": \"cyan\",\n",
    "        \"N-BEATS\": \"brown\",\n",
    "        \"N-HITS\": \"pink\",\n",
    "        \"SVM\": \"olive\",\n",
    "        \"GPR\": \"teal\",\n",
    "        \"Boost\": \"purple\"\n",
    "    }\n",
    "\n",
    "    for mname,infodict in results_dict.items():\n",
    "        val_df= infodict.get(\"val_pred_df\", None)\n",
    "        test_df= infodict.get(\"test_pred_df\", None)\n",
    "        frames=[]\n",
    "        if val_df is not None:\n",
    "            frames.append(val_df.rename(columns={\"Pred_Price_unscaled\":f\"Pred_{mname}\"}))\n",
    "        if test_df is not None:\n",
    "            frames.append(test_df.rename(columns={\"Pred_Price_unscaled\":f\"Pred_{mname}\"}))\n",
    "        if frames:\n",
    "            comb= pd.concat(frames, ignore_index=True)\n",
    "            comb= comb[[\"Date\",f\"Pred_{mname}\"]].drop_duplicates(\"Date\")\n",
    "            comb.sort_values(\"Date\", inplace=True)\n",
    "            merged_df= pd.merge(merged_df, comb, on=\"Date\", how=\"left\")\n",
    "\n",
    "    for mname in results_dict.keys():\n",
    "        col= f\"Pred_{mname}\"\n",
    "        if col in merged_df.columns:\n",
    "            plt.plot(\n",
    "                merged_df[\"Date\"],\n",
    "                merged_df[col],\n",
    "                color=color_map.get(mname,\"gray\"),\n",
    "                linestyle=\"-\",\n",
    "                label=f\"{mname} Pred\"\n",
    "            )\n",
    "\n",
    "    plt.axvspan(df_train[\"Date\"].min(), train_cutoff, color=\"skyblue\", alpha=0.1, label=\"Train\")\n",
    "    plt.axvspan(train_cutoff, val_cutoff, color=\"green\", alpha=0.1, label=\"Validation\")\n",
    "    plt.axvspan(test_cutoff, merged_df[\"Date\"].max(), color=\"yellow\", alpha=0.1, label=\"Test\")\n",
    "\n",
    "    plt.title(\"Joint Diagram: Actual Price vs. Predictions\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price(unscaled)\")\n",
    "    plt.legend(prop={\"size\":8})\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Zoom Test\n",
    "    df_test_zoom= merged_df[merged_df[\"Date\"]>= test_cutoff]\n",
    "    if not df_test_zoom.empty:\n",
    "        plt.figure(figsize=(12,5))\n",
    "        for mname in results_dict.keys():\n",
    "            c= f\"Pred_{mname}\"\n",
    "            if c in df_test_zoom.columns:\n",
    "                plt.plot(df_test_zoom[\"Date\"], df_test_zoom[c], linestyle=\"--\", label=f\"{mname} Pred\")\n",
    "        # overlay actual\n",
    "        df_complete= pd.read_csv(csv_file_complete)\n",
    "        df_complete[\"Date\"]= pd.to_datetime(df_complete[\"Date\"], errors=\"coerce\")\n",
    "        for drop_c in [\"Vol.\",\"Change %\"]:\n",
    "            if drop_c in df_complete.columns:\n",
    "                df_complete.drop(columns=[drop_c], inplace=True, errors=\"ignore\")\n",
    "        df_complete.sort_values(\"Date\", inplace=True)\n",
    "        for f in features:\n",
    "            df_complete[f]= df_complete[f].astype(str).str.replace(\",\",\"\",regex=True).astype(float)\n",
    "        df_test_only= df_complete[df_complete[\"Date\"]>= test_cutoff].copy()\n",
    "        plt.plot(df_test_only[\"Date\"], df_test_only[\"Price\"], color=\"black\",lw=2, label=\"ActualTestPrice\")\n",
    "\n",
    "        plt.title(\"Test Period Zoom-In\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Price\")\n",
    "        plt.legend(prop={\"size\":8})\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "############################################\n",
    "# 10. Final \"concatenated\" rolling forecasts\n",
    "############################################\n",
    "def plot_final_rolling_fc(final_rolling_fcst, df_actual,\n",
    "                          from_d, to_d):\n",
    "    if final_rolling_fcst.empty:\n",
    "        print(\"No final rolling forecast data. Exiting.\")\n",
    "        return\n",
    "\n",
    "    df_act= df_actual.copy()\n",
    "    df_act.sort_values(\"Date\", inplace=True)\n",
    "    mask= (df_act[\"Date\"]>= from_d)&(df_act[\"Date\"]<= to_d)\n",
    "    df_a= df_act[mask].copy()\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(df_a[\"Date\"], df_a[\"Price\"], color=\"black\", label=\"ActualPrice\")\n",
    "\n",
    "    color_map= {\n",
    "        \"LSTM\": \"red\",\n",
    "        \"GRU\": \"blue\",\n",
    "        \"RNN\": \"green\",\n",
    "        \"CNN\": \"orange\",\n",
    "        \"EnhancedLSTM\": \"magenta\",\n",
    "        \"Transformer\": \"cyan\",\n",
    "        \"N-BEATS\": \"brown\",\n",
    "        \"N-HITS\": \"pink\",\n",
    "        \"SVM\": \"olive\",\n",
    "        \"GPR\": \"teal\",\n",
    "        \"Boost\": \"purple\"\n",
    "    }\n",
    "    for m in final_rolling_fcst[\"Model\"].unique():\n",
    "        df_m= final_rolling_fcst[final_rolling_fcst[\"Model\"]== m].copy()\n",
    "        df_m= df_m[(df_m[\"ForecastDate\"]>=from_d)&(df_m[\"ForecastDate\"]<= to_d)]\n",
    "        if not df_m.empty:\n",
    "            plt.plot(df_m[\"ForecastDate\"], df_m[\"Pred_Price_unscaled\"],\n",
    "                     color=color_map.get(m,\"gray\"), linestyle=\"--\",label=f\"{m} Forecast\")\n",
    "\n",
    "    plt.title(f\"Rolling Forecast from {from_d.date()} to {to_d.date()}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend(prop={\"size\":8})\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "############################################\n",
    "# MAIN\n",
    "############################################\n",
    "if __name__==\"__main__\":\n",
    "    # 1) Load Data\n",
    "    df_scaled, df_complete_all, scaler= load_and_scale_data(\n",
    "        \"McDonald’s Stock Price History.csv\",\n",
    "        \"McDonald’s Stock Price History_Complete.csv\"\n",
    "    )\n",
    "\n",
    "    # 2) Example: train or load static\n",
    "    # ...\n",
    "    # 3) Rolling approach\n",
    "    final_rolling_fc= rolling_train_validate_predict_moe(\n",
    "        df_full= df_complete_all,\n",
    "        scaler_obj= scaler,\n",
    "        model_types= [\"LSTM\",\"GRU\",\"RNN\",\"CNN\",\"Transformer\",\"N-BEATS\",\"N-HITS\",\"SVM\",\"Boost\"],\n",
    "        start_train_cutoff=pd.to_datetime(\"2024-07-01\"),\n",
    "        start_val_cutoff=pd.to_datetime(\"2024-11-30\"),\n",
    "        start_pred=pd.to_datetime(\"2024-12-01\"),\n",
    "        end_pred=pd.to_datetime(\"2025-01-02\"),\n",
    "        horizon_days=30,\n",
    "        do_daily_plots=True\n",
    "    )\n",
    "\n",
    "    # 4) Plot final\n",
    "    print(\"===== ROLLING FC HEAD =====\")\n",
    "    print(final_rolling_fc.head(50))\n",
    "    plot_final_rolling_fc(final_rolling_fc, df_complete_all, \n",
    "                          from_d=pd.to_datetime(\"2024-12-01\"),\n",
    "                          to_d=pd.to_datetime(\"2025-01-31\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
