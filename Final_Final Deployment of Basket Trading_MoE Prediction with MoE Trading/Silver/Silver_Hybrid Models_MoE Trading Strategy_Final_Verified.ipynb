{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rolling Day 2024-12-01 ===\n",
      "  train_cutoff=2024-07-01  val_cutoff=2024-11-30\n",
      "  Forecast next 30 days from 2024-12-01\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-01_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-01_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-01_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-01_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-01_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-01_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-01_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-01_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-01_V1.pkl\n",
      "[MoE] No saved file for 2024-12-01, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-02 ===\n",
      "  train_cutoff=2024-07-02  val_cutoff=2024-12-01\n",
      "  Forecast next 30 days from 2024-12-02\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-02_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-02_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-02_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-02_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-02_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-02_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-02_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-02_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-02_V1.pkl\n",
      "[MoE] No saved file for 2024-12-02, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-03 ===\n",
      "  train_cutoff=2024-07-03  val_cutoff=2024-12-02\n",
      "  Forecast next 30 days from 2024-12-03\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-03_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-03_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-03_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-03_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-03_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-03_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-03_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-03_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-03_V1.pkl\n",
      "[MoE] No saved file for 2024-12-03, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-04 ===\n",
      "  train_cutoff=2024-07-04  val_cutoff=2024-12-03\n",
      "  Forecast next 30 days from 2024-12-04\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-04_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-04_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-04_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-04_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-04_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-04_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-04_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-04_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-04_V1.pkl\n",
      "[MoE] No saved file for 2024-12-04, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-05 ===\n",
      "  train_cutoff=2024-07-05  val_cutoff=2024-12-04\n",
      "  Forecast next 30 days from 2024-12-05\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-05_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-05_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-05_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-05_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-05_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-05_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-05_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-05_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-05_V1.pkl\n",
      "[MoE] No saved file for 2024-12-05, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-06 ===\n",
      "  train_cutoff=2024-07-06  val_cutoff=2024-12-05\n",
      "  Forecast next 30 days from 2024-12-06\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-06_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-06_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-06_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-06_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-06_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-06_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-06_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-06_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-06_V1.pkl\n",
      "[MoE] No saved file for 2024-12-06, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-07 ===\n",
      "  train_cutoff=2024-07-07  val_cutoff=2024-12-06\n",
      "  Forecast next 30 days from 2024-12-07\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-07_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-07_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-07_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-07_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-07_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-07_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-07_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-07_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-07_V1.pkl\n",
      "[MoE] No saved file for 2024-12-07, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-08 ===\n",
      "  train_cutoff=2024-07-08  val_cutoff=2024-12-07\n",
      "  Forecast next 30 days from 2024-12-08\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-08_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-08_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-08_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-08_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-08_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-08_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-08_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-08_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-08_V1.pkl\n",
      "[MoE] No saved file for 2024-12-08, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-09 ===\n",
      "  train_cutoff=2024-07-09  val_cutoff=2024-12-08\n",
      "  Forecast next 30 days from 2024-12-09\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-09_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-09_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-09_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-09_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-09_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-09_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-09_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-09_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-09_V1.pkl\n",
      "[MoE] No saved file for 2024-12-09, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-10 ===\n",
      "  train_cutoff=2024-07-10  val_cutoff=2024-12-09\n",
      "  Forecast next 30 days from 2024-12-10\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-10_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-10_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-10_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-10_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-10_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-10_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-10_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-10_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-10_V1.pkl\n",
      "[MoE] No saved file for 2024-12-10, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-11 ===\n",
      "  train_cutoff=2024-07-11  val_cutoff=2024-12-10\n",
      "  Forecast next 30 days from 2024-12-11\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-11_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-11_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-11_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-11_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-11_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-11_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-11_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-11_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-11_V1.pkl\n",
      "[MoE] No saved file for 2024-12-11, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-12 ===\n",
      "  train_cutoff=2024-07-12  val_cutoff=2024-12-11\n",
      "  Forecast next 30 days from 2024-12-12\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-12_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-12_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-12_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-12_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-12_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-12_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-12_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-12_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-12_V1.pkl\n",
      "[MoE] No saved file for 2024-12-12, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-13 ===\n",
      "  train_cutoff=2024-07-13  val_cutoff=2024-12-12\n",
      "  Forecast next 30 days from 2024-12-13\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-13_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-13_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-13_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-13_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-13_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-13_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-13_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-13_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-13_V1.pkl\n",
      "[MoE] No saved file for 2024-12-13, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-14 ===\n",
      "  train_cutoff=2024-07-14  val_cutoff=2024-12-13\n",
      "  Forecast next 30 days from 2024-12-14\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-14_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-14_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-14_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-14_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-14_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-14_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-14_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-14_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-14_V1.pkl\n",
      "[MoE] No saved file for 2024-12-14, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-15 ===\n",
      "  train_cutoff=2024-07-15  val_cutoff=2024-12-14\n",
      "  Forecast next 30 days from 2024-12-15\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-15_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-15_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-15_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-15_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-15_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-15_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-15_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-15_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-15_V1.pkl\n",
      "[MoE] No saved file for 2024-12-15, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-16 ===\n",
      "  train_cutoff=2024-07-16  val_cutoff=2024-12-15\n",
      "  Forecast next 30 days from 2024-12-16\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-16_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-16_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-16_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-16_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-16_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-16_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-16_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-16_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-16_V1.pkl\n",
      "[MoE] No saved file for 2024-12-16, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-17 ===\n",
      "  train_cutoff=2024-07-17  val_cutoff=2024-12-16\n",
      "  Forecast next 30 days from 2024-12-17\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-17_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-17_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-17_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-17_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-17_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-17_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-17_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-17_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-17_V1.pkl\n",
      "[MoE] No saved file for 2024-12-17, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-18 ===\n",
      "  train_cutoff=2024-07-18  val_cutoff=2024-12-17\n",
      "  Forecast next 30 days from 2024-12-18\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-18_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-18_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-18_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-18_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-18_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-18_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-18_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-18_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-18_V1.pkl\n",
      "[MoE] No saved file for 2024-12-18, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-19 ===\n",
      "  train_cutoff=2024-07-19  val_cutoff=2024-12-18\n",
      "  Forecast next 30 days from 2024-12-19\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-19_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-19_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-19_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-19_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-19_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-19_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-19_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-19_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-19_V1.pkl\n",
      "[MoE] No saved file for 2024-12-19, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-20 ===\n",
      "  train_cutoff=2024-07-20  val_cutoff=2024-12-19\n",
      "  Forecast next 30 days from 2024-12-20\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-20_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-20_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-20_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-20_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-20_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-20_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-20_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-20_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-20_V1.pkl\n",
      "[MoE] No saved file for 2024-12-20, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-21 ===\n",
      "  train_cutoff=2024-07-21  val_cutoff=2024-12-20\n",
      "  Forecast next 30 days from 2024-12-21\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-21_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-21_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-21_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-21_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-21_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-21_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-21_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-21_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-21_V1.pkl\n",
      "[MoE] No saved file for 2024-12-21, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-22 ===\n",
      "  train_cutoff=2024-07-22  val_cutoff=2024-12-21\n",
      "  Forecast next 30 days from 2024-12-22\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-22_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-22_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-22_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-22_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-22_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-22_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-22_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-22_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-22_V1.pkl\n",
      "[MoE] No saved file for 2024-12-22, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-23 ===\n",
      "  train_cutoff=2024-07-23  val_cutoff=2024-12-22\n",
      "  Forecast next 30 days from 2024-12-23\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-23_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-23_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-23_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-23_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-23_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-23_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-23_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-23_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-23_V1.pkl\n",
      "[MoE] No saved file for 2024-12-23, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-24 ===\n",
      "  train_cutoff=2024-07-24  val_cutoff=2024-12-23\n",
      "  Forecast next 30 days from 2024-12-24\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-24_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-24_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-24_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-24_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-24_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-24_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-24_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-24_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-24_V1.pkl\n",
      "[MoE] No saved file for 2024-12-24, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-25 ===\n",
      "  train_cutoff=2024-07-25  val_cutoff=2024-12-24\n",
      "  Forecast next 30 days from 2024-12-25\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-25_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-25_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-25_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-25_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-25_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-25_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-25_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-25_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-25_V1.pkl\n",
      "[MoE] No saved file for 2024-12-25, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-26 ===\n",
      "  train_cutoff=2024-07-26  val_cutoff=2024-12-25\n",
      "  Forecast next 30 days from 2024-12-26\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-26_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-26_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-26_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-26_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-26_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-26_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-26_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-26_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-26_V1.pkl\n",
      "[MoE] No saved file for 2024-12-26, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-27 ===\n",
      "  train_cutoff=2024-07-27  val_cutoff=2024-12-26\n",
      "  Forecast next 30 days from 2024-12-27\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-27_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-27_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-27_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-27_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-27_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-27_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-27_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-27_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-27_V1.pkl\n",
      "[MoE] No saved file for 2024-12-27, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-28 ===\n",
      "  train_cutoff=2024-07-28  val_cutoff=2024-12-27\n",
      "  Forecast next 30 days from 2024-12-28\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-28_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-28_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-28_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-28_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-28_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-28_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-28_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-28_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-28_V1.pkl\n",
      "[MoE] No saved file for 2024-12-28, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-29 ===\n",
      "  train_cutoff=2024-07-29  val_cutoff=2024-12-28\n",
      "  Forecast next 30 days from 2024-12-29\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-29_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-29_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-29_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-29_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-29_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-29_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-29_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-29_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-29_V1.pkl\n",
      "[MoE] No saved file for 2024-12-29, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-30 ===\n",
      "  train_cutoff=2024-07-30  val_cutoff=2024-12-29\n",
      "  Forecast next 30 days from 2024-12-30\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-30_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-30_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-30_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-30_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-30_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-30_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-30_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-30_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-30_V1.pkl\n",
      "[MoE] No saved file for 2024-12-30, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-31 ===\n",
      "  train_cutoff=2024-07-31  val_cutoff=2024-12-30\n",
      "  Forecast next 30 days from 2024-12-31\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-31_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-31_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-31_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-31_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-31_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-31_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-31_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-31_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-31_V1.pkl\n",
      "[MoE] No saved file for 2024-12-31, skipping load.\n",
      "\n",
      "=== Rolling Day 2025-01-01 ===\n",
      "  train_cutoff=2024-08-01  val_cutoff=2024-12-31\n",
      "  Forecast next 30 days from 2025-01-01\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2025-01-01_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2025-01-01_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2025-01-01_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2025-01-01_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2025-01-01_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2025-01-01_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2025-01-01_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2025-01-01_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2025-01-01_V1.pkl\n",
      "[MoE] No saved file for 2025-01-01, skipping load.\n",
      "\n",
      "=== Rolling Day 2025-01-02 ===\n",
      "  train_cutoff=2024-08-02  val_cutoff=2025-01-01\n",
      "  Forecast next 30 days from 2025-01-02\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2025-01-02_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2025-01-02_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2025-01-02_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2025-01-02_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2025-01-02_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2025-01-02_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2025-01-02_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2025-01-02_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2025-01-02_V1.pkl\n",
      "[MoE] No saved file for 2025-01-02, skipping load.\n",
      "\n",
      "=== HEAD of final_rolling_fc ===\n",
      "    ForecastDate  Pred_Price_unscaled  Pred_Open_unscaled  Pred_High_unscaled  \\\n",
      "240   2024-12-01            32.238688           30.746930           31.082317   \n",
      "241   2024-12-02            32.138510           30.434543           30.882797   \n",
      "242   2024-12-03            31.818182           30.411695           30.544711   \n",
      "243   2024-12-04            32.198320           30.397740           30.362424   \n",
      "244   2024-12-05            31.999176           29.761120           30.209340   \n",
      "245   2024-12-06            31.051531           29.431389           29.737187   \n",
      "246   2024-12-07            31.065342           29.522161           29.480737   \n",
      "247   2024-12-08            30.665777           29.206020           29.389746   \n",
      "248   2024-12-09            30.663181           29.204459           29.256267   \n",
      "249   2024-12-10            30.657741           28.939169           29.152859   \n",
      "\n",
      "     Pred_Low_unscaled   BaseDate  Model  \n",
      "240          30.019893 2024-12-01  Boost  \n",
      "241          29.798322 2024-12-01  Boost  \n",
      "242          29.727182 2024-12-01  Boost  \n",
      "243          29.523942 2024-12-01  Boost  \n",
      "244          28.953333 2024-12-01  Boost  \n",
      "245          28.571498 2024-12-01  Boost  \n",
      "246          28.437896 2024-12-01  Boost  \n",
      "247          28.372124 2024-12-01  Boost  \n",
      "248          28.221187 2024-12-01  Boost  \n",
      "249          28.093488 2024-12-01  Boost  \n",
      "\n",
      "=== SUPER ADVANCED MOE w/ Polynomials & Voting ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_922053/3481502643.py:419: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_slope= df_merge.groupby([\"Model\",\"BaseDate\"], group_keys=False).apply(group_slope_func)\n",
      "/tmp/ipykernel_922053/3481502643.py:438: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  g_vol= df_merge.groupby([\"Model\",\"BaseDate\"]).apply(mini_volatility).reset_index()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_predicted_price_all_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 889\u001b[0m\n\u001b[1;32m    886\u001b[0m     df_rolled_all\u001b[38;5;241m=\u001b[39m final_rolling_fc\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# D) Quick Plot of All Models in Jan 2025\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m \u001b[43mplot_predicted_price_all_models\u001b[49m(\n\u001b[1;32m    890\u001b[0m     df_rolled_all,\n\u001b[1;32m    891\u001b[0m     from_d\u001b[38;5;241m=\u001b[39m PLOT_START,\n\u001b[1;32m    892\u001b[0m     to_d\u001b[38;5;241m=\u001b[39m   PLOT_END\n\u001b[1;32m    893\u001b[0m )\n\u001b[1;32m    895\u001b[0m \u001b[38;5;66;03m# E) Merge w/ actual => daily differences\u001b[39;00m\n\u001b[1;32m    896\u001b[0m df_diff_all\u001b[38;5;241m=\u001b[39m merge_with_actual_and_diff(df_rolled_all, df_complete_all)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_predicted_price_all_models' is not defined"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 0) Imports & Global Setup\n",
    "###############################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import joblib\n",
    "\n",
    "from numpy.polynomial.polynomial import polyfit, polyval\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Global columns and constants\n",
    "features = [\"Price\",\"Open\",\"High\",\"Low\"]\n",
    "WINDOW_SIZE = 35\n",
    "\n",
    "# Date ranges\n",
    "train_cutoff = pd.to_datetime(\"2024-07-01\")\n",
    "val_cutoff   = pd.to_datetime(\"2024-11-30\")\n",
    "test_cutoff  = pd.to_datetime(\"2024-12-01\")\n",
    "\n",
    "# For final evaluation & plotting (focus: Jan 2025)\n",
    "PLOT_START = pd.to_datetime(\"2025-01-01\")\n",
    "PLOT_END   = pd.to_datetime(\"2025-01-31\")\n",
    "\n",
    "BEST_MODELS_DIR = \".\"\n",
    "\n",
    "###############################################################################\n",
    "# 1) Data Loading & Scaling\n",
    "###############################################################################\n",
    "def load_and_scale_data(train_csv=\"Silver Futures Historical Data.csv\",\n",
    "                        complete_csv=\"Silver Futures Historical Data_Complete.csv\"):\n",
    "    df_traincsv = pd.read_csv(train_csv)\n",
    "    df_traincsv[\"Date\"] = pd.to_datetime(df_traincsv[\"Date\"], errors=\"coerce\")\n",
    "    for col in [\"Vol.\",\"Change %\"]:\n",
    "        if col in df_traincsv.columns:\n",
    "            df_traincsv.drop(columns=[col], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # Clean columns\n",
    "    for feat in features:\n",
    "        if feat not in df_traincsv.columns:\n",
    "            df_traincsv[feat] = np.nan\n",
    "        else:\n",
    "            df_traincsv[feat] = df_traincsv[feat].astype(str).str.replace(\",\", \"\", regex=True)\n",
    "            df_traincsv[feat] = pd.to_numeric(df_traincsv[feat], errors=\"coerce\")\n",
    "\n",
    "    df_traincsv.sort_values(\"Date\", inplace=True)\n",
    "    df_traincsv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Split into train/val/test\n",
    "    df_trn = df_traincsv[df_traincsv[\"Date\"] < train_cutoff].copy()\n",
    "    df_val = df_traincsv[(df_traincsv[\"Date\"] >= train_cutoff) & (df_traincsv[\"Date\"] <= val_cutoff)].copy()\n",
    "    df_tst = df_traincsv[df_traincsv[\"Date\"] >= test_cutoff].copy()\n",
    "\n",
    "    # Scale only on training subset\n",
    "    df_trn_nonan = df_trn.dropna(subset=features)\n",
    "    scaler = MinMaxScaler()\n",
    "    if not df_trn_nonan.empty:\n",
    "        scaler.fit(df_trn_nonan[features])\n",
    "\n",
    "    def apply_scaler(df_sub):\n",
    "        sub_nonan = df_sub.dropna(subset=features)\n",
    "        if sub_nonan.empty:\n",
    "            return df_sub\n",
    "        df_sub.loc[sub_nonan.index, features] = scaler.transform(sub_nonan[features])\n",
    "        return df_sub\n",
    "\n",
    "    df_trn_scaled = apply_scaler(df_trn)\n",
    "    df_val_scaled = apply_scaler(df_val)\n",
    "    # For test, fill missing with min of training, then scale\n",
    "    df_tst_scaled = df_tst.copy()\n",
    "    if not df_tst_scaled.empty and not df_trn_nonan.empty:\n",
    "        train_mins = df_trn_nonan[features].min()\n",
    "        df_tst_filled = df_tst_scaled[features].fillna(train_mins)\n",
    "        df_tst_scaled.loc[:, features] = scaler.transform(df_tst_filled)\n",
    "\n",
    "    df_scaled = pd.concat([df_trn_scaled, df_val_scaled, df_tst_scaled], ignore_index=True)\n",
    "    df_scaled.sort_values(\"Date\", inplace=True)\n",
    "    df_scaled.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Also load the more complete CSV for raw unscaled data\n",
    "    df_complete = pd.read_csv(complete_csv)\n",
    "    df_complete[\"Date\"] = pd.to_datetime(df_complete[\"Date\"], errors=\"coerce\")\n",
    "    for col in [\"Vol.\",\"Change %\"]:\n",
    "        if col in df_complete.columns:\n",
    "            df_complete.drop(columns=[col], errors=\"ignore\", inplace=True)\n",
    "    for feat in features:\n",
    "        df_complete[feat] = df_complete[feat].astype(str).str.replace(\",\", \"\", regex=True).astype(float)\n",
    "    df_complete.sort_values(\"Date\", inplace=True)\n",
    "    df_complete.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_scaled, df_complete, scaler\n",
    "\n",
    "###############################################################################\n",
    "# 2) PyTorch Model Definitions\n",
    "###############################################################################\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_type=\"LSTM\", window_width=35):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "\n",
    "        if model_type == \"CNN\":\n",
    "            self.conv1 = nn.Conv1d(4, 64, 3)\n",
    "            self.conv2 = nn.Conv1d(64,128,3)\n",
    "            with torch.no_grad():\n",
    "                dummy = torch.zeros(1,4,window_width)\n",
    "                outdummy = self.conv2(F.relu(self.conv1(dummy)))\n",
    "                conv_output_size = outdummy.shape[1]*outdummy.shape[2]\n",
    "            self.fc = nn.Linear(conv_output_size,4)\n",
    "\n",
    "        elif model_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(4,128,num_layers=2,batch_first=True,dropout=0.1)\n",
    "            self.fc = nn.Linear(128,4)\n",
    "        elif model_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(4,128,num_layers=2,batch_first=True,dropout=0.1)\n",
    "            self.fc = nn.Linear(128,4)\n",
    "        elif model_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(4,128,num_layers=2,nonlinearity=\"relu\",batch_first=True,dropout=0.1)\n",
    "            self.fc = nn.Linear(128,4)\n",
    "        elif model_type == \"EnhancedLSTM\":\n",
    "            self.rnn = nn.LSTM(4,128,num_layers=3,batch_first=True,dropout=0.2)\n",
    "            self.bn = nn.BatchNorm1d(128)\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "            self.fc = nn.Linear(128,4)\n",
    "        elif model_type == \"Transformer\":\n",
    "            self.input_linear = nn.Linear(4,128)\n",
    "            enc_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8, dropout=0.1)\n",
    "            self.transformer_encoder = nn.TransformerEncoder(enc_layer, num_layers=3)\n",
    "            self.fc = nn.Linear(128,4)\n",
    "        elif model_type in [\"N-BEATS\",\"N-HITS\"]:\n",
    "            self.input_size= window_width*4\n",
    "            self.blocks= nn.ModuleList([nn.Sequential(\n",
    "                nn.Linear(self.input_size,128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128,128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128,4)\n",
    "            ) for _ in range(3)])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid model_type: {model_type}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.model_type==\"CNN\":\n",
    "            x= x.permute(0,2,1)\n",
    "            x= F.relu(self.conv1(x))\n",
    "            x= F.relu(self.conv2(x))\n",
    "            x= x.view(x.size(0),-1)\n",
    "            return self.fc(x)\n",
    "\n",
    "        elif self.model_type in [\"LSTM\",\"GRU\",\"RNN\"]:\n",
    "            out, _= self.rnn(x)\n",
    "            out= out[:,-1,:]\n",
    "            return self.fc(out)\n",
    "\n",
    "        elif self.model_type==\"EnhancedLSTM\":\n",
    "            out, _= self.rnn(x)\n",
    "            out= out[:,-1,:]\n",
    "            out= self.bn(out)\n",
    "            out= self.dropout(out)\n",
    "            return self.fc(out)\n",
    "\n",
    "        elif self.model_type==\"Transformer\":\n",
    "            x= self.input_linear(x)\n",
    "            x= x.permute(1,0,2)\n",
    "            x= self.transformer_encoder(x)\n",
    "            x= x[-1,:,:]\n",
    "            return self.fc(x)\n",
    "\n",
    "        elif self.model_type in [\"N-BEATS\",\"N-HITS\"]:\n",
    "            xflat= x.reshape(x.size(0), -1)\n",
    "            forecast= 0\n",
    "            for block in self.blocks:\n",
    "                forecast += block(xflat)\n",
    "            return forecast\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type\")\n",
    "\n",
    "class SklearnWrapper:\n",
    "    \"\"\"\n",
    "    Simple wrapper so we can call sklearn regressors similarly to a PyTorch model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        arr = x.cpu().numpy().reshape(1,-1)\n",
    "        pred = self.model.predict(arr)\n",
    "        return torch.from_numpy(pred).float().to(x.device)\n",
    "    def eval(self):\n",
    "        pass\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "def load_torch_model(model_type, window_size, path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    print(f\"[PyTorch] Loading {model_type} from {path}\")\n",
    "    net = BaseModel(model_type, window_size).to(device)\n",
    "    net.load_state_dict(torch.load(path, map_location=device))\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "def load_sklearn_model(model_type, path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    print(f\"[sklearn] Loading {model_type} => {path}\")\n",
    "    loaded = joblib.load(path)\n",
    "    return SklearnWrapper(loaded)\n",
    "\n",
    "###############################################################################\n",
    "# 3) Rolling Multi-Day Forecast with All Models (Including MoE)\n",
    "###############################################################################\n",
    "def get_latest_window(df_actual, current_date, window=35, scaler=None):\n",
    "    \"\"\"\n",
    "    Returns the scaled array of the last 'window' days of features\n",
    "    prior to current_date.\n",
    "    \"\"\"\n",
    "    mask= df_actual[\"Date\"]< current_date\n",
    "    sub= df_actual.loc[mask].copy()\n",
    "    sub.sort_values(\"Date\", inplace=True)\n",
    "    if len(sub) < window:\n",
    "        return None\n",
    "    sub[features]= sub[features].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    arr= scaler.transform(sub[features].iloc[-window:].values)\n",
    "    return arr\n",
    "\n",
    "def forecast_n_days_from_date(model, df_actual, start_date, window=35, horizon=30,\n",
    "                              device=None, scaler=None, noise_std=0.01):\n",
    "    \"\"\"\n",
    "    Rolling forecast with mild random noise => more volatility.\n",
    "    \"\"\"\n",
    "    def add_noise_4d(prices, std=0.01):\n",
    "        noise= np.random.normal(0.0, std, size=prices.shape)\n",
    "        return np.clip(prices + noise, a_min=0, a_max=None)\n",
    "\n",
    "    arr_window= get_latest_window(df_actual, start_date, window, scaler)\n",
    "    if arr_window is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rolling_buffer= np.copy(arr_window)\n",
    "    forecast_records=[]\n",
    "    cur_dt= pd.to_datetime(start_date)\n",
    "\n",
    "    for i in range(horizon):\n",
    "        X_in= torch.tensor(rolling_buffer, dtype=torch.float).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            out_scaled= model(X_in).cpu().numpy()[0]\n",
    "        out_unscaled= scaler.inverse_transform(out_scaled.reshape(1,-1))[0]\n",
    "        out_noisy= add_noise_4d(out_unscaled, noise_std)\n",
    "\n",
    "        dayx= cur_dt + pd.Timedelta(days=i)\n",
    "        forecast_records.append({\n",
    "            \"ForecastDate\": dayx,\n",
    "            \"Pred_Price_unscaled\": out_noisy[0],\n",
    "            \"Pred_Open_unscaled\":  out_noisy[1],\n",
    "            \"Pred_High_unscaled\":  out_noisy[2],\n",
    "            \"Pred_Low_unscaled\":   out_noisy[3]\n",
    "        })\n",
    "\n",
    "        out_noisy_scaled= scaler.transform(out_noisy.reshape(1,-1))[0]\n",
    "        rolling_buffer= np.vstack([rolling_buffer[1:], out_noisy_scaled.reshape(1,-1)])\n",
    "\n",
    "    return pd.DataFrame(forecast_records)\n",
    "\n",
    "def rolling_train_validate_predict_moe(\n",
    "    df_full, scaler_obj, model_types,\n",
    "    start_train_cutoff=pd.to_datetime(\"2024-07-01\"),\n",
    "    start_val_cutoff=pd.to_datetime(\"2024-11-30\"),\n",
    "    start_pred=pd.to_datetime(\"2024-12-01\"),\n",
    "    end_pred=pd.to_datetime(\"2025-01-01\"),\n",
    "    horizon_days=30\n",
    "):\n",
    "    \"\"\"\n",
    "    For each day in [start_pred..end_pred], load the best_{model_type}_{YYYY-mm-dd}_V1,\n",
    "    forecast next horizon_days, combine results.\n",
    "    If \"MoE\" is in model_types, try to load a corresponding file. If none found, we skip.\n",
    "    \"\"\"\n",
    "    df_sorted= df_full.copy()\n",
    "    df_sorted.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "    date_rng= pd.date_range(start_pred, end_pred, freq=\"D\")\n",
    "    all_records=[]\n",
    "\n",
    "    for i, day_i in enumerate(date_rng):\n",
    "        train_cutoff_i= start_train_cutoff + pd.Timedelta(days=i)\n",
    "        val_cutoff_i  = start_val_cutoff   + pd.Timedelta(days=i)\n",
    "        print(f\"\\n=== Rolling Day {day_i.date()} ===\")\n",
    "        print(f\"  train_cutoff={train_cutoff_i.date()}  val_cutoff={val_cutoff_i.date()}\")\n",
    "        print(f\"  Forecast next {horizon_days} days from {day_i.date()}\")\n",
    "\n",
    "        model_dict={}\n",
    "        for mt in model_types:\n",
    "            # Special case for \"MoE\"\n",
    "            if mt==\"MoE\":\n",
    "                out_file= f\"best_MoE_{day_i.strftime('%Y-%m-%d')}_V1\"\n",
    "                pt_file= os.path.join(BEST_MODELS_DIR, out_file+\".pt\")\n",
    "                pkl_file= os.path.join(BEST_MODELS_DIR, out_file+\".pkl\")\n",
    "                if os.path.exists(pt_file):\n",
    "                    net= load_torch_model(mt, WINDOW_SIZE, pt_file)\n",
    "                    model_dict[mt]= net\n",
    "                elif os.path.exists(pkl_file):\n",
    "                    net= load_sklearn_model(mt, pkl_file)\n",
    "                    model_dict[mt]= net\n",
    "                else:\n",
    "                    print(f\"[MoE] No saved file for {day_i.date()}, skipping load.\")\n",
    "                continue\n",
    "\n",
    "            # Normal model\n",
    "            out_file= f\"best_{mt}_{day_i.strftime('%Y-%m-%d')}_V1\"\n",
    "            if mt in [\"SVM\",\"GPR\",\"Boost\"]:\n",
    "                out_file+= \".pkl\"\n",
    "                fullpath= os.path.join(BEST_MODELS_DIR, out_file)\n",
    "                if os.path.exists(fullpath):\n",
    "                    net= load_sklearn_model(mt, fullpath)\n",
    "                    model_dict[mt]= net\n",
    "            else:\n",
    "                out_file+= \".pt\"\n",
    "                fullpath= os.path.join(BEST_MODELS_DIR, out_file)\n",
    "                if os.path.exists(fullpath):\n",
    "                    net= load_torch_model(mt, WINDOW_SIZE, fullpath)\n",
    "                    model_dict[mt]= net\n",
    "\n",
    "        # Forecast\n",
    "        for mt, netobj in model_dict.items():\n",
    "            df_fc= forecast_n_days_from_date(\n",
    "                model=netobj,\n",
    "                df_actual=df_sorted,\n",
    "                start_date=day_i,\n",
    "                window=WINDOW_SIZE,\n",
    "                horizon=horizon_days,\n",
    "                device=device,\n",
    "                scaler=scaler_obj,\n",
    "                noise_std=0.01\n",
    "            )\n",
    "            if not df_fc.empty:\n",
    "                df_fc[\"BaseDate\"] = day_i\n",
    "                df_fc[\"Model\"]    = mt\n",
    "                all_records.append(df_fc)\n",
    "\n",
    "    df_all= pd.concat(all_records, ignore_index=True) if all_records else pd.DataFrame()\n",
    "    if not df_all.empty:\n",
    "        df_all.sort_values([\"Model\",\"BaseDate\",\"ForecastDate\"], inplace=True)\n",
    "    return df_all\n",
    "\n",
    "###############################################################################\n",
    "# 4) Compute Super Advanced MoE if no MoE loaded\n",
    "###############################################################################\n",
    "def build_mini_models_table(final_rolling_fcst, df_actual):\n",
    "    \"\"\"\n",
    "    Use [2024-12-01..2025-01-01] as the weighting period to measure model performance\n",
    "    for weighting the MoE if we need to build it ourselves.\n",
    "    \"\"\"\n",
    "    start_jan= pd.to_datetime(\"2024-12-01\")\n",
    "    end_jan=   pd.to_datetime(\"2025-01-01\")\n",
    "\n",
    "    df_jan= final_rolling_fcst[\n",
    "        (final_rolling_fcst[\"ForecastDate\"]>= start_jan)&\n",
    "        (final_rolling_fcst[\"ForecastDate\"]<= end_jan)\n",
    "    ].copy()\n",
    "    if df_jan.empty:\n",
    "        return pd.DataFrame(), 0.01\n",
    "\n",
    "    df_act_jan= df_actual[\n",
    "        (df_actual[\"Date\"]>= start_jan)&\n",
    "        (df_actual[\"Date\"]<= end_jan)\n",
    "    ].copy()\n",
    "    df_act_jan.rename(columns={\"Date\":\"ForecastDate\",\"Price\":\"ActualPrice\"}, inplace=True)\n",
    "\n",
    "    df_merge= pd.merge(\n",
    "        df_jan[[\"Model\",\"BaseDate\",\"ForecastDate\",\"Pred_Price_unscaled\"]],\n",
    "        df_act_jan[[\"ForecastDate\",\"ActualPrice\"]],\n",
    "        on=\"ForecastDate\", how=\"inner\"\n",
    "    )\n",
    "    df_merge[\"AbsError\"] = (df_merge[\"Pred_Price_unscaled\"] - df_merge[\"ActualPrice\"]).abs()\n",
    "\n",
    "    # mae\n",
    "    g_mae= df_merge.groupby([\"Model\",\"BaseDate\"])[\"AbsError\"].mean().reset_index()\n",
    "    g_mae.rename(columns={\"AbsError\":\"mae_jan\"}, inplace=True)\n",
    "\n",
    "    # slope alignment => quick polynomial fit\n",
    "    def poly_slope_diff(sub):\n",
    "        sub= sub.sort_values(\"ForecastDate\")\n",
    "        xvals= np.arange(len(sub))\n",
    "        yvals= sub[\"Pred_Price_unscaled\"].values\n",
    "        if len(yvals)< 4:\n",
    "            return 0.5\n",
    "        coefs= polyfit(xvals,yvals,deg=2)\n",
    "        y_pred= polyval(xvals, coefs)\n",
    "        resid= yvals- y_pred\n",
    "        sse= np.mean(resid**2)\n",
    "        return float(np.exp(-sse))\n",
    "\n",
    "    def group_slope_func(grp):\n",
    "        grp[\"slopeAlign_jan\"]= poly_slope_diff(grp)\n",
    "        return grp\n",
    "\n",
    "    df_slope= df_merge.groupby([\"Model\",\"BaseDate\"], group_keys=False).apply(group_slope_func)\n",
    "    df_slope_agg= df_slope.groupby([\"Model\",\"BaseDate\"])[\"slopeAlign_jan\"].mean().reset_index()\n",
    "\n",
    "    # real vol\n",
    "    df_act_jan_sorted= df_act_jan.drop_duplicates(\"ForecastDate\").copy()\n",
    "    df_act_jan_sorted.sort_values(\"ForecastDate\", inplace=True)\n",
    "    df_act_jan_sorted[\"DayChange\"]= df_act_jan_sorted[\"ActualPrice\"].diff()\n",
    "    real_vol= df_act_jan_sorted[\"DayChange\"].std(skipna=True)\n",
    "    if pd.isna(real_vol) or real_vol< 1e-9:\n",
    "        real_vol= 0.01\n",
    "\n",
    "    def mini_volatility(sub):\n",
    "        sub= sub.sort_values(\"ForecastDate\")\n",
    "        sub[\"DayChange\"]= sub[\"Pred_Price_unscaled\"].diff()\n",
    "        vol_= sub[\"DayChange\"].std(skipna=True)\n",
    "        if pd.isna(vol_):\n",
    "            vol_= 0.01\n",
    "        return vol_\n",
    "\n",
    "    g_vol= df_merge.groupby([\"Model\",\"BaseDate\"]).apply(mini_volatility).reset_index()\n",
    "    g_vol.rename(columns={0:\"predVol_jan\"}, inplace=True)\n",
    "    def vol_align(v):\n",
    "        ratio= v/ real_vol\n",
    "        return np.exp(-abs(ratio-1.0))\n",
    "    g_vol[\"volAlign_jan\"]= g_vol[\"predVol_jan\"].apply(vol_align)\n",
    "\n",
    "    mini_models= pd.merge(g_mae, df_slope_agg, on=[\"Model\",\"BaseDate\"], how=\"left\")\n",
    "    mini_models= pd.merge(mini_models, g_vol[[\"Model\",\"BaseDate\",\"predVol_jan\",\"volAlign_jan\"]],\n",
    "                          on=[\"Model\",\"BaseDate\"], how=\"left\")\n",
    "\n",
    "    mae_min= mini_models[\"mae_jan\"].min()\n",
    "    mae_max= mini_models[\"mae_jan\"].max()\n",
    "    if np.isclose(mae_min, mae_max):\n",
    "        mini_models[\"baseAcc\"]= 1.0\n",
    "    else:\n",
    "        mini_models[\"baseAcc\"]= 1.0 - ((mini_models[\"mae_jan\"]- mae_min)/(mae_max- mae_min))\n",
    "\n",
    "    return mini_models, real_vol\n",
    "\n",
    "def compute_super_advanced_moe(final_rolling_fcst, df_actual):\n",
    "    \"\"\"\n",
    "    Weighted \"Mixture of Experts\" for entire [2024-12-01..2025-01-31].\n",
    "    If no MoE model is loaded from disk, we create one by weighting predictions\n",
    "    from other models found in final_rolling_fcst.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== SUPER ADVANCED MOE w/ Polynomials & Voting ===\\n\")\n",
    "    mini_models, real_vol= build_mini_models_table(final_rolling_fcst, df_actual)\n",
    "    if mini_models.empty:\n",
    "        print(\"No january data => fallback => uniform daily average\")\n",
    "        return pd.DataFrame(columns=[\"ForecastDate\",\"MOE_Price\"])\n",
    "\n",
    "    mm_dict={}\n",
    "    for i, row in mini_models.iterrows():\n",
    "        mm_dict[(row[\"Model\"], row[\"BaseDate\"])] = {\n",
    "            \"baseAcc\": row[\"baseAcc\"],\n",
    "            \"slopeAlign_jan\": row[\"slopeAlign_jan\"],\n",
    "            \"volAlign_jan\": row[\"volAlign_jan\"]\n",
    "        }\n",
    "\n",
    "    start_moe= pd.to_datetime(\"2024-12-01\")\n",
    "    end_moe=   pd.to_datetime(\"2025-01-31\")\n",
    "    all_days=  pd.date_range(start_moe, end_moe, freq=\"D\")\n",
    "\n",
    "    records=[]\n",
    "    for d in all_days:\n",
    "        df_d= final_rolling_fcst[ final_rolling_fcst[\"ForecastDate\"]== d ].copy()\n",
    "        if df_d.empty:\n",
    "            continue\n",
    "\n",
    "        d_prev= d - pd.Timedelta(days=1)\n",
    "        ups= 0\n",
    "        downs= 0\n",
    "        total= 0\n",
    "        w_sums= 0.0\n",
    "        weighted_price= 0.0\n",
    "\n",
    "        for idx, row in df_d.iterrows():\n",
    "            m= row[\"Model\"]\n",
    "            bD= row[\"BaseDate\"]\n",
    "            if (m,bD) not in mm_dict:\n",
    "                continue\n",
    "\n",
    "            baseAcc= mm_dict[(m,bD)][\"baseAcc\"]\n",
    "            slopeA= mm_dict[(m,bD)][\"slopeAlign_jan\"]\n",
    "            volA=   mm_dict[(m,bD)][\"volAlign_jan\"]\n",
    "\n",
    "            df_dprev= final_rolling_fcst[\n",
    "                (final_rolling_fcst[\"Model\"]==m)&\n",
    "                (final_rolling_fcst[\"BaseDate\"]==bD)&\n",
    "                (final_rolling_fcst[\"ForecastDate\"]==d_prev)\n",
    "            ]\n",
    "            if df_dprev.empty:\n",
    "                dirFactor=1.0\n",
    "            else:\n",
    "                pd_today= row[\"Pred_Price_unscaled\"]\n",
    "                pd_yest=  df_dprev[\"Pred_Price_unscaled\"].values[0]\n",
    "                if pd_today> pd_yest:\n",
    "                    ups += 1\n",
    "                    dirFactor=1.1\n",
    "                elif pd_today< pd_yest:\n",
    "                    downs += 1\n",
    "                    dirFactor=0.9\n",
    "                else:\n",
    "                    dirFactor=1.0\n",
    "                total += 1\n",
    "\n",
    "            w= baseAcc*slopeA*volA* dirFactor\n",
    "            w_sums+= w\n",
    "            weighted_price+= w* row[\"Pred_Price_unscaled\"]\n",
    "\n",
    "        if np.isclose(w_sums,0.0):\n",
    "            day_moe= df_d[\"Pred_Price_unscaled\"].mean()\n",
    "        else:\n",
    "            day_moe= weighted_price/ w_sums\n",
    "\n",
    "        # If majority are up or down, nudge the final price\n",
    "        if total>0:\n",
    "            fraction_up= ups/ total\n",
    "            fraction_down= downs/ total\n",
    "            if fraction_up> 0.7:\n",
    "                day_moe*= 1.01\n",
    "            elif fraction_down> 0.7:\n",
    "                day_moe*= 0.99\n",
    "\n",
    "        # final mild noise\n",
    "        day_moe+= np.random.normal(0, day_moe*0.002)\n",
    "        records.append({\"ForecastDate\": d, \"MOE_Price\": day_moe})\n",
    "\n",
    "    df_moe= pd.DataFrame(records)\n",
    "    df_moe.sort_values(\"ForecastDate\", inplace=True)\n",
    "    return df_moe\n",
    "\n",
    "def reduce_duplicates(df_in):\n",
    "    agg_df = df_in.groupby([\"Model\",\"ForecastDate\"], as_index=False).agg({\n",
    "        \"Pred_Price_unscaled\":\"mean\"\n",
    "    })\n",
    "    return agg_df\n",
    "\n",
    "def combine_base_and_moe(final_rolling_fc, df_moe):\n",
    "    df_moe_cpy = df_moe.copy()\n",
    "    df_moe_cpy[\"Model\"] = \"MoE\"\n",
    "    df_moe_cpy[\"BaseDate\"] = pd.NaT\n",
    "    df_moe_cpy.rename(columns={\"MOE_Price\":\"Pred_Price_unscaled\"}, inplace=True)\n",
    "\n",
    "    df_base = final_rolling_fc[[\"Model\",\"ForecastDate\",\"Pred_Price_unscaled\"]].copy()\n",
    "    df_combined = pd.concat([df_base, df_moe_cpy], ignore_index=True)\n",
    "\n",
    "    df_no_dupes = reduce_duplicates(df_combined)\n",
    "    df_no_dupes.sort_values([\"Model\",\"ForecastDate\"], inplace=True, ignore_index=True)\n",
    "    return df_no_dupes\n",
    "\n",
    "###############################################################################\n",
    "# 5) Merge with Actual => Daily Differences\n",
    "###############################################################################\n",
    "def merge_with_actual_and_diff(df_all_models, df_actual):\n",
    "    df_actual_cpy = df_actual[[\"Date\",\"Price\"]].rename(columns={\"Price\":\"ActualPrice\"}).copy()\n",
    "\n",
    "    df_merged = pd.merge(\n",
    "        df_all_models,\n",
    "        df_actual_cpy,\n",
    "        left_on=\"ForecastDate\",\n",
    "        right_on=\"Date\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    df_merged.dropna(subset=[\"ActualPrice\"], inplace=True)\n",
    "    df_merged.drop(columns=[\"Date\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    df_merged.rename(columns={\"ForecastDate\":\"Date\"}, inplace=True)\n",
    "    df_merged[\"DailyDiff\"] = df_merged[\"Pred_Price_unscaled\"] - df_merged[\"ActualPrice\"]\n",
    "    df_merged[\"AbsError\"]  = df_merged[\"DailyDiff\"].abs()\n",
    "\n",
    "    df_merged.sort_values([\"Model\",\"Date\"], inplace=True)\n",
    "    df_merged.reset_index(drop=True, inplace=True)\n",
    "    return df_merged\n",
    "\n",
    "###############################################################################\n",
    "# 6) Evaluate Errors & Confidence => produce df_scores\n",
    "###############################################################################\n",
    "def evaluate_models_confidence(df_diff):\n",
    "    df_eval = df_diff[\n",
    "        (df_diff[\"Date\"]>= PLOT_START)&\n",
    "        (df_diff[\"Date\"]<= PLOT_END)\n",
    "    ].copy()\n",
    "    model_scores = {}\n",
    "    for m in df_eval[\"Model\"].unique():\n",
    "        dsub= df_eval[df_eval[\"Model\"]==m].copy()\n",
    "        if dsub.empty:\n",
    "            continue\n",
    "        cum_err= dsub[\"AbsError\"].sum()\n",
    "\n",
    "        dsub.sort_values(\"Date\", inplace=True)\n",
    "        dsub[\"ActualDiff\"] = dsub[\"ActualPrice\"].diff()\n",
    "        dsub[\"PredDiff\"]   = dsub[\"Pred_Price_unscaled\"].diff()\n",
    "        dsub_valid= dsub.dropna(subset=[\"ActualDiff\",\"PredDiff\"]).copy()\n",
    "\n",
    "        def sign_match(a,b):\n",
    "            if a==0 and b==0:\n",
    "                return True\n",
    "            return np.sign(a)== np.sign(b)\n",
    "        dsub_valid[\"TrendMatch\"] = dsub_valid.apply(\n",
    "            lambda r: 1 if sign_match(r[\"ActualDiff\"], r[\"PredDiff\"]) else 0,\n",
    "            axis=1\n",
    "        )\n",
    "        trend_rate= dsub_valid[\"TrendMatch\"].mean() if len(dsub_valid)>0 else 0.0\n",
    "\n",
    "        model_scores[m] = {\n",
    "            \"cumulative_error\": cum_err,\n",
    "            \"trend_matching_rate\": trend_rate\n",
    "        }\n",
    "\n",
    "    df_scores= pd.DataFrame.from_dict(model_scores, orient=\"index\").reset_index()\n",
    "    df_scores.rename(columns={\"index\":\"Model\"}, inplace=True)\n",
    "\n",
    "    if not df_scores.empty:\n",
    "        ce_min= df_scores[\"cumulative_error\"].min()\n",
    "        ce_max= df_scores[\"cumulative_error\"].max()\n",
    "        if not np.isclose(ce_min, ce_max):\n",
    "            df_scores[\"normalized_error_score\"] = 1.0 - (df_scores[\"cumulative_error\"]-ce_min)/(ce_max-ce_min)\n",
    "        else:\n",
    "            df_scores[\"normalized_error_score\"] = 1.0\n",
    "\n",
    "        # Weighted combo => 70% trend, 30% error\n",
    "        df_scores[\"combined_confidence\"] = 0.7*df_scores[\"trend_matching_rate\"] + 0.3*df_scores[\"normalized_error_score\"]\n",
    "        df_scores.sort_values(\"combined_confidence\", ascending=False, inplace=True)\n",
    "        df_scores.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_scores\n",
    "\n",
    "def select_top5_including_moe(df_scores):\n",
    "    \"\"\"\n",
    "    Always include the 'MoE' model in top-5, plus 4 others with highest confidence.\n",
    "    Then re-normalize their 'combined_confidence' so sum=1.\n",
    "    \"\"\"\n",
    "    if df_scores.empty:\n",
    "        return df_scores.copy()\n",
    "\n",
    "    if \"MoE\" not in df_scores[\"Model\"].values:\n",
    "        # If MoE not present at all, just pick top-5\n",
    "        top5 = df_scores.head(5).copy()\n",
    "        csum = top5[\"combined_confidence\"].sum()\n",
    "        if csum>0:\n",
    "            top5[\"combined_confidence\"] /= csum\n",
    "        return top5\n",
    "\n",
    "    # Otherwise, separate out MoE row\n",
    "    df_sorted = df_scores.sort_values(\"combined_confidence\", ascending=False).reset_index(drop=True)\n",
    "    moe_row = df_sorted[df_sorted[\"Model\"]==\"MoE\"].iloc[0]\n",
    "    df_no_moe = df_sorted[df_sorted[\"Model\"]!=\"MoE\"].copy()\n",
    "\n",
    "    # pick top 4 from the others\n",
    "    top4 = df_no_moe.head(4).copy()\n",
    "    top5 = pd.concat([top4, pd.DataFrame([moe_row])], ignore_index=True)\n",
    "    top5.sort_values(\"combined_confidence\", ascending=False, inplace=True, ignore_index=True)\n",
    "\n",
    "    # re-normalize\n",
    "    csum = top5[\"combined_confidence\"].sum()\n",
    "    if csum> 0:\n",
    "        top5[\"combined_confidence\"] /= csum\n",
    "\n",
    "    return top5\n",
    "\n",
    "###############################################################################\n",
    "# 7) Polynomial Fit for the forced top-5 + Actual in [2025-01-01..2025-01-31]\n",
    "###############################################################################\n",
    "def polynomial_fit_top5_models(df_diff, df_top5, max_degree=70):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of best polynomials => {model: {degree, mse, coeffs}}\n",
    "    plus a merged DataFrame with [Date, Actual, top5-model columns, mdates_num].\n",
    "    \"\"\"\n",
    "    top5_models = df_top5[\"Model\"].tolist()\n",
    "\n",
    "    # Filter the underlying df_diff to Jan 2025 range\n",
    "    df_test = df_diff[\n",
    "        (df_diff[\"Date\"]>= PLOT_START)&\n",
    "        (df_diff[\"Date\"]<= PLOT_END)\n",
    "    ].copy()\n",
    "    if df_test.empty:\n",
    "        print(\"No data in test range for polynomial fitting.\")\n",
    "        return {}, pd.DataFrame()\n",
    "\n",
    "    # Pivot => each model's predicted price in separate column\n",
    "    df_pivot = df_test.pivot_table(\n",
    "        index=\"Date\", columns=\"Model\", values=\"Pred_Price_unscaled\", aggfunc=\"mean\"\n",
    "    ).reset_index()\n",
    "\n",
    "    # Also keep actual\n",
    "    df_actual_only = df_test.drop_duplicates(\"Date\")[[\"Date\",\"ActualPrice\"]].copy()\n",
    "    df_merged = pd.merge(df_pivot, df_actual_only, on=\"Date\", how=\"inner\")\n",
    "    df_merged.rename(columns={\"ActualPrice\":\"Actual\"}, inplace=True)\n",
    "    df_merged.sort_values(\"Date\", inplace=True)\n",
    "    df_merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # We want columns: \"Actual\" + top5\n",
    "    columns_to_fit = [\"Actual\"] + top5_models\n",
    "    df_merged = df_merged[[\"Date\"] + columns_to_fit].dropna()\n",
    "\n",
    "    if df_merged.empty:\n",
    "        print(\"No overlapping data for polynomial fit.\")\n",
    "        return {}, pd.DataFrame()\n",
    "\n",
    "    df_merged[\"mdates_num\"] = mdates.date2num(df_merged[\"Date\"])\n",
    "    n_points = len(df_merged)\n",
    "    idx = np.arange(n_points)\n",
    "    # simple train/val split: 3/4 train, 1/4 val\n",
    "    val_mask = (idx % 4 == 0)\n",
    "    train_mask = ~val_mask\n",
    "\n",
    "    best_poly = {}\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    for col in columns_to_fit:\n",
    "        x_train= df_merged.loc[train_mask,\"mdates_num\"].values\n",
    "        y_train= df_merged.loc[train_mask,col].values\n",
    "        x_val= df_merged.loc[val_mask,\"mdates_num\"].values\n",
    "        y_val= df_merged.loc[val_mask,col].values\n",
    "\n",
    "        best_deg = None\n",
    "        best_mse = float(\"inf\")\n",
    "        best_coefs= None\n",
    "        for d_ in range(1, max_degree+1):\n",
    "            if len(x_train)<= d_:\n",
    "                break\n",
    "            coefs = np.polyfit(x_train, y_train, d_)\n",
    "            y_pred_val= np.polyval(coefs, x_val)\n",
    "            mse_ = mean_squared_error(y_val, y_pred_val)\n",
    "            if mse_ < best_mse:\n",
    "                best_mse= mse_\n",
    "                best_deg= d_\n",
    "                best_coefs= coefs\n",
    "\n",
    "        if best_coefs is None:\n",
    "            best_poly[col] = {\n",
    "                \"degree\": 0,\n",
    "                \"mse\": 9999.0,\n",
    "                \"coeffs\": [0.0]\n",
    "            }\n",
    "        else:\n",
    "            best_poly[col] = {\n",
    "                \"degree\": best_deg,\n",
    "                \"mse\": best_mse,\n",
    "                \"coeffs\": best_coefs\n",
    "            }\n",
    "\n",
    "    return best_poly, df_merged\n",
    "\n",
    "def format_poly(coefs):\n",
    "    terms=[]\n",
    "    deg= len(coefs)-1\n",
    "    for i,c in enumerate(coefs):\n",
    "        p= deg-i\n",
    "        sign_part= f\"{c:+.4e}\"\n",
    "        if p>1:\n",
    "            terms.append(f\"{sign_part}*x^{p}\")\n",
    "        elif p==1:\n",
    "            terms.append(f\"{sign_part}*x\")\n",
    "        else:\n",
    "            terms.append(f\"{sign_part}\")\n",
    "    expr= \"\".join(terms).replace(\"+-\",\"-\")\n",
    "    if expr.startswith(\"+\"):\n",
    "        expr= expr[1:]\n",
    "    return expr\n",
    "\n",
    "def plot_polynomial_fits(df_merged, best_poly, top5_models):\n",
    "    columns_to_fit = [\"Actual\"] + top5_models\n",
    "    if df_merged.empty or not best_poly:\n",
    "        print(\"No data to plot for polynomial fits.\")\n",
    "        return\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n=== Best Polynomial Fitting (Forced Top-5 + Actual) ===\")\n",
    "    for col in columns_to_fit:\n",
    "        info = best_poly[col]\n",
    "        expr = format_poly(info[\"coeffs\"])\n",
    "        print(f\"[{col}] => Deg={info['degree']}  ValMSE={info['mse']:.6f}\")\n",
    "        print(f\"    y = {expr}\\n\")\n",
    "\n",
    "    # Plot polynomials\n",
    "    plt.figure(figsize=(12,7))\n",
    "    color_list= [\"black\",\"red\",\"blue\",\"green\",\"orange\",\"purple\",\"pink\",\"brown\",\"olive\",\"cyan\"]\n",
    "    color_map= {}\n",
    "    color_map[\"Actual\"] = \"black\"\n",
    "    for i,m in enumerate(top5_models):\n",
    "        if i < len(color_list)-1:\n",
    "            color_map[m] = color_list[i+1]\n",
    "        else:\n",
    "            color_map[m] = \"gray\"\n",
    "\n",
    "    # Distinguish train vs. val\n",
    "    n_points = len(df_merged)\n",
    "    idx= np.arange(n_points)\n",
    "    val_mask = (idx % 4 == 0)\n",
    "    train_mask = ~val_mask\n",
    "\n",
    "    plt.scatter(df_merged.loc[train_mask,\"Date\"], df_merged.loc[train_mask,\"Actual\"],\n",
    "                color=\"black\", marker=\"o\", label=\"Actual (Train)\")\n",
    "    plt.scatter(df_merged.loc[val_mask,\"Date\"], df_merged.loc[val_mask,\"Actual\"],\n",
    "                color=\"gray\", marker=\"x\", label=\"Actual (Val)\")\n",
    "\n",
    "    x_dense = np.linspace(df_merged[\"mdates_num\"].min(), df_merged[\"mdates_num\"].max(), 300)\n",
    "    x_dense_dates= [mdates.num2date(xx) for xx in x_dense]\n",
    "\n",
    "    for col in columns_to_fit:\n",
    "        coefs= best_poly[col][\"coeffs\"]\n",
    "        y_dense= np.polyval(coefs, x_dense)\n",
    "        c= color_map.get(col,\"gray\")\n",
    "        plt.plot(x_dense_dates, y_dense, color=c, lw=2, label=f\"{col} (Deg={best_poly[col]['degree']})\")\n",
    "\n",
    "    plt.title(\"Polynomial Fits (Top-5 + Actual)\\n(2025-01-01..2025-01-31)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend(prop={'size':8})\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Derivatives\n",
    "    plt.figure(figsize=(12,7))\n",
    "    for col in columns_to_fit:\n",
    "        coefs= best_poly[col][\"coeffs\"]\n",
    "        d_coefs= np.polyder(coefs)\n",
    "        y_deriv= np.polyval(d_coefs, x_dense)\n",
    "        c= color_map.get(col,\"gray\")\n",
    "        plt.plot(x_dense_dates, y_deriv, color=c, lw=2,\n",
    "                 label=f\"{col} Deriv (Deg={best_poly[col]['degree']-1})\")\n",
    "\n",
    "    plt.title(\"Polynomial Derivatives (Top-5 + Actual)\\n(2025-01-01..2025-01-31)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"d(Price)/d(Time)\")\n",
    "    plt.legend(prop={'size':8})\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "####################   Actual Implementation of the Code   ####################\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "# A) Load & Scale\n",
    "    df_scaled, df_complete_all, scaler = load_and_scale_data(\n",
    "        \"Silver Futures Historical Data.csv\",\n",
    "        \"Silver Futures Historical Data_Complete.csv\"\n",
    "    )\n",
    "\n",
    "    # B) Rolling Forecasts for [2024-12-01 .. 2025-01-02]\n",
    "    all_model_types = [\n",
    "        \"LSTM\",\"GRU\",\"RNN\",\"CNN\",\"Transformer\",\"N-BEATS\",\"N-HITS\",\"SVM\",\"Boost\",\"MoE\"\n",
    "    ]\n",
    "    final_rolling_fc= rolling_train_validate_predict_moe(\n",
    "        df_full= df_complete_all,\n",
    "        scaler_obj= scaler,\n",
    "        model_types= all_model_types,\n",
    "        start_train_cutoff= train_cutoff,\n",
    "        start_val_cutoff=   val_cutoff,\n",
    "        start_pred= pd.to_datetime(\"2024-12-01\"),\n",
    "        end_pred=   pd.to_datetime(\"2025-01-02\"),\n",
    "        horizon_days= 30\n",
    "    )\n",
    "    print(\"\\n=== HEAD of final_rolling_fc ===\")\n",
    "    print(final_rolling_fc.head(10))\n",
    "\n",
    "    # C) If MoE wasn't loaded, compute it via Super Advanced approach\n",
    "    if not any(final_rolling_fc[\"Model\"]==\"MoE\"):\n",
    "        df_moe= compute_super_advanced_moe(final_rolling_fc, df_complete_all)\n",
    "        df_rolled_all= combine_base_and_moe(final_rolling_fc, df_moe)\n",
    "    else:\n",
    "        df_rolled_all= final_rolling_fc.copy()\n",
    "\n",
    "    # D) Quick Plot of All Models in Jan 2025\n",
    "    plot_predicted_price_all_models(\n",
    "        df_rolled_all,\n",
    "        from_d= PLOT_START,\n",
    "        to_d=   PLOT_END\n",
    "    )\n",
    "\n",
    "    # E) Merge w/ actual => daily differences\n",
    "    df_diff_all= merge_with_actual_and_diff(df_rolled_all, df_complete_all)\n",
    "    plot_daily_differences_all_models(df_diff_all)\n",
    "\n",
    "    # F) Evaluate errors => df_scores => forcibly select top-5 (MoE included)\n",
    "    df_scores= evaluate_models_confidence(df_diff_all)\n",
    "    print(\"\\n=== Model Scores (Incl. MoE) for Jan 2025 ===\")\n",
    "    print(df_scores)\n",
    "\n",
    "    df_top5_incl_moe= select_top5_including_moe(df_scores)\n",
    "    print(\"\\n=== Forced Top-5 (MoE + 4) with Re-Normalized Confidence ===\")\n",
    "    print(df_top5_incl_moe)\n",
    "\n",
    "    # Convert that combined_confidence into a simple static_conf dict\n",
    "    # (Well treat these as the \"static\" portion for the dynamic weighting)\n",
    "    static_conf = {}\n",
    "    for i,row in df_top5_incl_moe.iterrows():\n",
    "        static_conf[row[\"Model\"]] = row[\"combined_confidence\"]\n",
    "\n",
    "    # G) Polynomial Fit for these top-5\n",
    "    best_poly, df_test_poly = polynomial_fit_top5_models(df_diff_all, df_top5_incl_moe, max_degree=70)\n",
    "    plot_polynomial_fits(df_test_poly, best_poly, df_top5_incl_moe[\"Model\"].tolist())\n",
    "\n",
    "###############################################################################\n",
    "# 8) Confidence-Based Voting Trading Strategy (Adapted to MoE + 4 Others)\n",
    "###############################################################################\n",
    "def re_normalize(d):\n",
    "    s = sum(d.values())\n",
    "    if s > 1e-9:\n",
    "        for k in d:\n",
    "            d[k] /= s\n",
    "    else:\n",
    "        n = len(d)\n",
    "        for k in d:\n",
    "            d[k] = 1.0 / n\n",
    "\n",
    "def confidence_based_voting_trading(df_test_poly, best_poly_info, best_model_names, static_conf):\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      - df_test_poly: a DataFrame with columns [\"Date\",\"Actual\",\"mdates_num\"] for Jan 2025\n",
    "      - best_poly_info: dict -> best_poly_info[model][\"coeffs\"]\n",
    "      - best_model_names: list of the 5 models (MoE + 4 others)\n",
    "      - static_conf: dictionary of base confidences for these 5 models\n",
    "\n",
    "    Returns:\n",
    "      df_sim: final DataFrame with signals, capital, etc.\n",
    "      trade_log: list of trades\n",
    "      final_conf_history: daily record of each model's composite confidence\n",
    "    \"\"\"\n",
    "    df_test_poly = df_test_poly.sort_values(\"Date\").reset_index(drop=True)\n",
    "    n_days = len(df_test_poly)\n",
    "\n",
    "    # 1) Compute each model's polynomial predicted prices\n",
    "    model_pred_prices = {}\n",
    "    for model in best_model_names:\n",
    "        c = best_poly_info[model][\"coeffs\"]\n",
    "        model_pred_prices[model] = np.polyval(c, df_test_poly[\"mdates_num\"])\n",
    "\n",
    "    # 2) Setup dynamic confidence\n",
    "    dyn_fit_conf    = {m: 0.5 for m in best_model_names}\n",
    "    dyn_grad_conf   = {m: 0.5 for m in best_model_names}\n",
    "    dyn_static_conf = {m: static_conf[m] for m in best_model_names}\n",
    "\n",
    "    # Weighted formula: (10% static, 30% fit, 60% gradient) => can be changed\n",
    "    w_static = 0.2\n",
    "    w_fit    = 0.3\n",
    "    w_grad   = 0.5\n",
    "\n",
    "    # Step sizes\n",
    "    alpha_fit_up     = 0.2\n",
    "    alpha_fit_down   = 0.2\n",
    "    alpha_grad_up    = 0.3\n",
    "    alpha_grad_down  = 0.4\n",
    "    alpha_static_up  = 0.1\n",
    "    alpha_static_down= 0.1\n",
    "\n",
    "    fit_threshold    = 1.0\n",
    "    static_threshold = 1.0\n",
    "\n",
    "    consecutive_all_wrong = 0\n",
    "    MAX_CONSECUTIVE_ALL_WRONG = 2\n",
    "    threshold = 0.6\n",
    "\n",
    "    signals = np.zeros(n_days, dtype=int)\n",
    "    final_conf_history = pd.DataFrame(index=range(n_days), columns=best_model_names)\n",
    "\n",
    "    def abs_err(m, i):\n",
    "        return abs(model_pred_prices[m][i] - df_test_poly.loc[i, \"Actual\"])\n",
    "\n",
    "    for i in range(n_days):\n",
    "        if i > 0:\n",
    "            # 1) Update dynamic fitting confidence\n",
    "            for m in best_model_names:\n",
    "                pred_change = model_pred_prices[m][i] - model_pred_prices[m][i - 1]\n",
    "                actual_change = df_test_poly.loc[i, \"Actual\"] - df_test_poly.loc[i - 1, \"Actual\"]\n",
    "                err = abs(pred_change - actual_change)\n",
    "                if err < fit_threshold:\n",
    "                    dyn_fit_conf[m] += alpha_fit_up\n",
    "                else:\n",
    "                    dyn_fit_conf[m] -= alpha_fit_down\n",
    "                dyn_fit_conf[m] = max(0, min(1, dyn_fit_conf[m]))\n",
    "\n",
    "            # 2) Update dynamic gradient confidence\n",
    "            for m in best_model_names:\n",
    "                model_sign = np.sign(model_pred_prices[m][i] - model_pred_prices[m][i - 1])\n",
    "                actual_sign = np.sign(df_test_poly.loc[i, \"Actual\"] - df_test_poly.loc[i - 1, \"Actual\"])\n",
    "                if model_sign == actual_sign:\n",
    "                    dyn_grad_conf[m] += alpha_grad_up\n",
    "                else:\n",
    "                    dyn_grad_conf[m] -= alpha_grad_down\n",
    "                dyn_grad_conf[m] = max(0, min(1, dyn_grad_conf[m]))\n",
    "\n",
    "            # 3) Update dynamic static confidence\n",
    "            for m in best_model_names:\n",
    "                error_static = abs_err(m, i)\n",
    "                if error_static < static_threshold:\n",
    "                    dyn_static_conf[m] += alpha_static_up\n",
    "                else:\n",
    "                    dyn_static_conf[m] -= alpha_static_down\n",
    "                dyn_static_conf[m] = max(0, min(1, dyn_static_conf[m]))\n",
    "\n",
    "        # Merge components\n",
    "        daily_conf = {}\n",
    "        for m in best_model_names:\n",
    "            daily_conf[m] = (w_static * dyn_static_conf[m] +\n",
    "                             w_fit    * dyn_fit_conf[m] +\n",
    "                             w_grad   * dyn_grad_conf[m])\n",
    "        re_normalize(daily_conf)\n",
    "\n",
    "        # Record daily composite confidence\n",
    "        for m in best_model_names:\n",
    "            final_conf_history.loc[i, m] = daily_conf[m]\n",
    "\n",
    "        # Weighted vote\n",
    "        if i==0:\n",
    "            # No movement from i-1 to i if i=0\n",
    "            signals[i] = 0\n",
    "            continue\n",
    "\n",
    "        vote_sum = 0.0\n",
    "        correct_count = 0\n",
    "        for m in best_model_names:\n",
    "            m_sign = np.sign(model_pred_prices[m][i] - model_pred_prices[m][i - 1])\n",
    "            a_sign = np.sign(df_test_poly.loc[i, \"Actual\"] - df_test_poly.loc[i - 1, \"Actual\"])\n",
    "            if m_sign == a_sign:\n",
    "                correct_count += 1\n",
    "            vote_sum += daily_conf[m] * m_sign\n",
    "\n",
    "        if correct_count == 0:\n",
    "            consecutive_all_wrong += 1\n",
    "        else:\n",
    "            consecutive_all_wrong = 0\n",
    "\n",
    "        if vote_sum >= threshold:\n",
    "            signals[i] = 1\n",
    "        elif vote_sum <= -threshold:\n",
    "            signals[i] = -1\n",
    "        else:\n",
    "            signals[i] = 0\n",
    "\n",
    "        # Optional override\n",
    "        if consecutive_all_wrong > MAX_CONSECUTIVE_ALL_WRONG:\n",
    "            signals[i] = -signals[i]\n",
    "\n",
    "    df_sim = df_test_poly.copy()\n",
    "    df_sim[\"Signal\"] = signals\n",
    "    return df_sim, final_conf_history\n",
    "\n",
    "###############################################################################\n",
    "# 9) Piecewise Offset-Based Trading Simulation\n",
    "###############################################################################\n",
    "def get_plot_segments(df, signal_col=\"Signal\"):\n",
    "    \"\"\"\n",
    "    Identify contiguous segments with the same signal.\n",
    "    Returns a list of tuples: (start_index, end_index, signal_value).\n",
    "    \"\"\"\n",
    "    segs = []\n",
    "    start_idx = 0\n",
    "    curr_signal = df.loc[0, signal_col]\n",
    "    for idx in range(1, len(df)):\n",
    "        if df.loc[idx, signal_col] != curr_signal:\n",
    "            segs.append((start_idx, idx - 1, curr_signal))\n",
    "            start_idx = idx\n",
    "            curr_signal = df.loc[idx, signal_col]\n",
    "    segs.append((start_idx, len(df) - 1, curr_signal))\n",
    "    return segs\n",
    "\n",
    "def piecewise_trading_simulation_offset(df_sim):\n",
    "    \"\"\"\n",
    "    Simulate trading using daily signals with an offset-based approach.\n",
    "    Start with $100 capital. For any multi-day segment (Long/Short), we realize\n",
    "    PnL at the final day of that segment. Single-day segments are effectively Flat.\n",
    "    \"\"\"\n",
    "    segments = get_plot_segments(df_sim, \"Signal\")\n",
    "    capital = 100.0\n",
    "    cap_arr = np.zeros(len(df_sim))\n",
    "    trade_log = []\n",
    "\n",
    "    for (start, end, sig) in segments:\n",
    "        start_date = df_sim.loc[start, \"Date\"]\n",
    "        end_date   = df_sim.loc[end, \"Date\"]\n",
    "        seg_days   = end - start + 1\n",
    "\n",
    "        # keep capital for entire segment\n",
    "        for i in range(start, end+1):\n",
    "            cap_arr[i] = capital\n",
    "\n",
    "        if seg_days < 2:\n",
    "            # 1-day => ignore\n",
    "            trade_log.append({\n",
    "                \"Signal\": \"Flat(ignored)\",\n",
    "                \"StartDate\": start_date,\n",
    "                \"EndDate\": end_date,\n",
    "                \"Days\": seg_days,\n",
    "                \"EntryPrice\": df_sim.loc[start, \"Actual\"],\n",
    "                \"ExitPrice\": df_sim.loc[end, \"Actual\"],\n",
    "                \"AbsDiff\": 0.0,\n",
    "                \"SegmentReturn(%)\": 0.0,\n",
    "                \"EndCapital\": capital\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        if sig == 0:\n",
    "            # Flat\n",
    "            trade_log.append({\n",
    "                \"Signal\": \"Flat\",\n",
    "                \"StartDate\": start_date,\n",
    "                \"EndDate\": end_date,\n",
    "                \"Days\": seg_days,\n",
    "                \"EntryPrice\": df_sim.loc[start, \"Actual\"],\n",
    "                \"ExitPrice\": df_sim.loc[end, \"Actual\"],\n",
    "                \"AbsDiff\": abs(df_sim.loc[end, \"Actual\"] - df_sim.loc[start, \"Actual\"]),\n",
    "                \"SegmentReturn(%)\": 0.0,\n",
    "                \"EndCapital\": capital\n",
    "            })\n",
    "        elif sig == 1:\n",
    "            # Long\n",
    "            entry_price = df_sim.loc[start, \"Actual\"]\n",
    "            exit_price  = df_sim.loc[end, \"Actual\"]\n",
    "            final_cap   = capital * (exit_price / entry_price)\n",
    "            seg_return  = final_cap / capital - 1.0\n",
    "            cap_arr[end] = final_cap\n",
    "            capital = final_cap\n",
    "            trade_log.append({\n",
    "                \"Signal\": \"Long\",\n",
    "                \"StartDate\": start_date,\n",
    "                \"EndDate\": end_date,\n",
    "                \"Days\": seg_days,\n",
    "                \"EntryPrice\": entry_price,\n",
    "                \"ExitPrice\": exit_price,\n",
    "                \"AbsDiff\": abs(exit_price - entry_price),\n",
    "                \"SegmentReturn(%)\": seg_return * 100.0,\n",
    "                \"EndCapital\": final_cap\n",
    "            })\n",
    "        else:\n",
    "            # Short\n",
    "            entry_price = df_sim.loc[start, \"Actual\"]\n",
    "            exit_price  = df_sim.loc[end, \"Actual\"]\n",
    "            final_cap   = capital * (entry_price / exit_price)\n",
    "            seg_return  = final_cap / capital - 1.0\n",
    "            cap_arr[end] = final_cap\n",
    "            capital = final_cap\n",
    "            trade_log.append({\n",
    "                \"Signal\": \"Short\",\n",
    "                \"StartDate\": start_date,\n",
    "                \"EndDate\": end_date,\n",
    "                \"Days\": seg_days,\n",
    "                \"EntryPrice\": entry_price,\n",
    "                \"ExitPrice\": exit_price,\n",
    "                \"AbsDiff\": abs(exit_price - entry_price),\n",
    "                \"SegmentReturn(%)\": seg_return * 100.0,\n",
    "                \"EndCapital\": final_cap\n",
    "            })\n",
    "\n",
    "    # Forward-fill capital if needed\n",
    "    last_cap= capital\n",
    "    for i in range(len(df_sim)):\n",
    "        if cap_arr[i]==0.0:\n",
    "            cap_arr[i]= last_cap\n",
    "        else:\n",
    "            last_cap= cap_arr[i]\n",
    "\n",
    "    df_sim[\"Capital\"] = cap_arr\n",
    "    df_sim[\"Cumulative_PnL\"] = df_sim[\"Capital\"] - 100.0\n",
    "    df_sim[\"Cumulative_Return\"] = (df_sim[\"Cumulative_PnL\"] / 100.0)* 100.0\n",
    "    return df_sim, trade_log\n",
    "\n",
    "def reindex_and_fill_signals(df_sim):\n",
    "    \"\"\"\n",
    "    Reindex df_sim to daily frequency. Forward-fill numeric columns.\n",
    "    Re-apply signals for any multi-day (>=2) segments.\n",
    "    \"\"\"\n",
    "    segments_raw= get_plot_segments(df_sim, \"Signal\")\n",
    "    valid_segments= []\n",
    "    for (s_idx, e_idx, sig_val) in segments_raw:\n",
    "        seg_days= e_idx- s_idx + 1\n",
    "        if seg_days>=2 and sig_val!=0:\n",
    "            start_date= df_sim.loc[s_idx,\"Date\"]\n",
    "            end_date=   df_sim.loc[e_idx,\"Date\"]\n",
    "            valid_segments.append((start_date,end_date,sig_val))\n",
    "\n",
    "    df_sim.set_index(\"Date\", inplace=True)\n",
    "    all_days= pd.date_range(start=df_sim.index.min(), end=df_sim.index.max(), freq=\"D\")\n",
    "    df_sim= df_sim.reindex(all_days)\n",
    "\n",
    "    # ffill\n",
    "    for col in [\"Capital\",\"Cumulative_PnL\",\"Cumulative_Return\",\"Actual\"]:\n",
    "        df_sim[col].ffill(inplace=True)\n",
    "\n",
    "    # reset signals => 0\n",
    "    df_sim[\"Signal\"]= 0\n",
    "    for (start_dt, end_dt, sig_val) in valid_segments:\n",
    "        mask= (df_sim.index>=start_dt)&(df_sim.index<= end_dt)\n",
    "        df_sim.loc[mask, \"Signal\"]= sig_val\n",
    "\n",
    "    df_sim.reset_index(inplace=True)\n",
    "    df_sim.rename(columns={\"index\":\"Date\"}, inplace=True)\n",
    "    return df_sim\n",
    "\n",
    "###############################################################################\n",
    "# 10) Plotting Routines\n",
    "###############################################################################\n",
    "def plot_predicted_price_all_models(df_rolled_all, from_d, to_d):\n",
    "    \"\"\"\n",
    "    Plot the average predicted price per model vs. date, in [from_d..to_d].\n",
    "    \"\"\"\n",
    "    df_sel= df_rolled_all[\n",
    "        (df_rolled_all[\"ForecastDate\"]>= from_d)&\n",
    "        (df_rolled_all[\"ForecastDate\"]<= to_d)\n",
    "    ].copy()\n",
    "    if df_sel.empty:\n",
    "        print(\"No data to plot in that date range.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    color_map = {\n",
    "        \"LSTM\":\"red\",\"GRU\":\"blue\",\"RNN\":\"green\",\"CNN\":\"orange\",\"EnhancedLSTM\":\"magenta\",\n",
    "        \"Transformer\":\"cyan\",\"N-BEATS\":\"brown\",\"N-HITS\":\"pink\",\"SVM\":\"olive\",\"Boost\":\"purple\",\"MoE\":\"black\"\n",
    "    }\n",
    "    for model_name in df_sel[\"Model\"].unique():\n",
    "        sub= df_sel[df_sel[\"Model\"]== model_name].copy()\n",
    "        sub_agg= sub.groupby(\"ForecastDate\", as_index=False)[\"Pred_Price_unscaled\"].mean()\n",
    "        c= color_map.get(model_name, \"gray\")\n",
    "        plt.plot(sub_agg[\"ForecastDate\"], sub_agg[\"Pred_Price_unscaled\"], color=c,\n",
    "                 label=model_name, alpha=0.8)\n",
    "\n",
    "    plt.title(f\"Predicted Price for All Models (incl. MoE): {from_d.date()} to {to_d.date()}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Predicted Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_daily_differences_all_models(df_diff):\n",
    "    \"\"\"\n",
    "    Plot daily differences (Pred - Actual) for all models in the final Jan-2025 range.\n",
    "    \"\"\"\n",
    "    df_plot= df_diff[\n",
    "        (df_diff[\"Date\"]>= PLOT_START)&\n",
    "        (df_diff[\"Date\"]<= PLOT_END)\n",
    "    ].copy()\n",
    "    if df_plot.empty:\n",
    "        print(\"No daily difference data in the test range.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    color_map = {\n",
    "        \"LSTM\":\"red\",\"GRU\":\"blue\",\"RNN\":\"green\",\"CNN\":\"orange\",\"EnhancedLSTM\":\"magenta\",\n",
    "        \"Transformer\":\"cyan\",\"N-BEATS\":\"brown\",\"N-HITS\":\"pink\",\"SVM\":\"olive\",\"Boost\":\"purple\",\"MoE\":\"black\"\n",
    "    }\n",
    "    for m in df_plot[\"Model\"].unique():\n",
    "        sub= df_plot[df_plot[\"Model\"]==m].copy()\n",
    "        sub_agg= sub.groupby(\"Date\", as_index=False)[\"DailyDiff\"].mean()\n",
    "        c= color_map.get(m,\"gray\")\n",
    "        plt.plot(sub_agg[\"Date\"], sub_agg[\"DailyDiff\"], label=m, color=c)\n",
    "\n",
    "    plt.axhline(0, color=\"black\", linestyle=\"--\", label=\"Zero Difference\")\n",
    "    plt.title(\"Daily Differences (Pred - Actual) for All Models (Incl. MoE)\\n(2025-01-01..2025-01-31)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Daily Difference\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_piecewise_results_daybyday(df_sim, trade_log):\n",
    "    \"\"\"\n",
    "    Stepwise PnL curve for transactions (offset settlement).\n",
    "    Color-coded by position signal: 0 => black, 1 => red, -1 => blue.\n",
    "    \"\"\"\n",
    "    print(\"===== Silver Futures Trade Log =====\")\n",
    "    for trd in trade_log:\n",
    "        print(\n",
    "            f\"{trd['Signal']:5s} | {trd['StartDate'].date()} -> {trd['EndDate'].date()} \"\n",
    "            f\"({trd['Days']} days) | \"\n",
    "            f\"Entry={trd['EntryPrice']:.2f}, Exit={trd['ExitPrice']:.2f} \"\n",
    "            f\"(AbsDiff={trd['AbsDiff']:.2f}) | \"\n",
    "            f\"Return: {trd['SegmentReturn(%)']:.2f}% | \"\n",
    "            f\"EndCapital={trd['EndCapital']:.2f}\"\n",
    "        )\n",
    "\n",
    "    final_cap = df_sim[\"Capital\"].iloc[-1]\n",
    "    final_pnl = df_sim[\"Cumulative_PnL\"].iloc[-1]\n",
    "    final_ret = df_sim[\"Cumulative_Return\"].iloc[-1]\n",
    "    print(\"\\n=== Final Results (Silver Futures) ===\")\n",
    "    print(f\"Final capital  = {final_cap:.2f}\")\n",
    "    print(f\"Final PnL      = {final_pnl:.2f}\")\n",
    "    print(f\"Final Return   = {final_ret:.2f}%\")\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "    color_map= {0:\"black\",1:\"red\",-1:\"blue\"}\n",
    "\n",
    "    n_rows= len(df_sim)\n",
    "    for i in range(1, n_rows):\n",
    "        sig_prev= df_sim.loc[i-1, \"Signal\"]\n",
    "        x_vals_horiz= [df_sim.loc[i-1, \"Date\"], df_sim.loc[i, \"Date\"]]\n",
    "        y_vals_horiz= [df_sim.loc[i-1, \"Capital\"], df_sim.loc[i-1, \"Capital\"]]\n",
    "        ax1.plot(x_vals_horiz, y_vals_horiz, color=color_map[sig_prev], linewidth=2)\n",
    "\n",
    "        x_vals_vert= [df_sim.loc[i, \"Date\"], df_sim.loc[i, \"Date\"]]\n",
    "        y_vals_vert= [df_sim.loc[i-1, \"Capital\"], df_sim.loc[i, \"Capital\"]]\n",
    "        ax1.plot(x_vals_vert, y_vals_vert, color=color_map[sig_prev], linewidth=2)\n",
    "\n",
    "    ax1.set_xlabel(\"Date\")\n",
    "    ax1.set_ylabel(\"Capital (PnL Curve)\", color=\"blue\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot the actual price on a secondary axis\n",
    "    ax2= ax1.twinx()\n",
    "    ax2.plot(df_sim[\"Date\"], df_sim[\"Actual\"], color=\"gray\", linestyle=\"--\", linewidth=2)\n",
    "    ax2.set_ylabel(\"Actual Price\", color=\"gray\")\n",
    "    ax2.tick_params(axis='y', labelcolor=\"gray\")\n",
    "\n",
    "    legend_elems = [\n",
    "        Line2D([0],[0], color='black', lw=2, label='Flat'),\n",
    "        Line2D([0],[0], color='red',   lw=2, label='Long'),\n",
    "        Line2D([0],[0], color='blue',  lw=2, label='Short')\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elems, loc=\"upper left\")\n",
    "    ax1.set_title(\"Day-by-Day Stepwise PnL Curve (Offset Settlement)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_piecewise_results_daybyday_pct(df_sim, trade_log):\n",
    "    \"\"\"\n",
    "    Plots the stepwise Cumulative Return (%) vs Date. \n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "    color_map= {0:\"black\",1:\"red\",-1:\"blue\"}\n",
    "\n",
    "    n_rows= len(df_sim)\n",
    "    for i in range(1, n_rows):\n",
    "        sig_prev= df_sim.loc[i-1, \"Signal\"]\n",
    "        x_vals_horiz= [df_sim.loc[i-1, \"Date\"], df_sim.loc[i, \"Date\"]]\n",
    "        y_vals_horiz= [df_sim.loc[i-1, \"Cumulative_Return\"], df_sim.loc[i-1, \"Cumulative_Return\"]]\n",
    "        ax1.plot(x_vals_horiz, y_vals_horiz, color=color_map[sig_prev], linewidth=2)\n",
    "\n",
    "        x_vals_vert= [df_sim.loc[i, \"Date\"], df_sim.loc[i, \"Date\"]]\n",
    "        y_vals_vert= [df_sim.loc[i-1, \"Cumulative_Return\"], df_sim.loc[i, \"Cumulative_Return\"]]\n",
    "        ax1.plot(x_vals_vert, y_vals_vert, color=color_map[sig_prev], linewidth=2)\n",
    "\n",
    "    ax1.set_xlabel(\"Date\")\n",
    "    ax1.set_ylabel(\"Cumulative Return (%)\", color=\"blue\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2= ax1.twinx()\n",
    "    ax2.plot(df_sim[\"Date\"], df_sim[\"Actual\"], color=\"gray\", linestyle=\"--\", linewidth=2)\n",
    "    ax2.set_ylabel(\"Actual Price\", color=\"gray\")\n",
    "    ax2.tick_params(axis='y', labelcolor=\"gray\")\n",
    "\n",
    "    ax1.set_title(\"Day-by-Day Stepwise PnL Percentage Returns\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# 11) Main Execution\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # H) Confidence-Based Voting Trading Strategy\n",
    "    #    (Using polynomial-based predictions from best_poly)\n",
    "    #    => Stepwise offset-based simulation\n",
    "    #    => Plots\n",
    "    if \"Actual\" not in best_poly:\n",
    "        # Ensure \"Actual\" polynomial is also stored (or do a fallback with direct values)\n",
    "        # But the above function does store an \"Actual\" entry. Just in case:\n",
    "        pass\n",
    "\n",
    "    # run dynamic approach\n",
    "    df_sim, final_conf_history= confidence_based_voting_trading(\n",
    "        df_test_poly= df_test_poly,\n",
    "        best_poly_info= best_poly,\n",
    "        best_model_names= df_top5_incl_moe[\"Model\"].tolist(),\n",
    "        static_conf= static_conf,\n",
    "    )\n",
    "\n",
    "    # I) Piecewise offset-based simulation\n",
    "    df_sim, trade_log= piecewise_trading_simulation_offset(df_sim)\n",
    "\n",
    "    # Reindex to daily freq & fill signals\n",
    "    df_sim= reindex_and_fill_signals(df_sim)\n",
    "\n",
    "    # Plot final results\n",
    "    plot_piecewise_results_daybyday(df_sim, trade_log)\n",
    "    plot_piecewise_results_daybyday_pct(df_sim, trade_log)\n",
    "\n",
    "    # J) (Optional) Plot final daily confidence using df_test_poly dates (which match final_conf_history)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    for m in df_top5_incl_moe[\"Model\"]:\n",
    "        plt.plot(df_test_poly[\"Date\"], final_conf_history[m], label=m)\n",
    "    plt.title(\"Final Daily Confidence Over Time (MoE + 4 Models)\\n(10% Static, 30% Fit, 60% Gradient)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Confidence Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RNN': 0.2382572411037236, 'N-BEATS': 0.19880338830037012, 'Transformer': 0.19632527954939616, 'N-HITS': 0.18637652369213656, 'MoE': 0.18023756735437355}\n",
      "['RNN', 'N-BEATS', 'Transformer', 'N-HITS', 'MoE']\n"
     ]
    }
   ],
   "source": [
    "# existing code\n",
    "df_sim, final_conf_history, w_static, w_fit, w_grad = confidence_based_voting_trading(\n",
    "    df_test_poly= df_test_poly,\n",
    "    best_poly_info= best_poly,\n",
    "    best_model_names= df_top5_incl_moe[\"Model\"].tolist(),\n",
    "    static_conf= static_conf\n",
    ")\n",
    "\n",
    "# I) Piecewise offset-based simulation\n",
    "print(static_conf)\n",
    "print( df_top5_incl_moe[\"Model\"].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
