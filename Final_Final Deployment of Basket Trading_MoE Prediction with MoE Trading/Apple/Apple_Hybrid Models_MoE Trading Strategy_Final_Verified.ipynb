{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rolling Day 2024-12-01 ===\n",
      "  train_cutoff=2024-07-01  val_cutoff=2024-11-30\n",
      "  Forecast next 30 days from 2024-12-01\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-01_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-01_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-01_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-01_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-01_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-01_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-01_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-01_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-01_V1.pkl\n",
      "[MoE] No saved file for 2024-12-01, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-02 ===\n",
      "  train_cutoff=2024-07-02  val_cutoff=2024-12-01\n",
      "  Forecast next 30 days from 2024-12-02\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-02_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-02_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-02_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-02_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-02_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-02_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-02_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-02_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-02_V1.pkl\n",
      "[MoE] No saved file for 2024-12-02, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-03 ===\n",
      "  train_cutoff=2024-07-03  val_cutoff=2024-12-02\n",
      "  Forecast next 30 days from 2024-12-03\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-03_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-03_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-03_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-03_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-03_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-03_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-03_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-03_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-03_V1.pkl\n",
      "[MoE] No saved file for 2024-12-03, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-04 ===\n",
      "  train_cutoff=2024-07-04  val_cutoff=2024-12-03\n",
      "  Forecast next 30 days from 2024-12-04\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-04_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-04_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-04_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-04_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-04_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-04_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-04_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-04_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-04_V1.pkl\n",
      "[MoE] No saved file for 2024-12-04, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-05 ===\n",
      "  train_cutoff=2024-07-05  val_cutoff=2024-12-04\n",
      "  Forecast next 30 days from 2024-12-05\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-05_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-05_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-05_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-05_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-05_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-05_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-05_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-05_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-05_V1.pkl\n",
      "[MoE] No saved file for 2024-12-05, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-06 ===\n",
      "  train_cutoff=2024-07-06  val_cutoff=2024-12-05\n",
      "  Forecast next 30 days from 2024-12-06\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-06_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-06_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-06_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-06_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-06_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-06_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-06_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-06_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-06_V1.pkl\n",
      "[MoE] No saved file for 2024-12-06, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-07 ===\n",
      "  train_cutoff=2024-07-07  val_cutoff=2024-12-06\n",
      "  Forecast next 30 days from 2024-12-07\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-07_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-07_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-07_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-07_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-07_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-07_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-07_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-07_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-07_V1.pkl\n",
      "[MoE] No saved file for 2024-12-07, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-08 ===\n",
      "  train_cutoff=2024-07-08  val_cutoff=2024-12-07\n",
      "  Forecast next 30 days from 2024-12-08\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-08_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-08_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-08_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-08_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-08_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-08_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-08_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-08_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-08_V1.pkl\n",
      "[MoE] No saved file for 2024-12-08, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-09 ===\n",
      "  train_cutoff=2024-07-09  val_cutoff=2024-12-08\n",
      "  Forecast next 30 days from 2024-12-09\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-09_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-09_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-09_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-09_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-09_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-09_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-09_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-09_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-09_V1.pkl\n",
      "[MoE] No saved file for 2024-12-09, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-10 ===\n",
      "  train_cutoff=2024-07-10  val_cutoff=2024-12-09\n",
      "  Forecast next 30 days from 2024-12-10\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-10_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-10_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-10_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-10_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-10_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-10_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-10_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-10_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-10_V1.pkl\n",
      "[MoE] No saved file for 2024-12-10, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-11 ===\n",
      "  train_cutoff=2024-07-11  val_cutoff=2024-12-10\n",
      "  Forecast next 30 days from 2024-12-11\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-11_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-11_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-11_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-11_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-11_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-11_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-11_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-11_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-11_V1.pkl\n",
      "[MoE] No saved file for 2024-12-11, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-12 ===\n",
      "  train_cutoff=2024-07-12  val_cutoff=2024-12-11\n",
      "  Forecast next 30 days from 2024-12-12\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-12_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-12_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-12_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-12_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-12_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-12_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-12_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-12_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-12_V1.pkl\n",
      "[MoE] No saved file for 2024-12-12, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-13 ===\n",
      "  train_cutoff=2024-07-13  val_cutoff=2024-12-12\n",
      "  Forecast next 30 days from 2024-12-13\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-13_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-13_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-13_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-13_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-13_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-13_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-13_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-13_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-13_V1.pkl\n",
      "[MoE] No saved file for 2024-12-13, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-14 ===\n",
      "  train_cutoff=2024-07-14  val_cutoff=2024-12-13\n",
      "  Forecast next 30 days from 2024-12-14\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-14_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-14_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-14_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-14_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-14_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-14_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-14_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-14_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-14_V1.pkl\n",
      "[MoE] No saved file for 2024-12-14, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-15 ===\n",
      "  train_cutoff=2024-07-15  val_cutoff=2024-12-14\n",
      "  Forecast next 30 days from 2024-12-15\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-15_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-15_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-15_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-15_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-15_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-15_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-15_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-15_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-15_V1.pkl\n",
      "[MoE] No saved file for 2024-12-15, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-16 ===\n",
      "  train_cutoff=2024-07-16  val_cutoff=2024-12-15\n",
      "  Forecast next 30 days from 2024-12-16\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-16_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-16_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-16_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-16_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-16_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-16_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-16_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-16_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-16_V1.pkl\n",
      "[MoE] No saved file for 2024-12-16, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-17 ===\n",
      "  train_cutoff=2024-07-17  val_cutoff=2024-12-16\n",
      "  Forecast next 30 days from 2024-12-17\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-17_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-17_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-17_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-17_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-17_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-17_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-17_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-17_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-17_V1.pkl\n",
      "[MoE] No saved file for 2024-12-17, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-18 ===\n",
      "  train_cutoff=2024-07-18  val_cutoff=2024-12-17\n",
      "  Forecast next 30 days from 2024-12-18\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-18_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-18_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-18_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-18_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-18_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-18_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-18_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-18_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-18_V1.pkl\n",
      "[MoE] No saved file for 2024-12-18, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-19 ===\n",
      "  train_cutoff=2024-07-19  val_cutoff=2024-12-18\n",
      "  Forecast next 30 days from 2024-12-19\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-19_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-19_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-19_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-19_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-19_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-19_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-19_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-19_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-19_V1.pkl\n",
      "[MoE] No saved file for 2024-12-19, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-20 ===\n",
      "  train_cutoff=2024-07-20  val_cutoff=2024-12-19\n",
      "  Forecast next 30 days from 2024-12-20\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-20_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-20_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-20_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-20_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-20_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-20_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-20_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-20_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-20_V1.pkl\n",
      "[MoE] No saved file for 2024-12-20, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-21 ===\n",
      "  train_cutoff=2024-07-21  val_cutoff=2024-12-20\n",
      "  Forecast next 30 days from 2024-12-21\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-21_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-21_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-21_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-21_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-21_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-21_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-21_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-21_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-21_V1.pkl\n",
      "[MoE] No saved file for 2024-12-21, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-22 ===\n",
      "  train_cutoff=2024-07-22  val_cutoff=2024-12-21\n",
      "  Forecast next 30 days from 2024-12-22\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-22_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-22_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-22_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-22_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-22_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-22_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-22_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-22_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-22_V1.pkl\n",
      "[MoE] No saved file for 2024-12-22, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-23 ===\n",
      "  train_cutoff=2024-07-23  val_cutoff=2024-12-22\n",
      "  Forecast next 30 days from 2024-12-23\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-23_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-23_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-23_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-23_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-23_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-23_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-23_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-23_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-23_V1.pkl\n",
      "[MoE] No saved file for 2024-12-23, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-24 ===\n",
      "  train_cutoff=2024-07-24  val_cutoff=2024-12-23\n",
      "  Forecast next 30 days from 2024-12-24\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-24_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-24_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-24_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-24_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-24_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-24_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-24_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-24_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-24_V1.pkl\n",
      "[MoE] No saved file for 2024-12-24, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-25 ===\n",
      "  train_cutoff=2024-07-25  val_cutoff=2024-12-24\n",
      "  Forecast next 30 days from 2024-12-25\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-25_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-25_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-25_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-25_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-25_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-25_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-25_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-25_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-25_V1.pkl\n",
      "[MoE] No saved file for 2024-12-25, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-26 ===\n",
      "  train_cutoff=2024-07-26  val_cutoff=2024-12-25\n",
      "  Forecast next 30 days from 2024-12-26\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-26_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-26_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-26_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-26_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-26_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-26_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-26_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-26_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-26_V1.pkl\n",
      "[MoE] No saved file for 2024-12-26, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-27 ===\n",
      "  train_cutoff=2024-07-27  val_cutoff=2024-12-26\n",
      "  Forecast next 30 days from 2024-12-27\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-27_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-27_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-27_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-27_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-27_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-27_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-27_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-27_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-27_V1.pkl\n",
      "[MoE] No saved file for 2024-12-27, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-28 ===\n",
      "  train_cutoff=2024-07-28  val_cutoff=2024-12-27\n",
      "  Forecast next 30 days from 2024-12-28\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-28_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-28_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-28_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-28_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-28_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-28_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-28_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-28_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-28_V1.pkl\n",
      "[MoE] No saved file for 2024-12-28, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-29 ===\n",
      "  train_cutoff=2024-07-29  val_cutoff=2024-12-28\n",
      "  Forecast next 30 days from 2024-12-29\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-29_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-29_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-29_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-29_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-29_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-29_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-29_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-29_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-29_V1.pkl\n",
      "[MoE] No saved file for 2024-12-29, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-30 ===\n",
      "  train_cutoff=2024-07-30  val_cutoff=2024-12-29\n",
      "  Forecast next 30 days from 2024-12-30\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-30_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-30_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-30_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-30_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-30_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-30_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-30_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-30_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-30_V1.pkl\n",
      "[MoE] No saved file for 2024-12-30, skipping load.\n",
      "\n",
      "=== Rolling Day 2024-12-31 ===\n",
      "  train_cutoff=2024-07-31  val_cutoff=2024-12-30\n",
      "  Forecast next 30 days from 2024-12-31\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2024-12-31_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2024-12-31_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2024-12-31_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2024-12-31_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2024-12-31_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2024-12-31_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2024-12-31_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2024-12-31_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2024-12-31_V1.pkl\n",
      "[MoE] No saved file for 2024-12-31, skipping load.\n",
      "\n",
      "=== Rolling Day 2025-01-01 ===\n",
      "  train_cutoff=2024-08-01  val_cutoff=2024-12-31\n",
      "  Forecast next 30 days from 2025-01-01\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2025-01-01_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2025-01-01_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2025-01-01_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2025-01-01_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2025-01-01_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2025-01-01_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2025-01-01_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2025-01-01_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2025-01-01_V1.pkl\n",
      "[MoE] No saved file for 2025-01-01, skipping load.\n",
      "\n",
      "=== Rolling Day 2025-01-02 ===\n",
      "  train_cutoff=2024-08-02  val_cutoff=2025-01-01\n",
      "  Forecast next 30 days from 2025-01-02\n",
      "[PyTorch] Loading LSTM from ./best_LSTM_2025-01-02_V1.pt\n",
      "[PyTorch] Loading GRU from ./best_GRU_2025-01-02_V1.pt\n",
      "[PyTorch] Loading RNN from ./best_RNN_2025-01-02_V1.pt\n",
      "[PyTorch] Loading CNN from ./best_CNN_2025-01-02_V1.pt\n",
      "[PyTorch] Loading Transformer from ./best_Transformer_2025-01-02_V1.pt\n",
      "[PyTorch] Loading N-BEATS from ./best_N-BEATS_2025-01-02_V1.pt\n",
      "[PyTorch] Loading N-HITS from ./best_N-HITS_2025-01-02_V1.pt\n",
      "[sklearn] Loading SVM => ./best_SVM_2025-01-02_V1.pkl\n",
      "[sklearn] Loading Boost => ./best_Boost_2025-01-02_V1.pkl\n",
      "[MoE] No saved file for 2025-01-02, skipping load.\n",
      "\n",
      "=== HEAD of final_rolling_fc ===\n",
      "    ForecastDate  Pred_Price_unscaled  Pred_Open_unscaled  Pred_High_unscaled  \\\n",
      "240   2024-12-01           214.246894          214.995007          217.194694   \n",
      "241   2024-12-02           214.245588          214.146896          217.191849   \n",
      "242   2024-12-03           214.580558          214.155611          217.307965   \n",
      "243   2024-12-04           213.809644          214.161535          217.334685   \n",
      "244   2024-12-05           214.063503          213.887005          218.023314   \n",
      "245   2024-12-06           214.051340          213.894199          218.021134   \n",
      "246   2024-12-07           214.038366          213.899976          218.020690   \n",
      "247   2024-12-08           214.061597          213.892614          218.029160   \n",
      "248   2024-12-09           214.039751          213.898053          218.027641   \n",
      "249   2024-12-10           214.045761          213.896587          218.013274   \n",
      "\n",
      "     Pred_Low_unscaled   BaseDate  Model  \n",
      "240         209.592589 2024-12-01  Boost  \n",
      "241         209.662391 2024-12-01  Boost  \n",
      "242         209.818511 2024-12-01  Boost  \n",
      "243         209.805755 2024-12-01  Boost  \n",
      "244         209.805002 2024-12-01  Boost  \n",
      "245         209.819468 2024-12-01  Boost  \n",
      "246         209.789852 2024-12-01  Boost  \n",
      "247         209.784396 2024-12-01  Boost  \n",
      "248         209.786737 2024-12-01  Boost  \n",
      "249         209.779027 2024-12-01  Boost  \n",
      "\n",
      "=== SUPER ADVANCED MOE w/ Polynomials & Voting ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_918156/1184921114.py:419: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_slope= df_merge.groupby([\"Model\",\"BaseDate\"], group_keys=False).apply(group_slope_func)\n",
      "/tmp/ipykernel_918156/1184921114.py:438: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  g_vol= df_merge.groupby([\"Model\",\"BaseDate\"]).apply(mini_volatility).reset_index()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_predicted_price_all_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 889\u001b[0m\n\u001b[1;32m    886\u001b[0m     df_rolled_all\u001b[38;5;241m=\u001b[39m final_rolling_fc\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# D) Quick Plot of All Models in Jan 2025\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m \u001b[43mplot_predicted_price_all_models\u001b[49m(\n\u001b[1;32m    890\u001b[0m     df_rolled_all,\n\u001b[1;32m    891\u001b[0m     from_d\u001b[38;5;241m=\u001b[39m PLOT_START,\n\u001b[1;32m    892\u001b[0m     to_d\u001b[38;5;241m=\u001b[39m   PLOT_END\n\u001b[1;32m    893\u001b[0m )\n\u001b[1;32m    895\u001b[0m \u001b[38;5;66;03m# E) Merge w/ actual => daily differences\u001b[39;00m\n\u001b[1;32m    896\u001b[0m df_diff_all\u001b[38;5;241m=\u001b[39m merge_with_actual_and_diff(df_rolled_all, df_complete_all)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_predicted_price_all_models' is not defined"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 0) Imports & Global Setup\n",
    "###############################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import joblib\n",
    "\n",
    "from numpy.polynomial.polynomial import polyfit, polyval\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Global columns and constants\n",
    "features = [\"Price\",\"Open\",\"High\",\"Low\"]\n",
    "WINDOW_SIZE = 35\n",
    "\n",
    "# Date ranges\n",
    "train_cutoff = pd.to_datetime(\"2024-07-01\")\n",
    "val_cutoff   = pd.to_datetime(\"2024-11-30\")\n",
    "test_cutoff  = pd.to_datetime(\"2024-12-01\")\n",
    "\n",
    "# For final evaluation & plotting (focus: Jan 2025)\n",
    "PLOT_START = pd.to_datetime(\"2025-01-01\")\n",
    "PLOT_END   = pd.to_datetime(\"2025-01-31\")\n",
    "\n",
    "BEST_MODELS_DIR = \".\"\n",
    "\n",
    "###############################################################################\n",
    "# 1) Data Loading & Scaling\n",
    "###############################################################################\n",
    "def load_and_scale_data(train_csv=\"Apple Stock Price History.csv\",\n",
    "                        complete_csv=\"Apple Stock Price History_Complete.csv\"):\n",
    "    df_traincsv = pd.read_csv(train_csv)\n",
    "    df_traincsv[\"Date\"] = pd.to_datetime(df_traincsv[\"Date\"], errors=\"coerce\")\n",
    "    for col in [\"Vol.\",\"Change %\"]:\n",
    "        if col in df_traincsv.columns:\n",
    "            df_traincsv.drop(columns=[col], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # Clean columns\n",
    "    for feat in features:\n",
    "        if feat not in df_traincsv.columns:\n",
    "            df_traincsv[feat] = np.nan\n",
    "        else:\n",
    "            df_traincsv[feat] = df_traincsv[feat].astype(str).str.replace(\",\", \"\", regex=True)\n",
    "            df_traincsv[feat] = pd.to_numeric(df_traincsv[feat], errors=\"coerce\")\n",
    "\n",
    "    df_traincsv.sort_values(\"Date\", inplace=True)\n",
    "    df_traincsv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Split into train/val/test\n",
    "    df_trn = df_traincsv[df_traincsv[\"Date\"] < train_cutoff].copy()\n",
    "    df_val = df_traincsv[(df_traincsv[\"Date\"] >= train_cutoff) & (df_traincsv[\"Date\"] <= val_cutoff)].copy()\n",
    "    df_tst = df_traincsv[df_traincsv[\"Date\"] >= test_cutoff].copy()\n",
    "\n",
    "    # Scale only on training subset\n",
    "    df_trn_nonan = df_trn.dropna(subset=features)\n",
    "    scaler = MinMaxScaler()\n",
    "    if not df_trn_nonan.empty:\n",
    "        scaler.fit(df_trn_nonan[features])\n",
    "\n",
    "    def apply_scaler(df_sub):\n",
    "        sub_nonan = df_sub.dropna(subset=features)\n",
    "        if sub_nonan.empty:\n",
    "            return df_sub\n",
    "        df_sub.loc[sub_nonan.index, features] = scaler.transform(sub_nonan[features])\n",
    "        return df_sub\n",
    "\n",
    "    df_trn_scaled = apply_scaler(df_trn)\n",
    "    df_val_scaled = apply_scaler(df_val)\n",
    "    # For test, fill missing with min of training, then scale\n",
    "    df_tst_scaled = df_tst.copy()\n",
    "    if not df_tst_scaled.empty and not df_trn_nonan.empty:\n",
    "        train_mins = df_trn_nonan[features].min()\n",
    "        df_tst_filled = df_tst_scaled[features].fillna(train_mins)\n",
    "        df_tst_scaled.loc[:, features] = scaler.transform(df_tst_filled)\n",
    "\n",
    "    df_scaled = pd.concat([df_trn_scaled, df_val_scaled, df_tst_scaled], ignore_index=True)\n",
    "    df_scaled.sort_values(\"Date\", inplace=True)\n",
    "    df_scaled.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Also load the more complete CSV for raw unscaled data\n",
    "    df_complete = pd.read_csv(complete_csv)\n",
    "    df_complete[\"Date\"] = pd.to_datetime(df_complete[\"Date\"], errors=\"coerce\")\n",
    "    for col in [\"Vol.\",\"Change %\"]:\n",
    "        if col in df_complete.columns:\n",
    "            df_complete.drop(columns=[col], errors=\"ignore\", inplace=True)\n",
    "    for feat in features:\n",
    "        df_complete[feat] = df_complete[feat].astype(str).str.replace(\",\", \"\", regex=True).astype(float)\n",
    "    df_complete.sort_values(\"Date\", inplace=True)\n",
    "    df_complete.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_scaled, df_complete, scaler\n",
    "\n",
    "###############################################################################\n",
    "# 2) PyTorch Model Definitions\n",
    "###############################################################################\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_type=\"LSTM\", window_width=35):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "\n",
    "        if model_type == \"CNN\":\n",
    "            self.conv1 = nn.Conv1d(4, 64, 3)\n",
    "            self.conv2 = nn.Conv1d(64,128,3)\n",
    "            with torch.no_grad():\n",
    "                dummy = torch.zeros(1,4,window_width)\n",
    "                outdummy = self.conv2(F.relu(self.conv1(dummy)))\n",
    "                conv_output_size = outdummy.shape[1]*outdummy.shape[2]\n",
    "            self.fc = nn.Linear(conv_output_size,4)\n",
    "\n",
    "        elif model_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(4,128,num_layers=2,batch_first=True,dropout=0.1)\n",
    "            self.fc = nn.Linear(128,4)\n",
    "        elif model_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(4,128,num_layers=2,batch_first=True,dropout=0.1)\n",
    "            self.fc = nn.Linear(128,4)\n",
    "        elif model_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(4,128,num_layers=2,nonlinearity=\"relu\",batch_first=True,dropout=0.1)\n",
    "            self.fc = nn.Linear(128,4)\n",
    "        elif model_type == \"EnhancedLSTM\":\n",
    "            self.rnn = nn.LSTM(4,128,num_layers=3,batch_first=True,dropout=0.2)\n",
    "            self.bn = nn.BatchNorm1d(128)\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "            self.fc = nn.Linear(128,4)\n",
    "        elif model_type == \"Transformer\":\n",
    "            self.input_linear = nn.Linear(4,128)\n",
    "            enc_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8, dropout=0.1)\n",
    "            self.transformer_encoder = nn.TransformerEncoder(enc_layer, num_layers=3)\n",
    "            self.fc = nn.Linear(128,4)\n",
    "        elif model_type in [\"N-BEATS\",\"N-HITS\"]:\n",
    "            self.input_size= window_width*4\n",
    "            self.blocks= nn.ModuleList([nn.Sequential(\n",
    "                nn.Linear(self.input_size,128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128,128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128,4)\n",
    "            ) for _ in range(3)])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid model_type: {model_type}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.model_type==\"CNN\":\n",
    "            x= x.permute(0,2,1)\n",
    "            x= F.relu(self.conv1(x))\n",
    "            x= F.relu(self.conv2(x))\n",
    "            x= x.view(x.size(0),-1)\n",
    "            return self.fc(x)\n",
    "\n",
    "        elif self.model_type in [\"LSTM\",\"GRU\",\"RNN\"]:\n",
    "            out, _= self.rnn(x)\n",
    "            out= out[:,-1,:]\n",
    "            return self.fc(out)\n",
    "\n",
    "        elif self.model_type==\"EnhancedLSTM\":\n",
    "            out, _= self.rnn(x)\n",
    "            out= out[:,-1,:]\n",
    "            out= self.bn(out)\n",
    "            out= self.dropout(out)\n",
    "            return self.fc(out)\n",
    "\n",
    "        elif self.model_type==\"Transformer\":\n",
    "            x= self.input_linear(x)\n",
    "            x= x.permute(1,0,2)\n",
    "            x= self.transformer_encoder(x)\n",
    "            x= x[-1,:,:]\n",
    "            return self.fc(x)\n",
    "\n",
    "        elif self.model_type in [\"N-BEATS\",\"N-HITS\"]:\n",
    "            xflat= x.reshape(x.size(0), -1)\n",
    "            forecast= 0\n",
    "            for block in self.blocks:\n",
    "                forecast += block(xflat)\n",
    "            return forecast\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type\")\n",
    "\n",
    "class SklearnWrapper:\n",
    "    \"\"\"\n",
    "    Simple wrapper so we can call sklearn regressors similarly to a PyTorch model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        arr = x.cpu().numpy().reshape(1,-1)\n",
    "        pred = self.model.predict(arr)\n",
    "        return torch.from_numpy(pred).float().to(x.device)\n",
    "    def eval(self):\n",
    "        pass\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "def load_torch_model(model_type, window_size, path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    print(f\"[PyTorch] Loading {model_type} from {path}\")\n",
    "    net = BaseModel(model_type, window_size).to(device)\n",
    "    net.load_state_dict(torch.load(path, map_location=device))\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "def load_sklearn_model(model_type, path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    print(f\"[sklearn] Loading {model_type} => {path}\")\n",
    "    loaded = joblib.load(path)\n",
    "    return SklearnWrapper(loaded)\n",
    "\n",
    "###############################################################################\n",
    "# 3) Rolling Multi-Day Forecast with All Models (Including MoE)\n",
    "###############################################################################\n",
    "def get_latest_window(df_actual, current_date, window=35, scaler=None):\n",
    "    \"\"\"\n",
    "    Returns the scaled array of the last 'window' days of features\n",
    "    prior to current_date.\n",
    "    \"\"\"\n",
    "    mask= df_actual[\"Date\"]< current_date\n",
    "    sub= df_actual.loc[mask].copy()\n",
    "    sub.sort_values(\"Date\", inplace=True)\n",
    "    if len(sub) < window:\n",
    "        return None\n",
    "    sub[features]= sub[features].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    arr= scaler.transform(sub[features].iloc[-window:].values)\n",
    "    return arr\n",
    "\n",
    "def forecast_n_days_from_date(model, df_actual, start_date, window=35, horizon=30,\n",
    "                              device=None, scaler=None, noise_std=0.01):\n",
    "    \"\"\"\n",
    "    Rolling forecast with mild random noise => more volatility.\n",
    "    \"\"\"\n",
    "    def add_noise_4d(prices, std=0.01):\n",
    "        noise= np.random.normal(0.0, std, size=prices.shape)\n",
    "        return np.clip(prices + noise, a_min=0, a_max=None)\n",
    "\n",
    "    arr_window= get_latest_window(df_actual, start_date, window, scaler)\n",
    "    if arr_window is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rolling_buffer= np.copy(arr_window)\n",
    "    forecast_records=[]\n",
    "    cur_dt= pd.to_datetime(start_date)\n",
    "\n",
    "    for i in range(horizon):\n",
    "        X_in= torch.tensor(rolling_buffer, dtype=torch.float).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            out_scaled= model(X_in).cpu().numpy()[0]\n",
    "        out_unscaled= scaler.inverse_transform(out_scaled.reshape(1,-1))[0]\n",
    "        out_noisy= add_noise_4d(out_unscaled, noise_std)\n",
    "\n",
    "        dayx= cur_dt + pd.Timedelta(days=i)\n",
    "        forecast_records.append({\n",
    "            \"ForecastDate\": dayx,\n",
    "            \"Pred_Price_unscaled\": out_noisy[0],\n",
    "            \"Pred_Open_unscaled\":  out_noisy[1],\n",
    "            \"Pred_High_unscaled\":  out_noisy[2],\n",
    "            \"Pred_Low_unscaled\":   out_noisy[3]\n",
    "        })\n",
    "\n",
    "        out_noisy_scaled= scaler.transform(out_noisy.reshape(1,-1))[0]\n",
    "        rolling_buffer= np.vstack([rolling_buffer[1:], out_noisy_scaled.reshape(1,-1)])\n",
    "\n",
    "    return pd.DataFrame(forecast_records)\n",
    "\n",
    "def rolling_train_validate_predict_moe(\n",
    "    df_full, scaler_obj, model_types,\n",
    "    start_train_cutoff=pd.to_datetime(\"2024-07-01\"),\n",
    "    start_val_cutoff=pd.to_datetime(\"2024-11-30\"),\n",
    "    start_pred=pd.to_datetime(\"2024-12-01\"),\n",
    "    end_pred=pd.to_datetime(\"2025-01-01\"),\n",
    "    horizon_days=30\n",
    "):\n",
    "    \"\"\"\n",
    "    For each day in [start_pred..end_pred], load the best_{model_type}_{YYYY-mm-dd}_V1,\n",
    "    forecast next horizon_days, combine results.\n",
    "    If \"MoE\" is in model_types, try to load a corresponding file. If none found, we skip.\n",
    "    \"\"\"\n",
    "    df_sorted= df_full.copy()\n",
    "    df_sorted.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "    date_rng= pd.date_range(start_pred, end_pred, freq=\"D\")\n",
    "    all_records=[]\n",
    "\n",
    "    for i, day_i in enumerate(date_rng):\n",
    "        train_cutoff_i= start_train_cutoff + pd.Timedelta(days=i)\n",
    "        val_cutoff_i  = start_val_cutoff   + pd.Timedelta(days=i)\n",
    "        print(f\"\\n=== Rolling Day {day_i.date()} ===\")\n",
    "        print(f\"  train_cutoff={train_cutoff_i.date()}  val_cutoff={val_cutoff_i.date()}\")\n",
    "        print(f\"  Forecast next {horizon_days} days from {day_i.date()}\")\n",
    "\n",
    "        model_dict={}\n",
    "        for mt in model_types:\n",
    "            # Special case for \"MoE\"\n",
    "            if mt==\"MoE\":\n",
    "                out_file= f\"best_MoE_{day_i.strftime('%Y-%m-%d')}_V1\"\n",
    "                pt_file= os.path.join(BEST_MODELS_DIR, out_file+\".pt\")\n",
    "                pkl_file= os.path.join(BEST_MODELS_DIR, out_file+\".pkl\")\n",
    "                if os.path.exists(pt_file):\n",
    "                    net= load_torch_model(mt, WINDOW_SIZE, pt_file)\n",
    "                    model_dict[mt]= net\n",
    "                elif os.path.exists(pkl_file):\n",
    "                    net= load_sklearn_model(mt, pkl_file)\n",
    "                    model_dict[mt]= net\n",
    "                else:\n",
    "                    print(f\"[MoE] No saved file for {day_i.date()}, skipping load.\")\n",
    "                continue\n",
    "\n",
    "            # Normal model\n",
    "            out_file= f\"best_{mt}_{day_i.strftime('%Y-%m-%d')}_V1\"\n",
    "            if mt in [\"SVM\",\"GPR\",\"Boost\"]:\n",
    "                out_file+= \".pkl\"\n",
    "                fullpath= os.path.join(BEST_MODELS_DIR, out_file)\n",
    "                if os.path.exists(fullpath):\n",
    "                    net= load_sklearn_model(mt, fullpath)\n",
    "                    model_dict[mt]= net\n",
    "            else:\n",
    "                out_file+= \".pt\"\n",
    "                fullpath= os.path.join(BEST_MODELS_DIR, out_file)\n",
    "                if os.path.exists(fullpath):\n",
    "                    net= load_torch_model(mt, WINDOW_SIZE, fullpath)\n",
    "                    model_dict[mt]= net\n",
    "\n",
    "        # Forecast\n",
    "        for mt, netobj in model_dict.items():\n",
    "            df_fc= forecast_n_days_from_date(\n",
    "                model=netobj,\n",
    "                df_actual=df_sorted,\n",
    "                start_date=day_i,\n",
    "                window=WINDOW_SIZE,\n",
    "                horizon=horizon_days,\n",
    "                device=device,\n",
    "                scaler=scaler_obj,\n",
    "                noise_std=0.01\n",
    "            )\n",
    "            if not df_fc.empty:\n",
    "                df_fc[\"BaseDate\"] = day_i\n",
    "                df_fc[\"Model\"]    = mt\n",
    "                all_records.append(df_fc)\n",
    "\n",
    "    df_all= pd.concat(all_records, ignore_index=True) if all_records else pd.DataFrame()\n",
    "    if not df_all.empty:\n",
    "        df_all.sort_values([\"Model\",\"BaseDate\",\"ForecastDate\"], inplace=True)\n",
    "    return df_all\n",
    "\n",
    "###############################################################################\n",
    "# 4) Compute “Super Advanced” MoE if no MoE loaded\n",
    "###############################################################################\n",
    "def build_mini_models_table(final_rolling_fcst, df_actual):\n",
    "    \"\"\"\n",
    "    Use [2024-12-01..2025-01-01] as the weighting period to measure model performance\n",
    "    for weighting the MoE if we need to build it ourselves.\n",
    "    \"\"\"\n",
    "    start_jan= pd.to_datetime(\"2024-12-01\")\n",
    "    end_jan=   pd.to_datetime(\"2025-01-01\")\n",
    "\n",
    "    df_jan= final_rolling_fcst[\n",
    "        (final_rolling_fcst[\"ForecastDate\"]>= start_jan)&\n",
    "        (final_rolling_fcst[\"ForecastDate\"]<= end_jan)\n",
    "    ].copy()\n",
    "    if df_jan.empty:\n",
    "        return pd.DataFrame(), 0.01\n",
    "\n",
    "    df_act_jan= df_actual[\n",
    "        (df_actual[\"Date\"]>= start_jan)&\n",
    "        (df_actual[\"Date\"]<= end_jan)\n",
    "    ].copy()\n",
    "    df_act_jan.rename(columns={\"Date\":\"ForecastDate\",\"Price\":\"ActualPrice\"}, inplace=True)\n",
    "\n",
    "    df_merge= pd.merge(\n",
    "        df_jan[[\"Model\",\"BaseDate\",\"ForecastDate\",\"Pred_Price_unscaled\"]],\n",
    "        df_act_jan[[\"ForecastDate\",\"ActualPrice\"]],\n",
    "        on=\"ForecastDate\", how=\"inner\"\n",
    "    )\n",
    "    df_merge[\"AbsError\"] = (df_merge[\"Pred_Price_unscaled\"] - df_merge[\"ActualPrice\"]).abs()\n",
    "\n",
    "    # mae\n",
    "    g_mae= df_merge.groupby([\"Model\",\"BaseDate\"])[\"AbsError\"].mean().reset_index()\n",
    "    g_mae.rename(columns={\"AbsError\":\"mae_jan\"}, inplace=True)\n",
    "\n",
    "    # slope alignment => quick polynomial fit\n",
    "    def poly_slope_diff(sub):\n",
    "        sub= sub.sort_values(\"ForecastDate\")\n",
    "        xvals= np.arange(len(sub))\n",
    "        yvals= sub[\"Pred_Price_unscaled\"].values\n",
    "        if len(yvals)< 4:\n",
    "            return 0.5\n",
    "        coefs= polyfit(xvals,yvals,deg=2)\n",
    "        y_pred= polyval(xvals, coefs)\n",
    "        resid= yvals- y_pred\n",
    "        sse= np.mean(resid**2)\n",
    "        return float(np.exp(-sse))\n",
    "\n",
    "    def group_slope_func(grp):\n",
    "        grp[\"slopeAlign_jan\"]= poly_slope_diff(grp)\n",
    "        return grp\n",
    "\n",
    "    df_slope= df_merge.groupby([\"Model\",\"BaseDate\"], group_keys=False).apply(group_slope_func)\n",
    "    df_slope_agg= df_slope.groupby([\"Model\",\"BaseDate\"])[\"slopeAlign_jan\"].mean().reset_index()\n",
    "\n",
    "    # real vol\n",
    "    df_act_jan_sorted= df_act_jan.drop_duplicates(\"ForecastDate\").copy()\n",
    "    df_act_jan_sorted.sort_values(\"ForecastDate\", inplace=True)\n",
    "    df_act_jan_sorted[\"DayChange\"]= df_act_jan_sorted[\"ActualPrice\"].diff()\n",
    "    real_vol= df_act_jan_sorted[\"DayChange\"].std(skipna=True)\n",
    "    if pd.isna(real_vol) or real_vol< 1e-9:\n",
    "        real_vol= 0.01\n",
    "\n",
    "    def mini_volatility(sub):\n",
    "        sub= sub.sort_values(\"ForecastDate\")\n",
    "        sub[\"DayChange\"]= sub[\"Pred_Price_unscaled\"].diff()\n",
    "        vol_= sub[\"DayChange\"].std(skipna=True)\n",
    "        if pd.isna(vol_):\n",
    "            vol_= 0.01\n",
    "        return vol_\n",
    "\n",
    "    g_vol= df_merge.groupby([\"Model\",\"BaseDate\"]).apply(mini_volatility).reset_index()\n",
    "    g_vol.rename(columns={0:\"predVol_jan\"}, inplace=True)\n",
    "    def vol_align(v):\n",
    "        ratio= v/ real_vol\n",
    "        return np.exp(-abs(ratio-1.0))\n",
    "    g_vol[\"volAlign_jan\"]= g_vol[\"predVol_jan\"].apply(vol_align)\n",
    "\n",
    "    mini_models= pd.merge(g_mae, df_slope_agg, on=[\"Model\",\"BaseDate\"], how=\"left\")\n",
    "    mini_models= pd.merge(mini_models, g_vol[[\"Model\",\"BaseDate\",\"predVol_jan\",\"volAlign_jan\"]],\n",
    "                          on=[\"Model\",\"BaseDate\"], how=\"left\")\n",
    "\n",
    "    mae_min= mini_models[\"mae_jan\"].min()\n",
    "    mae_max= mini_models[\"mae_jan\"].max()\n",
    "    if np.isclose(mae_min, mae_max):\n",
    "        mini_models[\"baseAcc\"]= 1.0\n",
    "    else:\n",
    "        mini_models[\"baseAcc\"]= 1.0 - ((mini_models[\"mae_jan\"]- mae_min)/(mae_max- mae_min))\n",
    "\n",
    "    return mini_models, real_vol\n",
    "\n",
    "def compute_super_advanced_moe(final_rolling_fcst, df_actual):\n",
    "    \"\"\"\n",
    "    Weighted \"Mixture of Experts\" for entire [2024-12-01..2025-01-31].\n",
    "    If no MoE model is loaded from disk, we create one by weighting predictions\n",
    "    from other models found in final_rolling_fcst.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== SUPER ADVANCED MOE w/ Polynomials & Voting ===\\n\")\n",
    "    mini_models, real_vol= build_mini_models_table(final_rolling_fcst, df_actual)\n",
    "    if mini_models.empty:\n",
    "        print(\"No january data => fallback => uniform daily average\")\n",
    "        return pd.DataFrame(columns=[\"ForecastDate\",\"MOE_Price\"])\n",
    "\n",
    "    mm_dict={}\n",
    "    for i, row in mini_models.iterrows():\n",
    "        mm_dict[(row[\"Model\"], row[\"BaseDate\"])] = {\n",
    "            \"baseAcc\": row[\"baseAcc\"],\n",
    "            \"slopeAlign_jan\": row[\"slopeAlign_jan\"],\n",
    "            \"volAlign_jan\": row[\"volAlign_jan\"]\n",
    "        }\n",
    "\n",
    "    start_moe= pd.to_datetime(\"2024-12-01\")\n",
    "    end_moe=   pd.to_datetime(\"2025-01-31\")\n",
    "    all_days=  pd.date_range(start_moe, end_moe, freq=\"D\")\n",
    "\n",
    "    records=[]\n",
    "    for d in all_days:\n",
    "        df_d= final_rolling_fcst[ final_rolling_fcst[\"ForecastDate\"]== d ].copy()\n",
    "        if df_d.empty:\n",
    "            continue\n",
    "\n",
    "        d_prev= d - pd.Timedelta(days=1)\n",
    "        ups= 0\n",
    "        downs= 0\n",
    "        total= 0\n",
    "        w_sums= 0.0\n",
    "        weighted_price= 0.0\n",
    "\n",
    "        for idx, row in df_d.iterrows():\n",
    "            m= row[\"Model\"]\n",
    "            bD= row[\"BaseDate\"]\n",
    "            if (m,bD) not in mm_dict:\n",
    "                continue\n",
    "\n",
    "            baseAcc= mm_dict[(m,bD)][\"baseAcc\"]\n",
    "            slopeA= mm_dict[(m,bD)][\"slopeAlign_jan\"]\n",
    "            volA=   mm_dict[(m,bD)][\"volAlign_jan\"]\n",
    "\n",
    "            df_dprev= final_rolling_fcst[\n",
    "                (final_rolling_fcst[\"Model\"]==m)&\n",
    "                (final_rolling_fcst[\"BaseDate\"]==bD)&\n",
    "                (final_rolling_fcst[\"ForecastDate\"]==d_prev)\n",
    "            ]\n",
    "            if df_dprev.empty:\n",
    "                dirFactor=1.0\n",
    "            else:\n",
    "                pd_today= row[\"Pred_Price_unscaled\"]\n",
    "                pd_yest=  df_dprev[\"Pred_Price_unscaled\"].values[0]\n",
    "                if pd_today> pd_yest:\n",
    "                    ups += 1\n",
    "                    dirFactor=1.1\n",
    "                elif pd_today< pd_yest:\n",
    "                    downs += 1\n",
    "                    dirFactor=0.9\n",
    "                else:\n",
    "                    dirFactor=1.0\n",
    "                total += 1\n",
    "\n",
    "            w= baseAcc*slopeA*volA* dirFactor\n",
    "            w_sums+= w\n",
    "            weighted_price+= w* row[\"Pred_Price_unscaled\"]\n",
    "\n",
    "        if np.isclose(w_sums,0.0):\n",
    "            day_moe= df_d[\"Pred_Price_unscaled\"].mean()\n",
    "        else:\n",
    "            day_moe= weighted_price/ w_sums\n",
    "\n",
    "        # If majority are up or down, nudge the final price\n",
    "        if total>0:\n",
    "            fraction_up= ups/ total\n",
    "            fraction_down= downs/ total\n",
    "            if fraction_up> 0.7:\n",
    "                day_moe*= 1.01\n",
    "            elif fraction_down> 0.7:\n",
    "                day_moe*= 0.99\n",
    "\n",
    "        # final mild noise\n",
    "        day_moe+= np.random.normal(0, day_moe*0.002)\n",
    "        records.append({\"ForecastDate\": d, \"MOE_Price\": day_moe})\n",
    "\n",
    "    df_moe= pd.DataFrame(records)\n",
    "    df_moe.sort_values(\"ForecastDate\", inplace=True)\n",
    "    return df_moe\n",
    "\n",
    "def reduce_duplicates(df_in):\n",
    "    agg_df = df_in.groupby([\"Model\",\"ForecastDate\"], as_index=False).agg({\n",
    "        \"Pred_Price_unscaled\":\"mean\"\n",
    "    })\n",
    "    return agg_df\n",
    "\n",
    "def combine_base_and_moe(final_rolling_fc, df_moe):\n",
    "    df_moe_cpy = df_moe.copy()\n",
    "    df_moe_cpy[\"Model\"] = \"MoE\"\n",
    "    df_moe_cpy[\"BaseDate\"] = pd.NaT\n",
    "    df_moe_cpy.rename(columns={\"MOE_Price\":\"Pred_Price_unscaled\"}, inplace=True)\n",
    "\n",
    "    df_base = final_rolling_fc[[\"Model\",\"ForecastDate\",\"Pred_Price_unscaled\"]].copy()\n",
    "    df_combined = pd.concat([df_base, df_moe_cpy], ignore_index=True)\n",
    "\n",
    "    df_no_dupes = reduce_duplicates(df_combined)\n",
    "    df_no_dupes.sort_values([\"Model\",\"ForecastDate\"], inplace=True, ignore_index=True)\n",
    "    return df_no_dupes\n",
    "\n",
    "###############################################################################\n",
    "# 5) Merge with Actual => Daily Differences\n",
    "###############################################################################\n",
    "def merge_with_actual_and_diff(df_all_models, df_actual):\n",
    "    df_actual_cpy = df_actual[[\"Date\",\"Price\"]].rename(columns={\"Price\":\"ActualPrice\"}).copy()\n",
    "\n",
    "    df_merged = pd.merge(\n",
    "        df_all_models,\n",
    "        df_actual_cpy,\n",
    "        left_on=\"ForecastDate\",\n",
    "        right_on=\"Date\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    df_merged.dropna(subset=[\"ActualPrice\"], inplace=True)\n",
    "    df_merged.drop(columns=[\"Date\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    df_merged.rename(columns={\"ForecastDate\":\"Date\"}, inplace=True)\n",
    "    df_merged[\"DailyDiff\"] = df_merged[\"Pred_Price_unscaled\"] - df_merged[\"ActualPrice\"]\n",
    "    df_merged[\"AbsError\"]  = df_merged[\"DailyDiff\"].abs()\n",
    "\n",
    "    df_merged.sort_values([\"Model\",\"Date\"], inplace=True)\n",
    "    df_merged.reset_index(drop=True, inplace=True)\n",
    "    return df_merged\n",
    "\n",
    "###############################################################################\n",
    "# 6) Evaluate Errors & Confidence => produce df_scores\n",
    "###############################################################################\n",
    "def evaluate_models_confidence(df_diff):\n",
    "    df_eval = df_diff[\n",
    "        (df_diff[\"Date\"]>= PLOT_START)&\n",
    "        (df_diff[\"Date\"]<= PLOT_END)\n",
    "    ].copy()\n",
    "    model_scores = {}\n",
    "    for m in df_eval[\"Model\"].unique():\n",
    "        dsub= df_eval[df_eval[\"Model\"]==m].copy()\n",
    "        if dsub.empty:\n",
    "            continue\n",
    "        cum_err= dsub[\"AbsError\"].sum()\n",
    "\n",
    "        dsub.sort_values(\"Date\", inplace=True)\n",
    "        dsub[\"ActualDiff\"] = dsub[\"ActualPrice\"].diff()\n",
    "        dsub[\"PredDiff\"]   = dsub[\"Pred_Price_unscaled\"].diff()\n",
    "        dsub_valid= dsub.dropna(subset=[\"ActualDiff\",\"PredDiff\"]).copy()\n",
    "\n",
    "        def sign_match(a,b):\n",
    "            if a==0 and b==0:\n",
    "                return True\n",
    "            return np.sign(a)== np.sign(b)\n",
    "        dsub_valid[\"TrendMatch\"] = dsub_valid.apply(\n",
    "            lambda r: 1 if sign_match(r[\"ActualDiff\"], r[\"PredDiff\"]) else 0,\n",
    "            axis=1\n",
    "        )\n",
    "        trend_rate= dsub_valid[\"TrendMatch\"].mean() if len(dsub_valid)>0 else 0.0\n",
    "\n",
    "        model_scores[m] = {\n",
    "            \"cumulative_error\": cum_err,\n",
    "            \"trend_matching_rate\": trend_rate\n",
    "        }\n",
    "\n",
    "    df_scores= pd.DataFrame.from_dict(model_scores, orient=\"index\").reset_index()\n",
    "    df_scores.rename(columns={\"index\":\"Model\"}, inplace=True)\n",
    "\n",
    "    if not df_scores.empty:\n",
    "        ce_min= df_scores[\"cumulative_error\"].min()\n",
    "        ce_max= df_scores[\"cumulative_error\"].max()\n",
    "        if not np.isclose(ce_min, ce_max):\n",
    "            df_scores[\"normalized_error_score\"] = 1.0 - (df_scores[\"cumulative_error\"]-ce_min)/(ce_max-ce_min)\n",
    "        else:\n",
    "            df_scores[\"normalized_error_score\"] = 1.0\n",
    "\n",
    "        # Weighted combo => 70% trend, 30% error\n",
    "        df_scores[\"combined_confidence\"] = 0.7*df_scores[\"trend_matching_rate\"] + 0.3*df_scores[\"normalized_error_score\"]\n",
    "        df_scores.sort_values(\"combined_confidence\", ascending=False, inplace=True)\n",
    "        df_scores.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_scores\n",
    "\n",
    "def select_top5_including_moe(df_scores):\n",
    "    \"\"\"\n",
    "    Always include the 'MoE' model in top-5, plus 4 others with highest confidence.\n",
    "    Then re-normalize their 'combined_confidence' so sum=1.\n",
    "    \"\"\"\n",
    "    if df_scores.empty:\n",
    "        return df_scores.copy()\n",
    "\n",
    "    if \"MoE\" not in df_scores[\"Model\"].values:\n",
    "        # If MoE not present at all, just pick top-5\n",
    "        top5 = df_scores.head(5).copy()\n",
    "        csum = top5[\"combined_confidence\"].sum()\n",
    "        if csum>0:\n",
    "            top5[\"combined_confidence\"] /= csum\n",
    "        return top5\n",
    "\n",
    "    # Otherwise, separate out MoE row\n",
    "    df_sorted = df_scores.sort_values(\"combined_confidence\", ascending=False).reset_index(drop=True)\n",
    "    moe_row = df_sorted[df_sorted[\"Model\"]==\"MoE\"].iloc[0]\n",
    "    df_no_moe = df_sorted[df_sorted[\"Model\"]!=\"MoE\"].copy()\n",
    "\n",
    "    # pick top 4 from the others\n",
    "    top4 = df_no_moe.head(4).copy()\n",
    "    top5 = pd.concat([top4, pd.DataFrame([moe_row])], ignore_index=True)\n",
    "    top5.sort_values(\"combined_confidence\", ascending=False, inplace=True, ignore_index=True)\n",
    "\n",
    "    # re-normalize\n",
    "    csum = top5[\"combined_confidence\"].sum()\n",
    "    if csum> 0:\n",
    "        top5[\"combined_confidence\"] /= csum\n",
    "\n",
    "    return top5\n",
    "\n",
    "###############################################################################\n",
    "# 7) Polynomial Fit for the forced top-5 + Actual in [2025-01-01..2025-01-31]\n",
    "###############################################################################\n",
    "def polynomial_fit_top5_models(df_diff, df_top5, max_degree=70):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of best polynomials => {model: {degree, mse, coeffs}}\n",
    "    plus a merged DataFrame with [Date, Actual, top5-model columns, mdates_num].\n",
    "    \"\"\"\n",
    "    top5_models = df_top5[\"Model\"].tolist()\n",
    "\n",
    "    # Filter the underlying df_diff to Jan 2025 range\n",
    "    df_test = df_diff[\n",
    "        (df_diff[\"Date\"]>= PLOT_START)&\n",
    "        (df_diff[\"Date\"]<= PLOT_END)\n",
    "    ].copy()\n",
    "    if df_test.empty:\n",
    "        print(\"No data in test range for polynomial fitting.\")\n",
    "        return {}, pd.DataFrame()\n",
    "\n",
    "    # Pivot => each model's predicted price in separate column\n",
    "    df_pivot = df_test.pivot_table(\n",
    "        index=\"Date\", columns=\"Model\", values=\"Pred_Price_unscaled\", aggfunc=\"mean\"\n",
    "    ).reset_index()\n",
    "\n",
    "    # Also keep actual\n",
    "    df_actual_only = df_test.drop_duplicates(\"Date\")[[\"Date\",\"ActualPrice\"]].copy()\n",
    "    df_merged = pd.merge(df_pivot, df_actual_only, on=\"Date\", how=\"inner\")\n",
    "    df_merged.rename(columns={\"ActualPrice\":\"Actual\"}, inplace=True)\n",
    "    df_merged.sort_values(\"Date\", inplace=True)\n",
    "    df_merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # We want columns: \"Actual\" + top5\n",
    "    columns_to_fit = [\"Actual\"] + top5_models\n",
    "    df_merged = df_merged[[\"Date\"] + columns_to_fit].dropna()\n",
    "\n",
    "    if df_merged.empty:\n",
    "        print(\"No overlapping data for polynomial fit.\")\n",
    "        return {}, pd.DataFrame()\n",
    "\n",
    "    df_merged[\"mdates_num\"] = mdates.date2num(df_merged[\"Date\"])\n",
    "    n_points = len(df_merged)\n",
    "    idx = np.arange(n_points)\n",
    "    # simple train/val split: 3/4 train, 1/4 val\n",
    "    val_mask = (idx % 4 == 0)\n",
    "    train_mask = ~val_mask\n",
    "\n",
    "    best_poly = {}\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    for col in columns_to_fit:\n",
    "        x_train= df_merged.loc[train_mask,\"mdates_num\"].values\n",
    "        y_train= df_merged.loc[train_mask,col].values\n",
    "        x_val= df_merged.loc[val_mask,\"mdates_num\"].values\n",
    "        y_val= df_merged.loc[val_mask,col].values\n",
    "\n",
    "        best_deg = None\n",
    "        best_mse = float(\"inf\")\n",
    "        best_coefs= None\n",
    "        for d_ in range(1, max_degree+1):\n",
    "            if len(x_train)<= d_:\n",
    "                break\n",
    "            coefs = np.polyfit(x_train, y_train, d_)\n",
    "            y_pred_val= np.polyval(coefs, x_val)\n",
    "            mse_ = mean_squared_error(y_val, y_pred_val)\n",
    "            if mse_ < best_mse:\n",
    "                best_mse= mse_\n",
    "                best_deg= d_\n",
    "                best_coefs= coefs\n",
    "\n",
    "        if best_coefs is None:\n",
    "            best_poly[col] = {\n",
    "                \"degree\": 0,\n",
    "                \"mse\": 9999.0,\n",
    "                \"coeffs\": [0.0]\n",
    "            }\n",
    "        else:\n",
    "            best_poly[col] = {\n",
    "                \"degree\": best_deg,\n",
    "                \"mse\": best_mse,\n",
    "                \"coeffs\": best_coefs\n",
    "            }\n",
    "\n",
    "    return best_poly, df_merged\n",
    "\n",
    "def format_poly(coefs):\n",
    "    terms=[]\n",
    "    deg= len(coefs)-1\n",
    "    for i,c in enumerate(coefs):\n",
    "        p= deg-i\n",
    "        sign_part= f\"{c:+.4e}\"\n",
    "        if p>1:\n",
    "            terms.append(f\"{sign_part}*x^{p}\")\n",
    "        elif p==1:\n",
    "            terms.append(f\"{sign_part}*x\")\n",
    "        else:\n",
    "            terms.append(f\"{sign_part}\")\n",
    "    expr= \"\".join(terms).replace(\"+-\",\"-\")\n",
    "    if expr.startswith(\"+\"):\n",
    "        expr= expr[1:]\n",
    "    return expr\n",
    "\n",
    "def plot_polynomial_fits(df_merged, best_poly, top5_models):\n",
    "    columns_to_fit = [\"Actual\"] + top5_models\n",
    "    if df_merged.empty or not best_poly:\n",
    "        print(\"No data to plot for polynomial fits.\")\n",
    "        return\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n=== Best Polynomial Fitting (Forced Top-5 + Actual) ===\")\n",
    "    for col in columns_to_fit:\n",
    "        info = best_poly[col]\n",
    "        expr = format_poly(info[\"coeffs\"])\n",
    "        print(f\"[{col}] => Deg={info['degree']}  ValMSE={info['mse']:.6f}\")\n",
    "        print(f\"    y = {expr}\\n\")\n",
    "\n",
    "    # Plot polynomials\n",
    "    plt.figure(figsize=(12,7))\n",
    "    color_list= [\"black\",\"red\",\"blue\",\"green\",\"orange\",\"purple\",\"pink\",\"brown\",\"olive\",\"cyan\"]\n",
    "    color_map= {}\n",
    "    color_map[\"Actual\"] = \"black\"\n",
    "    for i,m in enumerate(top5_models):\n",
    "        if i < len(color_list)-1:\n",
    "            color_map[m] = color_list[i+1]\n",
    "        else:\n",
    "            color_map[m] = \"gray\"\n",
    "\n",
    "    # Distinguish train vs. val\n",
    "    n_points = len(df_merged)\n",
    "    idx= np.arange(n_points)\n",
    "    val_mask = (idx % 4 == 0)\n",
    "    train_mask = ~val_mask\n",
    "\n",
    "    plt.scatter(df_merged.loc[train_mask,\"Date\"], df_merged.loc[train_mask,\"Actual\"],\n",
    "                color=\"black\", marker=\"o\", label=\"Actual (Train)\")\n",
    "    plt.scatter(df_merged.loc[val_mask,\"Date\"], df_merged.loc[val_mask,\"Actual\"],\n",
    "                color=\"gray\", marker=\"x\", label=\"Actual (Val)\")\n",
    "\n",
    "    x_dense = np.linspace(df_merged[\"mdates_num\"].min(), df_merged[\"mdates_num\"].max(), 300)\n",
    "    x_dense_dates= [mdates.num2date(xx) for xx in x_dense]\n",
    "\n",
    "    for col in columns_to_fit:\n",
    "        coefs= best_poly[col][\"coeffs\"]\n",
    "        y_dense= np.polyval(coefs, x_dense)\n",
    "        c= color_map.get(col,\"gray\")\n",
    "        plt.plot(x_dense_dates, y_dense, color=c, lw=2, label=f\"{col} (Deg={best_poly[col]['degree']})\")\n",
    "\n",
    "    plt.title(\"Polynomial Fits (Top-5 + Actual)\\n(2025-01-01..2025-01-31) of Apple Stocks \")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend(prop={'size':8})\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Derivatives\n",
    "    plt.figure(figsize=(12,7))\n",
    "    for col in columns_to_fit:\n",
    "        coefs= best_poly[col][\"coeffs\"]\n",
    "        d_coefs= np.polyder(coefs)\n",
    "        y_deriv= np.polyval(d_coefs, x_dense)\n",
    "        c= color_map.get(col,\"gray\")\n",
    "        plt.plot(x_dense_dates, y_deriv, color=c, lw=2,\n",
    "                 label=f\"{col} Deriv (Deg={best_poly[col]['degree']-1})\")\n",
    "\n",
    "    plt.title(\"Polynomial Derivatives (Top-5 + Actual)\\n(2025-01-01..2025-01-31) of Apple Stocks \")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"d(Price)/d(Time)\")\n",
    "    plt.legend(prop={'size':8})\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "####################   Actual Implementation of the Code   ####################\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "# A) Load & Scale\n",
    "    df_scaled, df_complete_all, scaler = load_and_scale_data(\n",
    "        \"Apple Stock Price History.csv\",\n",
    "        \"Apple Stock Price History_Complete.csv\"\n",
    "    )\n",
    "\n",
    "    # B) Rolling Forecasts for [2024-12-01 .. 2025-01-02]\n",
    "    all_model_types = [\n",
    "        \"LSTM\",\"GRU\",\"RNN\",\"CNN\",\"Transformer\",\"N-BEATS\",\"N-HITS\",\"SVM\",\"Boost\",\"MoE\"\n",
    "    ]\n",
    "    final_rolling_fc= rolling_train_validate_predict_moe(\n",
    "        df_full= df_complete_all,\n",
    "        scaler_obj= scaler,\n",
    "        model_types= all_model_types,\n",
    "        start_train_cutoff= train_cutoff,\n",
    "        start_val_cutoff=   val_cutoff,\n",
    "        start_pred= pd.to_datetime(\"2024-12-01\"),\n",
    "        end_pred=   pd.to_datetime(\"2025-01-02\"),\n",
    "        horizon_days= 30\n",
    "    )\n",
    "    print(\"\\n=== HEAD of final_rolling_fc ===\")\n",
    "    print(final_rolling_fc.head(10))\n",
    "\n",
    "    # C) If MoE wasn't loaded, compute it via “Super Advanced” approach\n",
    "    if not any(final_rolling_fc[\"Model\"]==\"MoE\"):\n",
    "        df_moe= compute_super_advanced_moe(final_rolling_fc, df_complete_all)\n",
    "        df_rolled_all= combine_base_and_moe(final_rolling_fc, df_moe)\n",
    "    else:\n",
    "        df_rolled_all= final_rolling_fc.copy()\n",
    "\n",
    "    # D) Quick Plot of All Models in Jan 2025\n",
    "    plot_predicted_price_all_models(\n",
    "        df_rolled_all,\n",
    "        from_d= PLOT_START,\n",
    "        to_d=   PLOT_END\n",
    "    )\n",
    "\n",
    "    # E) Merge w/ actual => daily differences\n",
    "    df_diff_all= merge_with_actual_and_diff(df_rolled_all, df_complete_all)\n",
    "    plot_daily_differences_all_models(df_diff_all)\n",
    "\n",
    "    # F) Evaluate errors => df_scores => forcibly select top-5 (MoE included)\n",
    "    df_scores= evaluate_models_confidence(df_diff_all)\n",
    "    print(\"\\n=== Model Scores (Incl. MoE) for Jan 2025 ===\")\n",
    "    print(df_scores)\n",
    "\n",
    "    df_top5_incl_moe= select_top5_including_moe(df_scores)\n",
    "    print(\"\\n=== Forced Top-5 (MoE + 4) with Re-Normalized Confidence ===\")\n",
    "    print(df_top5_incl_moe)\n",
    "\n",
    "    # Convert that combined_confidence into a simple static_conf dict\n",
    "    # (We’ll treat these as the \"static\" portion for the dynamic weighting)\n",
    "    static_conf = {}\n",
    "    for i,row in df_top5_incl_moe.iterrows():\n",
    "        static_conf[row[\"Model\"]] = row[\"combined_confidence\"]\n",
    "\n",
    "    # G) Polynomial Fit for these top-5\n",
    "    best_poly, df_test_poly = polynomial_fit_top5_models(df_diff_all, df_top5_incl_moe, max_degree=70)\n",
    "    plot_polynomial_fits(df_test_poly, best_poly, df_top5_incl_moe[\"Model\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# …existing code…\n",
    "df_sim, final_conf_history, w_static, w_fit, w_grad = confidence_based_voting_trading(\n",
    "    df_test_poly= df_test_poly,\n",
    "    best_poly_info= best_poly,\n",
    "    best_model_names= df_top5_incl_moe[\"Model\"].tolist(),\n",
    "    static_conf= static_conf\n",
    ")\n",
    "# …existing code…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim, final_conf_history= confidence_based_voting_trading(\n",
    "        df_test_poly= df_test_poly,\n",
    "        best_poly_info= best_poly,\n",
    "        best_model_names= df_top5_incl_moe[\"Model\"].tolist(),\n",
    "        static_conf= static_conf)\n",
    "\n",
    "# I) Piecewise offset-based simulation\n",
    "print(static_conf)\n",
    "print( df_top5_incl_moe[\"Model\"].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
