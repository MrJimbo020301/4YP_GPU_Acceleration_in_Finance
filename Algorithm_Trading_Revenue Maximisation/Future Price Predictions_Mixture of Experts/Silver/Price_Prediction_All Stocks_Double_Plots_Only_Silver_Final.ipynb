{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True  # Let CuDNN pick the fastest algorithm for your hardware\n",
    "\n",
    "# Additional imports for sklearn-based models:\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "###############################################################################\n",
    "# 1. Set random seeds\n",
    "###############################################################################\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "###############################################################################\n",
    "# Global hyperparameters\n",
    "###############################################################################\n",
    "WINDOW_SIZE = 35\n",
    "NUM_EPOCHS  = 2000\n",
    "\n",
    "features = [\"Price\", \"Open\", \"High\", \"Low\"]\n",
    "csv_file = \"Silver Futures Historical Data.csv\"\n",
    "\n",
    "###############################################################################\n",
    "# 2. Load & Clean CSV\n",
    "###############################################################################\n",
    "df = pd.read_csv(csv_file)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "df.drop(columns=[\"Vol.\", \"Change %\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "for col in features:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "    else:\n",
    "        df[col] = df[col].astype(str).str.replace(\",\", \"\", regex=True)\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "df.sort_values(\"Date\", inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"CSV date range:\")\n",
    "print(\"  Min date:\", df[\"Date\"].min())\n",
    "print(\"  Max date:\", df[\"Date\"].max())\n",
    "print(\"Number of rows in df:\", len(df))\n",
    "print(\"Rows that have all features = NaN:\", df[features].isna().all(axis=1).sum())\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3. Date cutoffs: TRAIN, VAL, TEST\n",
    "# ------------------------------------------------------------------------\n",
    "train_cutoff = pd.to_datetime(\"2024-08-01\")\n",
    "val_cutoff   = pd.to_datetime(\"2024-12-31\")\n",
    "test_cutoff  = pd.to_datetime(\"2025-01-02\")\n",
    "\n",
    "df_train = df[df[\"Date\"] < train_cutoff].copy()\n",
    "df_val   = df[(df[\"Date\"] >= train_cutoff) & (df[\"Date\"] <= val_cutoff)].copy()\n",
    "df_test  = df[df[\"Date\"] >= test_cutoff].copy()\n",
    "\n",
    "print(f\"TRAIN rows: {len(df_train)}\")\n",
    "print(f\"VAL   rows: {len(df_val)}\")\n",
    "print(f\"TEST  rows: {len(df_test)}\")\n",
    "\n",
    "df_train_nonan = df_train.dropna(subset=features)\n",
    "if df_train_nonan.empty:\n",
    "    raise ValueError(\"No valid numeric feature rows in the training set...\")\n",
    "\n",
    "###############################################################################\n",
    "# 4. Fit MinMaxScaler on TRAIN only\n",
    "###############################################################################\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df_train_nonan[features])\n",
    "\n",
    "df_train_scaled = df_train.copy()\n",
    "df_val_scaled   = df_val.copy()\n",
    "df_test_scaled  = df_test.copy()\n",
    "\n",
    "df_train_scaled.loc[df_train_nonan.index, features] = scaler.transform(df_train_nonan[features])\n",
    "\n",
    "val_no_nan = df_val_scaled.dropna(subset=features)\n",
    "if not val_no_nan.empty:\n",
    "    df_val_scaled.loc[val_no_nan.index, features] = scaler.transform(val_no_nan[features])\n",
    "\n",
    "train_mins = df_train_nonan[features].min()\n",
    "df_test_filled = df_test_scaled[features].fillna(train_mins)\n",
    "df_test_scaled.loc[:, features] = scaler.transform(df_test_filled)\n",
    "\n",
    "# Combine all scaled data (for validation and test walk-forward procedures)\n",
    "df_scaled = pd.concat([df_train_scaled, df_val_scaled, df_test_scaled], ignore_index=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current device:\", device)\n",
    "\n",
    "###############################################################################\n",
    "# 5A. Create the training sequences\n",
    "###############################################################################\n",
    "def make_sequences(df_subset, window=WINDOW_SIZE):\n",
    "    arr = df_subset[features].values\n",
    "    dts = df_subset[\"Date\"].values\n",
    "    X_list, y_list, date_list = [], [], []\n",
    "    for i in range(window, len(arr)):\n",
    "        X_window = arr[i - window: i]\n",
    "        y_target = arr[i]\n",
    "        X_list.append(X_window)\n",
    "        y_list.append(y_target)\n",
    "        date_list.append(dts[i])\n",
    "    return np.array(X_list), np.array(y_list), np.array(date_list)\n",
    "\n",
    "train_scaled_no_nan = df_train_scaled.dropna(subset=features)\n",
    "X_train_all, y_train_all, train_dates_all = make_sequences(train_scaled_no_nan, window=WINDOW_SIZE)\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train_all).float()\n",
    "y_train_tensor = torch.from_numpy(y_train_all).float()\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=len(train_dataset),  # single-batch approach\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "# For sklearn-based models, flatten the time window.\n",
    "X_train_flat = X_train_all.reshape((X_train_all.shape[0], WINDOW_SIZE * 4))\n",
    "# y_train_all is shape: (n_samples, 4)\n",
    "\n",
    "###############################################################################\n",
    "# 5B. \"Half-blind\" validation loss function (for training)\n",
    "###############################################################################\n",
    "def half_blind_validation_loss(model, df_scaled, val_start, val_end, window=WINDOW_SIZE):\n",
    "    df_work = df_scaled.copy()\n",
    "    df_work.sort_values(\"Date\", inplace=True)\n",
    "    \n",
    "    val_mask = (df_work[\"Date\"] >= val_start) & (df_work[\"Date\"] <= val_end)\n",
    "    val_dates = np.sort(df_work.loc[val_mask, \"Date\"].unique())\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for d in val_dates:\n",
    "            idx_current = df_work.index[df_work[\"Date\"] == d]\n",
    "            if len(idx_current) == 0:\n",
    "                continue\n",
    "            idx_current = idx_current[0]\n",
    "            start_idx = idx_current - window\n",
    "            if start_idx < 0:\n",
    "                continue\n",
    "            window_df = df_work.iloc[start_idx:idx_current][features]\n",
    "            window_df = window_df.apply(pd.to_numeric, errors=\"coerce\").astype(float)\n",
    "            window_data = window_df.values\n",
    "            if np.isnan(window_data).any():\n",
    "                continue\n",
    "            X_input = torch.from_numpy(window_data).float().unsqueeze(0).to(device)\n",
    "            pred_4_scaled = model(X_input).cpu()[0]\n",
    "            true_vals = df_work.loc[idx_current, features]\n",
    "            true_vals = pd.to_numeric(true_vals, errors=\"coerce\").astype(float).values\n",
    "            if np.isnan(true_vals).any():\n",
    "                continue\n",
    "            loss = criterion(pred_4_scaled, torch.tensor(true_vals).float())\n",
    "            losses.append(loss.item())\n",
    "    if len(losses) == 0:\n",
    "        return 999999.0\n",
    "    return np.mean(losses)\n",
    "\n",
    "###############################################################################\n",
    "# 5C. \"Half-blind\" validation predictions (non-iterative version)\n",
    "###############################################################################\n",
    "def half_blind_validation_preds_df(model, df_scaled, val_start, val_end, window=WINDOW_SIZE):\n",
    "    df_work = df_scaled.copy()\n",
    "    df_work.sort_values(\"Date\", inplace=True)\n",
    "    \n",
    "    val_mask = (df_work[\"Date\"] >= val_start) & (df_work[\"Date\"] <= val_end)\n",
    "    val_dates = np.sort(df_work.loc[val_mask, \"Date\"].unique())\n",
    "    \n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for d in val_dates:\n",
    "            idx_current = df_work.index[df_work[\"Date\"] == d]\n",
    "            if len(idx_current) == 0:\n",
    "                continue\n",
    "            idx_current = idx_current[0]\n",
    "            start_idx = idx_current - window\n",
    "            if start_idx < 0:\n",
    "                continue\n",
    "            window_df = df_work.iloc[start_idx:idx_current][features]\n",
    "            window_df = window_df.apply(pd.to_numeric, errors=\"coerce\").astype(float)\n",
    "            window_data = window_df.values\n",
    "            if np.isnan(window_data).any():\n",
    "                continue\n",
    "            X_input = torch.from_numpy(window_data).float().unsqueeze(0).to(device)\n",
    "            pred_4_scaled = model(X_input).cpu().numpy()[0]\n",
    "            preds.append((d, *pred_4_scaled))\n",
    "    \n",
    "    pred_df = pd.DataFrame(\n",
    "        preds,\n",
    "        columns=[\"Date\", \"Pred_Price_scaled\", \"Pred_Open_scaled\", \"Pred_High_scaled\", \"Pred_Low_scaled\"]\n",
    "    )\n",
    "    pred_df.sort_values(\"Date\", inplace=True)\n",
    "    scl_array = pred_df[[\"Pred_Price_scaled\", \"Pred_Open_scaled\", \"Pred_High_scaled\", \"Pred_Low_scaled\"]].values\n",
    "    unsc_array = scaler.inverse_transform(scl_array)\n",
    "    pred_df[\"Pred_Price_unscaled\"] = unsc_array[:, 0]\n",
    "    pred_df[\"Pred_Open_unscaled\"]  = unsc_array[:, 1]\n",
    "    pred_df[\"Pred_High_unscaled\"]  = unsc_array[:, 2]\n",
    "    pred_df[\"Pred_Low_unscaled\"]   = unsc_array[:, 3]\n",
    "    return pred_df\n",
    "\n",
    "###############################################################################\n",
    "# Iterative (next-day) half-blind validation predictions\n",
    "###############################################################################\n",
    "def iterative_half_blind_validation_preds(model, df_all_scaled, val_start, val_end, window=WINDOW_SIZE):\n",
    "    df_work = df_all_scaled.copy()\n",
    "    df_work.sort_values(\"Date\", inplace=True)\n",
    "    \n",
    "    val_mask = (df_work[\"Date\"] >= val_start) & (df_work[\"Date\"] <= val_end)\n",
    "    val_dates = np.sort(df_work.loc[val_mask, \"Date\"].unique())\n",
    "    \n",
    "    # Identify the starting index for validation.\n",
    "    val_idx = df_work.index[df_work[\"Date\"] >= val_start]\n",
    "    if len(val_idx) == 0:\n",
    "        raise ValueError(\"No validation data available after val_start.\")\n",
    "    first_val_idx = val_idx[0]\n",
    "    \n",
    "    current_window = df_work.iloc[first_val_idx - window:first_val_idx][features].values\n",
    "    preds = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for d in val_dates:\n",
    "            X_input = torch.from_numpy(current_window).float().unsqueeze(0).to(device)\n",
    "            pred_scaled = model(X_input).cpu().numpy()[0]\n",
    "            preds.append((d, *pred_scaled))\n",
    "            \n",
    "            # \"Uncover\" the actual observation for day d.\n",
    "            actual_row = df_work[df_work[\"Date\"] == d][features].values\n",
    "            if actual_row.shape[0] == 0:\n",
    "                continue\n",
    "            actual = actual_row[0]\n",
    "            current_window = np.vstack((current_window[1:], actual))\n",
    "    \n",
    "    pred_df = pd.DataFrame(\n",
    "        preds,\n",
    "        columns=[\"Date\", \"Pred_Price_scaled\", \"Pred_Open_scaled\", \"Pred_High_scaled\", \"Pred_Low_scaled\"]\n",
    "    )\n",
    "    pred_df.sort_values(\"Date\", inplace=True)\n",
    "    \n",
    "    scl_array = pred_df[[\"Pred_Price_scaled\", \"Pred_Open_scaled\", \"Pred_High_scaled\", \"Pred_Low_scaled\"]].values\n",
    "    unsc_array = inverse_transform_4cols(scl_array)\n",
    "    pred_df[\"Pred_Price_unscaled\"] = unsc_array[:, 0]\n",
    "    pred_df[\"Pred_Open_unscaled\"]  = unsc_array[:, 1]\n",
    "    pred_df[\"Pred_High_unscaled\"]  = unsc_array[:, 2]\n",
    "    pred_df[\"Pred_Low_unscaled\"]   = unsc_array[:, 3]\n",
    "    return pred_df\n",
    "\n",
    "###############################################################################\n",
    "# 6. Define the base PyTorch model (supports several types)\n",
    "###############################################################################\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_type=\"LSTM\", window_width=WINDOW_SIZE):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.window_width = window_width\n",
    "        \n",
    "        if model_type == \"CNN\":\n",
    "            self.conv1 = nn.Conv1d(in_channels=4, out_channels=64, kernel_size=3)\n",
    "            self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "            with torch.no_grad():\n",
    "                dummy_input = torch.zeros(1, 4, self.window_width)\n",
    "                dummy_output = self.conv2(F.relu(self.conv1(dummy_input)))\n",
    "                conv_output_size = dummy_output.shape[1] * dummy_output.shape[2]\n",
    "            self.fc = nn.Linear(conv_output_size, 4)\n",
    "        elif model_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(input_size=4, hidden_size=128, num_layers=2,\n",
    "                               batch_first=True, dropout=0.1)\n",
    "            self.fc = nn.Linear(128, 4)\n",
    "        elif model_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(input_size=4, hidden_size=128, num_layers=2,\n",
    "                              batch_first=True, dropout=0.1)\n",
    "            self.fc = nn.Linear(128, 4)\n",
    "        elif model_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(input_size=4, hidden_size=128, num_layers=2,\n",
    "                              batch_first=True, nonlinearity=\"relu\", dropout=0.1)\n",
    "            self.fc = nn.Linear(128, 4)\n",
    "        elif model_type == \"EnhancedLSTM\":\n",
    "            self.rnn = nn.LSTM(input_size=4, hidden_size=128, num_layers=3,\n",
    "                               batch_first=True, dropout=0.2)\n",
    "            self.bn = nn.BatchNorm1d(128)\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "            self.fc = nn.Linear(128, 4)\n",
    "        elif model_type == \"Transformer\":\n",
    "            self.input_linear = nn.Linear(4, 128)\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8, dropout=0.1)\n",
    "            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
    "            self.fc = nn.Linear(128, 4)\n",
    "        elif model_type == \"N-BEATS\":\n",
    "            self.input_size = window_width * 4\n",
    "            self.blocks = nn.ModuleList([nn.Sequential(\n",
    "                nn.Linear(self.input_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 4)\n",
    "            ) for _ in range(3)])\n",
    "        elif model_type == \"N-HITS\":\n",
    "            self.input_size = window_width * 4\n",
    "            self.blocks = nn.ModuleList([nn.Sequential(\n",
    "                nn.Linear(self.input_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 4)\n",
    "            ) for _ in range(3)])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.model_type == \"CNN\":\n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = x.view(x.size(0), -1)\n",
    "            out = self.fc(x)\n",
    "        elif self.model_type in [\"LSTM\", \"GRU\", \"RNN\"]:\n",
    "            out, _ = self.rnn(x)\n",
    "            out = out[:, -1, :]\n",
    "            out = self.fc(out)\n",
    "        elif self.model_type == \"EnhancedLSTM\":\n",
    "            out, _ = self.rnn(x)\n",
    "            out = out[:, -1, :]\n",
    "            out = self.bn(out)\n",
    "            out = self.dropout(out)\n",
    "            out = self.fc(out)\n",
    "        elif self.model_type == \"Transformer\":\n",
    "            x = self.input_linear(x)  # [batch, window, 128]\n",
    "            x = x.permute(1, 0, 2)     # [window, batch, 128]\n",
    "            x = self.transformer_encoder(x)\n",
    "            x = x[-1, :, :]\n",
    "            out = self.fc(x)\n",
    "        elif self.model_type in [\"N-BEATS\", \"N-HITS\"]:\n",
    "            x_flat = x.reshape(x.size(0), -1)\n",
    "            forecast = 0\n",
    "            for block in self.blocks:\n",
    "                forecast = forecast + block(x_flat)\n",
    "            out = forecast\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type\")\n",
    "        return out\n",
    "\n",
    "###############################################################\n",
    "# Early Stopping class\n",
    "###############################################################\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=35, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "###############################################################\n",
    "# 7. Inverse transform helper\n",
    "###############################################################\n",
    "def inverse_transform_4cols(y_4):\n",
    "    return scaler.inverse_transform(y_4)\n",
    "\n",
    "###############################################################\n",
    "# 8. Training function for PyTorch models\n",
    "###############################################################\n",
    "def train_model(model_type=\"LSTM\", num_epochs=NUM_EPOCHS):\n",
    "    model = BaseModel(model_type, window_width=WINDOW_SIZE).to(device)\n",
    "    best_path = f\"best_{model_type}_Silver_V4.pt\"\n",
    "    \n",
    "    # If the best model file exists, load and return the model immediately.\n",
    "    if os.path.exists(best_path):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "            model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "        print(f\"[{model_type}] Loaded existing model from {best_path}\")\n",
    "        return model\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.005)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=35)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=35)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for Xb, yb in train_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(Xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        \n",
    "        # ---------------- (1) Validate every 5 epochs, print every 100 --------------\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            val_loss = half_blind_validation_loss(\n",
    "                model,\n",
    "                df_scaled,\n",
    "                val_start=train_cutoff,\n",
    "                val_end=val_cutoff,\n",
    "                window=WINDOW_SIZE\n",
    "            )\n",
    "            scheduler.step(val_loss)\n",
    "            early_stopping(val_loss)\n",
    "            \n",
    "            # Update best model if improved\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), best_path)\n",
    "            \n",
    "            # (2) Print out model performance only every 100 epochs\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"[{model_type}] Epoch {epoch+1}, Train={avg_train_loss:.6f}, Val={val_loss:.6f}\")\n",
    "            \n",
    "            # Check early stopping\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"[{model_type}] Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        # -----------------------------------------------------------------------------\n",
    "        else:\n",
    "            pass  # Skip validation for these epochs for speed\n",
    "\n",
    "    # Load best model if saved\n",
    "    if os.path.exists(best_path):\n",
    "        model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    return model\n",
    "\n",
    "###############################################################\n",
    "# 8B. Training function for sklearn-based models (SVM, GPR, Boost)\n",
    "###############################################################\n",
    "class SklearnWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        x_np = x.cpu().detach().numpy().reshape(1, -1)\n",
    "        pred = self.model.predict(x_np)  # shape: (1,4)\n",
    "        return torch.from_numpy(pred).float().to(x.device)\n",
    "    def eval(self):\n",
    "        pass\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "def train_model_sklearn(model_type, X_train_flat, y_train):\n",
    "    best_path = f\"best_{model_type}_Silver_V4.pkl\"\n",
    "    if os.path.exists(best_path):\n",
    "        model = joblib.load(best_path)\n",
    "        print(f\"[{model_type}] Loaded existing model from {best_path}\")\n",
    "    else:\n",
    "        if model_type == \"SVM\":\n",
    "            base_model = SVR(kernel='rbf', C=10.0, epsilon=0.01)\n",
    "        elif model_type == \"GPR\":\n",
    "            from sklearn.gaussian_process.kernels import RBF\n",
    "            kernel = RBF(length_scale=1.0)\n",
    "            base_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-2)\n",
    "        elif model_type == \"Boost\":\n",
    "            base_model = GradientBoostingRegressor(n_estimators=200, max_depth=3)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported sklearn model type\")\n",
    "        multi_model = MultiOutputRegressor(base_model)\n",
    "        multi_model.fit(X_train_flat, y_train)\n",
    "        joblib.dump(multi_model, best_path)\n",
    "        model = multi_model\n",
    "    return SklearnWrapper(model)\n",
    "\n",
    "###############################################################\n",
    "# 9. Validation predictions + Test predictions\n",
    "###############################################################\n",
    "def walk_forward_test(model, df_all_scaled, test_start, window=WINDOW_SIZE):\n",
    "    df_work = df_all_scaled.copy()\n",
    "    df_work.sort_values(\"Date\", inplace=True)\n",
    "    preds = []\n",
    "    test_dates = np.sort(df_work[df_work[\"Date\"] >= test_start][\"Date\"].unique())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for d in test_dates:\n",
    "            idx_current = df_work.index[df_work[\"Date\"] == d]\n",
    "            if len(idx_current) == 0:\n",
    "                continue\n",
    "            idx_current = idx_current[0]\n",
    "            start_idx = idx_current - window\n",
    "            if start_idx < 0:\n",
    "                continue\n",
    "            window_df = df_work.iloc[start_idx:idx_current][features]\n",
    "            window_df = window_df.apply(pd.to_numeric, errors=\"coerce\").astype(float)\n",
    "            window_data = window_df.values\n",
    "            if np.isnan(window_data).any():\n",
    "                continue\n",
    "            X_input = torch.from_numpy(window_data).float().unsqueeze(0).to(device)\n",
    "            y_pred_4_scaled = model(X_input).cpu().numpy()[0]\n",
    "            df_work.loc[idx_current, features] = y_pred_4_scaled\n",
    "            preds.append((d, *y_pred_4_scaled))\n",
    "    pred_df = pd.DataFrame(\n",
    "        preds,\n",
    "        columns=[\"Date\", \"Pred_Price_scaled\", \"Pred_Open_scaled\", \"Pred_High_scaled\", \"Pred_Low_scaled\"]\n",
    "    )\n",
    "    pred_df.sort_values(\"Date\", inplace=True)\n",
    "    scl_array = pred_df[[\"Pred_Price_scaled\", \"Pred_Open_scaled\", \"Pred_High_scaled\", \"Pred_Low_scaled\"]].values\n",
    "    unsc_array = inverse_transform_4cols(scl_array)\n",
    "    pred_df[\"Pred_Price_unscaled\"] = unsc_array[:, 0]\n",
    "    pred_df[\"Pred_Open_unscaled\"]  = unsc_array[:, 1]\n",
    "    pred_df[\"Pred_High_unscaled\"]  = unsc_array[:, 2]\n",
    "    pred_df[\"Pred_Low_unscaled\"]   = unsc_array[:, 3]\n",
    "    return pred_df\n",
    "\n",
    "###############################################################\n",
    "# 10. Evaluate & Plot\n",
    "###############################################################\n",
    "def evaluate_and_plot_all_models(results_dict):\n",
    "    df_real = df_scaled[[\"Date\", \"Price\", \"Open\", \"High\", \"Low\"]].copy()\n",
    "    df_real.sort_values(\"Date\", inplace=True)\n",
    "    mask_trainval = (df_real[\"Date\"] < test_cutoff)\n",
    "    df_real_plot = df_real.loc[mask_trainval].copy()\n",
    "    real_4_scaled = df_real_plot[features].values\n",
    "    real_4_unscaled = inverse_transform_4cols(real_4_scaled)\n",
    "    df_real_plot[\"Real_Price_unscaled\"] = real_4_unscaled[:, 0]\n",
    "    \n",
    "    plt.figure(figsize=(14,7))\n",
    "    plt.plot(df_real_plot[\"Date\"], df_real_plot[\"Real_Price_unscaled\"],\n",
    "             color=\"black\", label=\"Actual Price (Train+Val)\")\n",
    "    \n",
    "    color_map = {\n",
    "        \"LSTM\": \"red\",\n",
    "        \"GRU\": \"blue\",\n",
    "        \"RNN\": \"green\",\n",
    "        \"CNN\": \"orange\",\n",
    "        \"EnhancedLSTM\": \"magenta\",\n",
    "        \"Transformer\": \"cyan\",\n",
    "        \"N-BEATS\": \"brown\",\n",
    "        \"N-HITS\": \"pink\",\n",
    "        \"SVM\": \"olive\",\n",
    "        \"GPR\": \"teal\",\n",
    "        \"Boost\": \"purple\"\n",
    "    }\n",
    "    \n",
    "    merged_df = df_real.copy()\n",
    "    merged_df.sort_values(\"Date\", inplace=True)\n",
    "    \n",
    "    for model_name, info in results_dict.items():\n",
    "        val_pred_df = info[\"val_pred_df\"]\n",
    "        test_pred_df = info[\"test_pred_df\"]\n",
    "        frames = []\n",
    "        if val_pred_df is not None:\n",
    "            frames.append(val_pred_df.rename(columns={\"Pred_Price_unscaled\": f\"Pred_{model_name}\"}))\n",
    "        if test_pred_df is not None:\n",
    "            frames.append(test_pred_df.rename(columns={\"Pred_Price_unscaled\": f\"Pred_{model_name}\"}))\n",
    "        if not frames:\n",
    "            continue\n",
    "        combined_df = pd.concat(frames, ignore_index=True)\n",
    "        combined_df = combined_df[[\"Date\", f\"Pred_{model_name}\"]].copy()\n",
    "        combined_df.drop_duplicates(\"Date\", keep=\"last\", inplace=True)\n",
    "        combined_df.sort_values(\"Date\", inplace=True)\n",
    "        merged_df = pd.merge(merged_df, combined_df, on=\"Date\", how=\"left\")\n",
    "    \n",
    "    for model_name, info in results_dict.items():\n",
    "        col = f\"Pred_{model_name}\"\n",
    "        if col in merged_df.columns:\n",
    "            plt.plot(merged_df[\"Date\"], merged_df[col],\n",
    "                     color=color_map.get(model_name, \"gray\"),\n",
    "                     linestyle=\"-\",\n",
    "                     label=f\"{model_name} Prediction\")\n",
    "    \n",
    "    plt.axvspan(df_train[\"Date\"].min(), train_cutoff, color=\"skyblue\", alpha=0.1, label=\"Train\")\n",
    "    plt.axvspan(train_cutoff, val_cutoff, color=\"green\", alpha=0.1, label=\"Validation\")\n",
    "    plt.axvspan(test_cutoff, merged_df[\"Date\"].max(), color=\"yellow\", alpha=0.1, label=\"Test\")\n",
    "    \n",
    "    plt.title(\"Joint Diagram: Actual Price vs. Model Predictions of Silver's Stock\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price (unscaled)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Zoom-in Plot for the test period.\n",
    "    df_test_zoom = merged_df[merged_df[\"Date\"] >= test_cutoff]\n",
    "    if not df_test_zoom.empty:\n",
    "        plt.figure(figsize=(12,5))\n",
    "        for model_name, info in results_dict.items():\n",
    "            col = f\"Pred_{model_name}\"\n",
    "            if col in df_test_zoom.columns:\n",
    "                plt.plot(df_test_zoom[\"Date\"], df_test_zoom[col],\n",
    "                         color=color_map.get(model_name, \"gray\"),\n",
    "                         linestyle=\"--\",\n",
    "                         label=f\"{model_name} Prediction\")\n",
    "        \n",
    "        # Load actual test prices from the complete CSV.\n",
    "        df_complete = pd.read_csv(\"Silver Futures Historical Data_Complete.csv\")\n",
    "        df_complete.drop(columns=[\"Vol.\", \"Change %\"], errors=\"ignore\", inplace=True)\n",
    "        df_complete['Date'] = pd.to_datetime(df_complete['Date'])\n",
    "        df_complete.sort_values(\"Date\", inplace=True)\n",
    "        df_test_actual = df_complete[df_complete[\"Date\"] >= test_cutoff].copy()\n",
    "        plt.plot(df_test_actual[\"Date\"], df_test_actual[\"Price\"],\n",
    "                 color='black', linewidth=2, label='Actual Test Price')\n",
    "        \n",
    "        plt.title(\"Test Period Zoom-in on Silver's Stock\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Price (unscaled)\")\n",
    "        plt.legend(prop={'size':8})\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "###############################################################\n",
    "# 11. MAIN\n",
    "###############################################################\n",
    "if __name__ == \"__main__\":\n",
    "    all_model_types = [\"LSTM\", \"GRU\", \"RNN\", \"CNN\", \"Transformer\", \"N-BEATS\", \"N-HITS\", \"SVM\", \"Boost\"]\n",
    "    results_dict = {}\n",
    "    \n",
    "    for mt in all_model_types:\n",
    "        print(f\"\\n=== Training {mt} model ===\")\n",
    "        if mt in [\"SVM\", \"GPR\", \"Boost\"]:\n",
    "            model = train_model_sklearn(mt, X_train_flat, y_train_all)\n",
    "        else:\n",
    "            model = train_model(mt, num_epochs=NUM_EPOCHS)\n",
    "        \n",
    "        if mt in [\"SVM\", \"Transformer\", \"Boost\"]:\n",
    "            val_pred_df = iterative_half_blind_validation_preds(\n",
    "                model,\n",
    "                df_scaled,\n",
    "                val_start=train_cutoff,\n",
    "                val_end=val_cutoff,\n",
    "                window=WINDOW_SIZE\n",
    "            )\n",
    "        else:\n",
    "            val_pred_df = half_blind_validation_preds_df(\n",
    "                model,\n",
    "                df_scaled,\n",
    "                val_start=train_cutoff,\n",
    "                val_end=val_cutoff,\n",
    "                window=WINDOW_SIZE\n",
    "            )\n",
    "        \n",
    "        test_pred_df = walk_forward_test(\n",
    "            model,\n",
    "            df_all_scaled=df_scaled,\n",
    "            test_start=test_cutoff,\n",
    "            window=WINDOW_SIZE\n",
    "        )\n",
    "        \n",
    "        results_dict[mt] = {\n",
    "            \"val_pred_df\": val_pred_df,\n",
    "            \"test_pred_df\": test_pred_df\n",
    "        }\n",
    "        print(f\"{mt} has no real test data => skip MAPE\")\n",
    "    \n",
    "    evaluate_and_plot_all_models(results_dict)\n",
    "\n",
    "# ================================\n",
    "# Additional Plot: Joint Diagram of Actual vs. Predicted Prices\n",
    "# ================================\n",
    "\n",
    "df_actual = pd.read_csv(\"Silver Futures Historical Data_Complete.csv\")\n",
    "df_actual.drop(columns=[\"Vol.\", \"Change %\"], errors=\"ignore\", inplace=True)\n",
    "df_actual['Date'] = pd.to_datetime(df_actual['Date'])\n",
    "df_actual.sort_values(by='Date', ascending=True, inplace=True)\n",
    "df_actual.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Clean numerical columns: remove commas and convert to float\n",
    "num_cols = df_actual.columns.drop(\"Date\")\n",
    "df_actual[num_cols] = df_actual[num_cols].replace({',': ''}, regex=True)\n",
    "df_actual[num_cols] = df_actual[num_cols].astype('float64')\n",
    "\n",
    "actual_dates = df_actual['Date']\n",
    "actual_prices = df_actual['Price']\n",
    "\n",
    "for model_name, pred_info in results_dict.items():\n",
    "    if pred_info[\"test_pred_df\"] is not None and not pred_info[\"test_pred_df\"].empty:\n",
    "        pred_info[\"test_pred_df\"]['Date'] = pd.to_datetime(pred_info[\"test_pred_df\"]['Date'])\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(actual_dates, actual_prices, color='black', linewidth=2, label='Actual Price')\n",
    "\n",
    "model_colors = {\n",
    "    \"LSTM\": \"red\",\n",
    "    \"GRU\": \"blue\",\n",
    "    \"RNN\": \"green\",\n",
    "    \"CNN\": \"orange\",\n",
    "    \"Transformer\": \"cyan\",\n",
    "    \"N-BEATS\": \"brown\",\n",
    "    \"N-HITS\": \"pink\",\n",
    "    \"SVM\": \"olive\",\n",
    "    \"Boost\": \"purple\"\n",
    "}\n",
    "\n",
    "for model_name, pred_info in results_dict.items():\n",
    "    test_df = pred_info[\"test_pred_df\"]\n",
    "    if test_df is not None and not test_df.empty:\n",
    "        plt.plot(pd.to_datetime(test_df[\"Date\"]), test_df[\"Pred_Price_unscaled\"],\n",
    "                 color=model_colors.get(model_name, \"gray\"),\n",
    "                 linestyle=\"--\", linewidth=2,\n",
    "                 label=f'{model_name} Prediction')\n",
    "\n",
    "test_period_start = pd.to_datetime(\"2025-01-02\")\n",
    "plt.axvspan(test_period_start, actual_dates.iloc[-1], color='yellow', alpha=0.1, label='Test Period')\n",
    "plt.axvspan(df_train[\"Date\"].min(), train_cutoff, color=\"skyblue\", alpha=0.1, label=\"Train\")\n",
    "plt.axvspan(train_cutoff, val_cutoff, color=\"green\", alpha=0.1, label=\"Validation\")\n",
    "\n",
    "plt.title(\"Joint Diagram: Actual Price vs. Model Predictions\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
