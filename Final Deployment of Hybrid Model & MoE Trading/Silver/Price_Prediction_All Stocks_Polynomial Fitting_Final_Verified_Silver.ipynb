{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "# Additional imports for sklearn-based models:\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "###############################################################\n",
    "# 1. Set random seeds\n",
    "###############################################################\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "###############################################################\n",
    "# Global hyperparameters\n",
    "###############################################################\n",
    "WINDOW_SIZE = 35\n",
    "NUM_EPOCHS  = 800\n",
    "\n",
    "features = [\"Price\", \"Open\", \"High\", \"Low\"]\n",
    "csv_file = \"Silver Futures Historical Data.csv\"\n",
    "\n",
    "###############################################################\n",
    "# 2. Load & Clean CSV\n",
    "###############################################################\n",
    "df = pd.read_csv(csv_file)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "df.drop(columns=[\"Vol.\", \"Change %\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "for col in features:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "    else:\n",
    "        df[col] = df[col].astype(str).str.replace(\",\", \"\", regex=True)\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "df.sort_values(\"Date\", inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"CSV date range:\")\n",
    "print(\"  Min date:\", df[\"Date\"].min())\n",
    "print(\"  Max date:\", df[\"Date\"].max())\n",
    "print(\"Number of rows in df:\", len(df))\n",
    "print(\"Rows that have all features = NaN:\", df[features].isna().all(axis=1).sum())\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3. Date cutoffs: TRAIN, VAL, TEST\n",
    "# ------------------------------------------------------------------------\n",
    "train_cutoff = pd.to_datetime(\"2024-08-01\")\n",
    "val_cutoff   = pd.to_datetime(\"2024-12-31\")\n",
    "test_cutoff  = pd.to_datetime(\"2025-01-02\")\n",
    "\n",
    "df_train = df[df[\"Date\"] < train_cutoff].copy()\n",
    "df_val   = df[(df[\"Date\"] >= train_cutoff) & (df[\"Date\"] <= val_cutoff)].copy()\n",
    "df_test  = df[df[\"Date\"] >= test_cutoff].copy()\n",
    "\n",
    "print(f\"TRAIN rows: {len(df_train)}\")\n",
    "print(f\"VAL   rows: {len(df_val)}\")\n",
    "print(f\"TEST  rows: {len(df_test)}\")\n",
    "\n",
    "df_train_nonan = df_train.dropna(subset=features)\n",
    "if df_train_nonan.empty:\n",
    "    raise ValueError(\"No valid numeric feature rows in the training set...\")\n",
    "\n",
    "###############################################################\n",
    "# 4. Fit MinMaxScaler on TRAIN only\n",
    "###############################################################\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df_train_nonan[features])\n",
    "\n",
    "df_train_scaled = df_train.copy()\n",
    "df_val_scaled   = df_val.copy()\n",
    "df_test_scaled  = df_test.copy()\n",
    "\n",
    "df_train_scaled.loc[df_train_nonan.index, features] = scaler.transform(df_train_nonan[features])\n",
    "\n",
    "val_no_nan = df_val_scaled.dropna(subset=features)\n",
    "if not val_no_nan.empty:\n",
    "    df_val_scaled.loc[val_no_nan.index, features] = scaler.transform(val_no_nan[features])\n",
    "\n",
    "train_mins = df_train_nonan[features].min()\n",
    "df_test_filled = df_test_scaled[features].fillna(train_mins)\n",
    "df_test_scaled.loc[:, features] = scaler.transform(df_test_filled)\n",
    "\n",
    "# Combine all scaled data (for validation and test walk-forward procedures)\n",
    "df_scaled = pd.concat([df_train_scaled, df_val_scaled, df_test_scaled], ignore_index=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###############################################################\n",
    "# 5A. Create the training sequences\n",
    "###############################################################\n",
    "def make_sequences(df_subset, window=WINDOW_SIZE):\n",
    "    arr = df_subset[features].values\n",
    "    dts = df_subset[\"Date\"].values\n",
    "    X_list, y_list, date_list = [], [], []\n",
    "    for i in range(window, len(arr)):\n",
    "        X_window = arr[i - window: i]\n",
    "        y_target = arr[i]\n",
    "        X_list.append(X_window)\n",
    "        y_list.append(y_target)\n",
    "        date_list.append(dts[i])\n",
    "    return np.array(X_list), np.array(y_list), np.array(date_list)\n",
    "\n",
    "train_scaled_no_nan = df_train_scaled.dropna(subset=features)\n",
    "X_train_all, y_train_all, train_dates_all = make_sequences(train_scaled_no_nan, window=WINDOW_SIZE)\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train_all).float()\n",
    "y_train_tensor = torch.from_numpy(y_train_all).float()\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# For sklearn-based models we flatten the time window.\n",
    "X_train_flat = X_train_all.reshape((X_train_all.shape[0], WINDOW_SIZE * 4))\n",
    "# y_train_all remains shape: (n_samples, 4)\n",
    "\n",
    "###############################################################\n",
    "# 5B. \"Half-blind\" validation loss function (for training)\n",
    "###############################################################\n",
    "def half_blind_validation_loss(model, df_scaled, val_start, val_end, window=WINDOW_SIZE):\n",
    "    df_work = df_scaled.copy()\n",
    "    df_work.sort_values(\"Date\", inplace=True)\n",
    "    \n",
    "    val_mask = (df_work[\"Date\"] >= val_start) & (df_work[\"Date\"] <= val_end)\n",
    "    val_dates = np.sort(df_work.loc[val_mask, \"Date\"].unique())\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for d in val_dates:\n",
    "            idx_current = df_work.index[df_work[\"Date\"] == d]\n",
    "            if len(idx_current) == 0:\n",
    "                continue\n",
    "            idx_current = idx_current[0]\n",
    "            start_idx = idx_current - window\n",
    "            if start_idx < 0:\n",
    "                continue\n",
    "            window_df = df_work.iloc[start_idx:idx_current][features]\n",
    "            window_df = window_df.apply(pd.to_numeric, errors=\"coerce\").astype(float)\n",
    "            window_data = window_df.values\n",
    "            if np.isnan(window_data).any():\n",
    "                continue\n",
    "            X_input = torch.from_numpy(window_data).float().unsqueeze(0).to(device)\n",
    "            pred_4_scaled = model(X_input).cpu()[0]\n",
    "            true_vals = df_work.loc[idx_current, features]\n",
    "            true_vals = pd.to_numeric(true_vals, errors=\"coerce\").astype(float).values\n",
    "            if np.isnan(true_vals).any():\n",
    "                continue\n",
    "            loss = criterion(pred_4_scaled, torch.tensor(true_vals).float())\n",
    "            losses.append(loss.item())\n",
    "    if len(losses) == 0:\n",
    "        return 999999.0\n",
    "    return np.mean(losses)\n",
    "\n",
    "###############################################################\n",
    "# 5C. \"Half-blind\" validation predictions (non-iterative version)\n",
    "###############################################################\n",
    "def half_blind_validation_preds_df(model, df_scaled, val_start, val_end, window=WINDOW_SIZE):\n",
    "    df_work = df_scaled.copy()\n",
    "    df_work.sort_values(\"Date\", inplace=True)\n",
    "    \n",
    "    val_mask = (df_work[\"Date\"] >= val_start) & (df_work[\"Date\"] <= val_end)\n",
    "    val_dates = np.sort(df_work.loc[val_mask, \"Date\"].unique())\n",
    "    \n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for d in val_dates:\n",
    "            idx_current = df_work.index[df_work[\"Date\"] == d]\n",
    "            if len(idx_current) == 0:\n",
    "                continue\n",
    "            idx_current = idx_current[0]\n",
    "            start_idx = idx_current - window\n",
    "            if start_idx < 0:\n",
    "                continue\n",
    "            window_df = df_work.iloc[start_idx:idx_current][features]\n",
    "            window_df = window_df.apply(pd.to_numeric, errors=\"coerce\").astype(float)\n",
    "            window_data = window_df.values\n",
    "            if np.isnan(window_data).any():\n",
    "                continue\n",
    "            X_input = torch.from_numpy(window_data).float().unsqueeze(0).to(device)\n",
    "            pred_4_scaled = model(X_input).cpu().numpy()[0]\n",
    "            preds.append((d, *pred_4_scaled))\n",
    "    \n",
    "    pred_df = pd.DataFrame(\n",
    "        preds,\n",
    "        columns=[\"Date\", \"Pred_Price_scaled\", \"Pred_Open_scaled\", \"Pred_High_scaled\", \"Pred_Low_scaled\"]\n",
    "    )\n",
    "    pred_df.sort_values(\"Date\", inplace=True)\n",
    "    scl_array = pred_df[[\"Pred_Price_scaled\", \"Pred_Open_scaled\", \"Pred_High_scaled\", \"Pred_Low_scaled\"]].values\n",
    "    unsc_array = scaler.inverse_transform(scl_array)\n",
    "    pred_df[\"Pred_Price_unscaled\"] = unsc_array[:, 0]\n",
    "    pred_df[\"Pred_Open_unscaled\"]  = unsc_array[:, 1]\n",
    "    pred_df[\"Pred_High_unscaled\"]  = unsc_array[:, 2]\n",
    "    pred_df[\"Pred_Low_unscaled\"]   = unsc_array[:, 3]\n",
    "    return pred_df\n",
    "\n",
    "###############################################################\n",
    "# NEW: Iterative (next-day) half-blind validation predictions\n",
    "###############################################################\n",
    "def iterative_half_blind_validation_preds(model, df_all_scaled, val_start, val_end, window=WINDOW_SIZE):\n",
    "    \"\"\"\n",
    "    Simulates a next-day (iterative) half-blind validation.\n",
    "    It starts with the final training window (last WINDOW_SIZE rows before val_start),\n",
    "    then for each validation day:\n",
    "      - It uses the current window to predict the next day.\n",
    "      - It then “unveils” the actual observation for that day from df_all_scaled.\n",
    "      - The window is updated by dropping the oldest row and appending the actual observed row.\n",
    "    \"\"\"\n",
    "    df_work = df_all_scaled.copy()\n",
    "    df_work.sort_values(\"Date\", inplace=True)\n",
    "    \n",
    "    # Identify the starting index for validation.\n",
    "    val_idx = df_work.index[df_work[\"Date\"] >= val_start]\n",
    "    if len(val_idx) == 0:\n",
    "        raise ValueError(\"No validation data available after val_start.\")\n",
    "    first_val_idx = val_idx[0]\n",
    "    \n",
    "    # Initialize current window from the last WINDOW_SIZE rows before validation starts.\n",
    "    current_window = df_work.iloc[first_val_idx - window:first_val_idx][features].values\n",
    "    preds = []\n",
    "    \n",
    "    # Get all validation dates in order.\n",
    "    val_dates = np.sort(df_work[df_work[\"Date\"] >= val_start][\"Date\"].unique())\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for d in val_dates:\n",
    "            # Use current window to predict next-day (day d) scaled values.\n",
    "            X_input = torch.from_numpy(current_window).float().unsqueeze(0).to(device)\n",
    "            pred_scaled = model(X_input).cpu().numpy()[0]\n",
    "            preds.append((d, *pred_scaled))\n",
    "            \n",
    "            # \"Uncover\" the actual observation for day d.\n",
    "            actual_row = df_work[df_work[\"Date\"] == d][features].values\n",
    "            if actual_row.shape[0] == 0:\n",
    "                continue\n",
    "            actual = actual_row[0]\n",
    "            # Update the window: remove oldest row and append the actual observation.\n",
    "            current_window = np.vstack((current_window[1:], actual))\n",
    "    \n",
    "    pred_df = pd.DataFrame(\n",
    "        preds,\n",
    "        columns=[\"Date\", \"Pred_Price_scaled\", \"Pred_Open_scaled\", \"Pred_High_scaled\", \"Pred_Low_scaled\"]\n",
    "    )\n",
    "    pred_df.sort_values(\"Date\", inplace=True)\n",
    "    \n",
    "    scl_array = pred_df[[\"Pred_Price_scaled\", \"Pred_Open_scaled\", \"Pred_High_scaled\", \"Pred_Low_scaled\"]].values\n",
    "    unsc_array = scaler.inverse_transform(scl_array)\n",
    "    pred_df[\"Pred_Price_unscaled\"] = unsc_array[:, 0]\n",
    "    pred_df[\"Pred_Open_unscaled\"]  = unsc_array[:, 1]\n",
    "    pred_df[\"Pred_High_unscaled\"]  = unsc_array[:, 2]\n",
    "    pred_df[\"Pred_Low_unscaled\"]   = unsc_array[:, 3]\n",
    "    return pred_df\n",
    "\n",
    "###############################################################\n",
    "# 6. Define the base PyTorch model (supports several types)\n",
    "###############################################################\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_type=\"LSTM\", window_width=WINDOW_SIZE):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.window_width = window_width\n",
    "        \n",
    "        if model_type == \"CNN\":\n",
    "            self.conv1 = nn.Conv1d(in_channels=4, out_channels=64, kernel_size=3)\n",
    "            self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "            with torch.no_grad():\n",
    "                dummy_input = torch.zeros(1, 4, self.window_width)\n",
    "                dummy_output = self.conv2(F.relu(self.conv1(dummy_input)))\n",
    "                conv_output_size = dummy_output.shape[1] * dummy_output.shape[2]\n",
    "            self.fc = nn.Linear(conv_output_size, 4)\n",
    "        elif model_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(input_size=4, hidden_size=128, num_layers=2,\n",
    "                               batch_first=True, dropout=0.1)\n",
    "            self.fc = nn.Linear(128, 4)\n",
    "        elif model_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(input_size=4, hidden_size=128, num_layers=2,\n",
    "                              batch_first=True, dropout=0.1)\n",
    "            self.fc = nn.Linear(128, 4)\n",
    "        elif model_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(input_size=4, hidden_size=128, num_layers=2,\n",
    "                              batch_first=True, nonlinearity=\"relu\", dropout=0.1)\n",
    "            self.fc = nn.Linear(128, 4)\n",
    "        elif model_type == \"EnhancedLSTM\":\n",
    "            self.rnn = nn.LSTM(input_size=4, hidden_size=128, num_layers=3,\n",
    "                               batch_first=True, dropout=0.2)\n",
    "            self.bn = nn.BatchNorm1d(128)\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "            self.fc = nn.Linear(128, 4)\n",
    "        elif model_type == \"Transformer\":\n",
    "            self.input_linear = nn.Linear(4, 128)\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8, dropout=0.1)\n",
    "            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
    "            self.fc = nn.Linear(128, 4)\n",
    "        elif model_type == \"N-BEATS\":\n",
    "            self.input_size = window_width * 4\n",
    "            self.blocks = nn.ModuleList([nn.Sequential(\n",
    "                nn.Linear(self.input_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 4)\n",
    "            ) for _ in range(3)])\n",
    "        elif model_type == \"N-HITS\":\n",
    "            self.input_size = window_width * 4\n",
    "            self.blocks = nn.ModuleList([nn.Sequential(\n",
    "                nn.Linear(self.input_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 4)\n",
    "            ) for _ in range(3)])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.model_type == \"CNN\":\n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = x.view(x.size(0), -1)\n",
    "            out = self.fc(x)\n",
    "        elif self.model_type == \"LSTM\":\n",
    "            out, _ = self.rnn(x)\n",
    "            out = out[:, -1, :]\n",
    "            out = self.fc(out)\n",
    "        elif self.model_type == \"GRU\":\n",
    "            out, _ = self.rnn(x)\n",
    "            out = out[:, -1, :]\n",
    "            out = self.fc(out)\n",
    "        elif self.model_type == \"RNN\":\n",
    "            out, _ = self.rnn(x)\n",
    "            out = out[:, -1, :]\n",
    "            out = self.fc(out)\n",
    "        elif self.model_type == \"EnhancedLSTM\":\n",
    "            out, _ = self.rnn(x)\n",
    "            out = out[:, -1, :]\n",
    "            out = self.bn(out)\n",
    "            out = self.dropout(out)\n",
    "            out = self.fc(out)\n",
    "        elif self.model_type == \"Transformer\":\n",
    "            x = self.input_linear(x)  # [batch, window, 128]\n",
    "            x = x.permute(1, 0, 2)      # [window, batch, 128]\n",
    "            x = self.transformer_encoder(x)\n",
    "            x = x[-1, :, :]            # last time step\n",
    "            out = self.fc(x)\n",
    "        elif self.model_type == \"N-BEATS\":\n",
    "            x_flat = x.reshape(x.size(0), -1)\n",
    "            forecast = 0\n",
    "            for block in self.blocks:\n",
    "                forecast = forecast + block(x_flat)\n",
    "            out = forecast\n",
    "        elif self.model_type == \"N-HITS\":\n",
    "            x_flat = x.reshape(x.size(0), -1)\n",
    "            forecast = 0\n",
    "            for block in self.blocks:\n",
    "                forecast = forecast + block(x_flat)\n",
    "            out = forecast\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type\")\n",
    "        return out\n",
    "\n",
    "###############################################################\n",
    "# Early Stopping class\n",
    "###############################################################\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=150, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "###############################################################\n",
    "# 7. Inverse transform helper\n",
    "###############################################################\n",
    "def inverse_transform_4cols(y_4):\n",
    "    return scaler.inverse_transform(y_4)\n",
    "\n",
    "###############################################################\n",
    "# 8. Training function for PyTorch models\n",
    "###############################################################\n",
    "def train_model(model_type=\"LSTM\", num_epochs=NUM_EPOCHS):\n",
    "    model = BaseModel(model_type, window_width=WINDOW_SIZE).to(device)\n",
    "    best_path = f\"best_{model_type}_Silver_V1.pt\"\n",
    "    \n",
    "    # If the best model file exists, load and return the model immediately.\n",
    "    if os.path.exists(best_path):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "            model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "        print(f\"[{model_type}] Loaded existing model from {best_path}\")\n",
    "        return model\n",
    "\n",
    "    # Otherwise, train the model.\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0005)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=150)\n",
    "    early_stopping = EarlyStopping(patience=150)\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for Xb, yb in train_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(Xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        val_loss = half_blind_validation_loss(model, df_scaled, val_start=train_cutoff, val_end=val_cutoff, window=WINDOW_SIZE)\n",
    "        scheduler.step(val_loss)\n",
    "        early_stopping(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"[{model_type}] Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"[{model_type}] Epoch {epoch+1}, Train={avg_train_loss:.6f}, Val={val_loss:.6f}\")\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    return model\n",
    "\n",
    "###############################################################\n",
    "# 8B. Training function for sklearn-based models (SVM, GPR, Boost)\n",
    "###############################################################\n",
    "class SklearnWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        # x: [1, window, 4] tensor -> flatten to [1, window*4]\n",
    "        x_np = x.cpu().detach().numpy().reshape(1, -1)\n",
    "        pred = self.model.predict(x_np)  # shape: (1,4)\n",
    "        return torch.from_numpy(pred).float().to(x.device)\n",
    "    def eval(self):\n",
    "        pass\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "def train_model_sklearn(model_type, X_train_flat, y_train):\n",
    "    best_path = f\"best_{model_type}_Silver_V1.pkl\"\n",
    "    if os.path.exists(best_path):\n",
    "        model = joblib.load(best_path)\n",
    "        print(f\"[{model_type}] Loaded existing model from {best_path}\")\n",
    "    else:\n",
    "        if model_type == \"SVM\":\n",
    "            base_model = SVR(kernel='rbf', C=10.0, epsilon=0.01)\n",
    "        elif model_type == \"GPR\":\n",
    "            from sklearn.gaussian_process.kernels import RBF\n",
    "            kernel = RBF(length_scale=1.0)\n",
    "            base_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-2)\n",
    "        elif model_type == \"Boost\":\n",
    "            base_model = GradientBoostingRegressor(n_estimators=200, max_depth=3)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported sklearn model type\")\n",
    "        multi_model = MultiOutputRegressor(base_model)\n",
    "        multi_model.fit(X_train_flat, y_train)\n",
    "        joblib.dump(multi_model, best_path)\n",
    "        model = multi_model\n",
    "    return SklearnWrapper(model)\n",
    "\n",
    "###############################################################\n",
    "# 9. Validation predictions + Test predictions\n",
    "###############################################################\n",
    "def walk_forward_test(model, df_all_scaled, test_start, window=WINDOW_SIZE):\n",
    "    df_work = df_all_scaled.copy()\n",
    "    df_work.sort_values(\"Date\", inplace=True)\n",
    "    preds = []\n",
    "    test_dates = np.sort(df_work[df_work[\"Date\"] >= test_start][\"Date\"].unique())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for d in test_dates:\n",
    "            idx_current = df_work.index[df_work[\"Date\"] == d]\n",
    "            if len(idx_current) == 0:\n",
    "                continue\n",
    "            idx_current = idx_current[0]\n",
    "            start_idx = idx_current - window\n",
    "            if start_idx < 0:\n",
    "                continue\n",
    "            window_df = df_work.iloc[start_idx:idx_current][features]\n",
    "            window_df = window_df.apply(pd.to_numeric, errors=\"coerce\").astype(float)\n",
    "            window_data = window_df.values\n",
    "            if np.isnan(window_data).any():\n",
    "                continue\n",
    "            X_input = torch.from_numpy(window_data).float().unsqueeze(0).to(device)\n",
    "            y_pred_4_scaled = model(X_input).cpu().numpy()[0]\n",
    "            df_work.loc[idx_current, features] = y_pred_4_scaled\n",
    "            preds.append((d, *y_pred_4_scaled))\n",
    "    pred_df = pd.DataFrame(\n",
    "        preds,\n",
    "        columns=[\"Date\", \"Pred_Price_scaled\", \"Pred_Open_scaled\", \"Pred_High_scaled\", \"Pred_Low_scaled\"]\n",
    "    )\n",
    "    pred_df.sort_values(\"Date\", inplace=True)\n",
    "    scl_array = pred_df[[\"Pred_Price_scaled\", \"Pred_Open_scaled\", \"Pred_High_scaled\", \"Pred_Low_scaled\"]].values\n",
    "    unsc_array = inverse_transform_4cols(scl_array)\n",
    "    pred_df[\"Pred_Price_unscaled\"] = unsc_array[:, 0]\n",
    "    pred_df[\"Pred_Open_unscaled\"]  = unsc_array[:, 1]\n",
    "    pred_df[\"Pred_High_unscaled\"]  = unsc_array[:, 2]\n",
    "    pred_df[\"Pred_Low_unscaled\"]   = unsc_array[:, 3]\n",
    "    return pred_df\n",
    "\n",
    "###############################################################\n",
    "# 10. Evaluate & Plot\n",
    "###############################################################\n",
    "def evaluate_and_plot_all_models(results_dict):\n",
    "    # Get the actual (unscaled) train+val data.\n",
    "    df_real = df_scaled[[\"Date\", \"Price\", \"Open\", \"High\", \"Low\"]].copy()\n",
    "    df_real.sort_values(\"Date\", inplace=True)\n",
    "    mask_trainval = (df_real[\"Date\"] < test_cutoff)\n",
    "    df_real_plot = df_real.loc[mask_trainval].copy()\n",
    "    real_4_scaled = df_real_plot[features].values\n",
    "    real_4_unscaled = inverse_transform_4cols(real_4_scaled)\n",
    "    df_real_plot[\"Real_Price_unscaled\"] = real_4_unscaled[:, 0]\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Plot 1: Joint Diagram: Actual Price vs. Model Predictions\n",
    "    # -------------------------------\n",
    "    plt.figure(figsize=(14,7))\n",
    "    plt.plot(df_real_plot[\"Date\"], df_real_plot[\"Real_Price_unscaled\"],\n",
    "             color=\"black\", label=\"Actual Price (Train+Val)\")\n",
    "    \n",
    "    color_map = {\n",
    "        \"LSTM\": \"red\",\n",
    "        \"GRU\": \"blue\",\n",
    "        \"RNN\": \"green\",\n",
    "        \"CNN\": \"orange\",\n",
    "        \"EnhancedLSTM\": \"magenta\",\n",
    "        \"Transformer\": \"cyan\",\n",
    "        \"N-BEATS\": \"brown\",\n",
    "        \"N-HITS\": \"pink\",\n",
    "        \"SVM\": \"olive\",\n",
    "        \"GPR\": \"teal\",\n",
    "        \"Boost\": \"purple\"\n",
    "    }\n",
    "    \n",
    "    # Merge predictions from all models.\n",
    "    merged_df = df_real.copy()\n",
    "    merged_df.sort_values(\"Date\", inplace=True)\n",
    "    \n",
    "    for model_name, info in results_dict.items():\n",
    "        val_pred_df = info[\"val_pred_df\"]\n",
    "        test_pred_df = info[\"test_pred_df\"]\n",
    "        frames = []\n",
    "        if val_pred_df is not None:\n",
    "            frames.append(val_pred_df.rename(columns={\"Pred_Price_unscaled\": f\"Pred_{model_name}\"}))\n",
    "        if test_pred_df is not None:\n",
    "            frames.append(test_pred_df.rename(columns={\"Pred_Price_unscaled\": f\"Pred_{model_name}\"}))\n",
    "        if not frames:\n",
    "            continue\n",
    "        combined_df = pd.concat(frames, ignore_index=True)\n",
    "        combined_df = combined_df[[\"Date\", f\"Pred_{model_name}\"]].copy()\n",
    "        combined_df.drop_duplicates(\"Date\", keep=\"last\", inplace=True)\n",
    "        combined_df.sort_values(\"Date\", inplace=True)\n",
    "        merged_df = pd.merge(merged_df, combined_df, on=\"Date\", how=\"left\")\n",
    "    \n",
    "    # Plot model predictions.\n",
    "    for model_name, info in results_dict.items():\n",
    "        col = f\"Pred_{model_name}\"\n",
    "        if col in merged_df.columns:\n",
    "            plt.plot(merged_df[\"Date\"], merged_df[col],\n",
    "                     color=color_map.get(model_name, \"gray\"),\n",
    "                     linestyle=\"-\",\n",
    "                     label=f\"{model_name} Prediction\")\n",
    "    \n",
    "    plt.axvspan(df_train[\"Date\"].min(), train_cutoff, color=\"skyblue\", alpha=0.1, label=\"Train\")\n",
    "    plt.axvspan(train_cutoff, val_cutoff, color=\"green\", alpha=0.1, label=\"Validation\")\n",
    "    plt.axvspan(test_cutoff, merged_df[\"Date\"].max(), color=\"yellow\", alpha=0.1, label=\"Test\")\n",
    "    \n",
    "    plt.title(\"Joint Diagram: Actual Price vs. Model Predictions of Silver's Stock\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price (unscaled)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Plot 2: Test Period Zoom-in (with Actual Test Prices)\n",
    "    # -------------------------------\n",
    "    df_test_zoom = merged_df[merged_df[\"Date\"] >= test_cutoff]\n",
    "    if not df_test_zoom.empty:\n",
    "        plt.figure(figsize=(12,5))\n",
    "        for model_name, info in results_dict.items():\n",
    "            col = f\"Pred_{model_name}\"\n",
    "            if col in df_test_zoom.columns:\n",
    "                plt.plot(df_test_zoom[\"Date\"], df_test_zoom[col],\n",
    "                         color=color_map.get(model_name, \"gray\"),\n",
    "                         linestyle=\"--\",\n",
    "                         label=f\"{model_name} Prediction\")\n",
    "        # Load actual test prices from the complete CSV.\n",
    "        df_complete = pd.read_csv(\"Silver Futures Historical Data_Complete.csv\")\n",
    "        df_complete.drop(columns=[\"Vol.\", \"Change %\"], errors=\"ignore\", inplace=True)\n",
    "        df_complete['Date'] = pd.to_datetime(df_complete['Date'])\n",
    "        df_complete.sort_values(\"Date\", inplace=True)\n",
    "        df_test_actual = df_complete[df_complete[\"Date\"] >= test_cutoff].copy()\n",
    "        # Plot actual test price (assumed unscaled).\n",
    "        plt.plot(df_test_actual[\"Date\"], df_test_actual[\"Price\"],\n",
    "                 color='black', linewidth=2, label='Actual Test Price')\n",
    "        \n",
    "        plt.title(\"Test Period Zoom-in on Silver's Stock\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Price (unscaled)\")\n",
    "        plt.legend(prop={'size':8})\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "###############################################################\n",
    "# 11. MAIN\n",
    "###############################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # List of all models to run (both PyTorch and sklearn-based)\n",
    "    all_model_types = [\"LSTM\", \"GRU\", \"RNN\", \"CNN\", \"Transformer\", \"N-BEATS\", \"N-HITS\", \"SVM\", \"Boost\"]\n",
    "    results_dict = {}\n",
    "    \n",
    "    for mt in all_model_types:\n",
    "        print(f\"\\n=== Training {mt} model ===\")\n",
    "        if mt in [\"SVM\", \"GPR\", \"Boost\"]:\n",
    "            model = train_model_sklearn(mt, X_train_flat, y_train_all)\n",
    "        else:\n",
    "            model = train_model(mt, num_epochs=NUM_EPOCHS)\n",
    "        \n",
    "        # For models like SVM, Transformer, and Boost, use iterative (next-day) validation:\n",
    "        if mt in [\"SVM\", \"Transformer\", \"Boost\"]:\n",
    "            val_pred_df = iterative_half_blind_validation_preds(\n",
    "                model,\n",
    "                df_scaled,\n",
    "                val_start=train_cutoff,  # start validation right after training\n",
    "                val_end=val_cutoff,\n",
    "                window=WINDOW_SIZE\n",
    "            )\n",
    "        else:\n",
    "            val_pred_df = half_blind_validation_preds_df(\n",
    "                model,\n",
    "                df_scaled,\n",
    "                val_start=train_cutoff,\n",
    "                val_end=val_cutoff,\n",
    "                window=WINDOW_SIZE\n",
    "            )\n",
    "        \n",
    "        test_pred_df = walk_forward_test(\n",
    "            model,\n",
    "            df_all_scaled=df_scaled,\n",
    "            test_start=test_cutoff,\n",
    "            window=WINDOW_SIZE\n",
    "        )\n",
    "        \n",
    "        results_dict[mt] = {\n",
    "            \"val_pred_df\": val_pred_df,\n",
    "            \"test_pred_df\": test_pred_df\n",
    "        }\n",
    "        print(f\"{mt} has no real test data => skip MAPE\")\n",
    "    \n",
    "    evaluate_and_plot_all_models(results_dict)\n",
    "\n",
    "# ================================\n",
    "# Additional Plot: Joint Diagram of Actual vs. Predicted Prices\n",
    "# ================================\n",
    "\n",
    "# Load and prepare actual price data from another CSV\n",
    "df_actual = pd.read_csv(\"Silver Futures Historical Data_Complete.csv\")\n",
    "df_actual.drop(columns=[\"Vol.\", \"Change %\"], errors=\"ignore\", inplace=True)\n",
    "df_actual['Date'] = pd.to_datetime(df_actual['Date'])\n",
    "df_actual.sort_values(by='Date', ascending=True, inplace=True)\n",
    "df_actual.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Clean numerical columns: remove commas and convert to float\n",
    "num_cols = df_actual.columns.drop(\"Date\")\n",
    "df_actual[num_cols] = df_actual[num_cols].replace({',': ''}, regex=True)\n",
    "df_actual[num_cols] = df_actual[num_cols].astype('float64')\n",
    "\n",
    "# Extract the actual price curve\n",
    "actual_dates = df_actual['Date']\n",
    "actual_prices = df_actual['Price']\n",
    "\n",
    "# Prepare model prediction data from results_dict\n",
    "# (Ensure each test_pred_df has its Date column as datetime)\n",
    "for model_name, pred_info in results_dict.items():\n",
    "    if pred_info[\"test_pred_df\"] is not None and not pred_info[\"test_pred_df\"].empty:\n",
    "        pred_info[\"test_pred_df\"]['Date'] = pd.to_datetime(pred_info[\"test_pred_df\"]['Date'])\n",
    "\n",
    "# Plot the joint diagram\n",
    "plt.figure(figsize=(14, 7))\n",
    "# Plot actual prices (from the complete CSV)\n",
    "plt.plot(actual_dates, actual_prices, color='black', linewidth=1.5, label='Actual Price')\n",
    "\n",
    "# Define custom colors for each model prediction\n",
    "model_colors = {\n",
    "    \"LSTM\": \"red\",\n",
    "    \"GRU\": \"blue\",\n",
    "    \"RNN\": \"green\",\n",
    "    \"CNN\": \"orange\",\n",
    "    \"Transformer\": \"cyan\",\n",
    "    \"N-BEATS\": \"brown\",\n",
    "    \"N-HITS\": \"pink\",\n",
    "    \"SVM\": \"olive\",\n",
    "    \"Boost\": \"purple\"\n",
    "}\n",
    "\n",
    "# Overlay each model's test predictions\n",
    "for model_name, pred_info in results_dict.items():\n",
    "    test_df = pred_info[\"test_pred_df\"]\n",
    "    if test_df is not None and not test_df.empty:\n",
    "        plt.plot(pd.to_datetime(test_df[\"Date\"]), test_df[\"Pred_Price_unscaled\"],\n",
    "                 color=model_colors.get(model_name, \"gray\"),\n",
    "                 linestyle=\"--\", linewidth=1.5,\n",
    "                 label=f'{model_name} Prediction')\n",
    "\n",
    "# Optionally, highlight the test period (adjust the start date as needed)\n",
    "test_period_start = pd.to_datetime(\"2025-01-02\")\n",
    "plt.axvspan(test_period_start, actual_dates.iloc[-1], color='yellow', alpha=0.1, label='Test Period')\n",
    "plt.axvspan(df_train[\"Date\"].min(), train_cutoff, color=\"skyblue\", alpha=0.1, label=\"Train\")\n",
    "plt.axvspan(train_cutoff, val_cutoff, color=\"green\", alpha=0.1, label=\"Validation\")\n",
    "\n",
    "plt.title(\"Joint Diagram: Actual Price vs. Model Predictions of Silver's Future Prices\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Additional Cell: Groundtruth Comparison & Cumulative Absolute Error Calculation\n",
    "\n",
    "# %% [code]\n",
    "# Load groundtruth actual test prices from the complete CSV\n",
    "df_groundtruth = pd.read_csv(\"Silver Futures Historical Data_Complete.csv\")\n",
    "df_groundtruth.drop(columns=[\"Vol.\", \"Change %\"], errors=\"ignore\", inplace=True)\n",
    "df_groundtruth['Date'] = pd.to_datetime(df_groundtruth['Date'])\n",
    "df_groundtruth.sort_values(\"Date\", inplace=True)\n",
    "df_groundtruth.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter the data to only include the test period (dates >= test_cutoff)\n",
    "df_groundtruth_test = df_groundtruth[df_groundtruth[\"Date\"] >= test_cutoff][[\"Date\", \"Price\"]].copy()\n",
    "\n",
    "# Dictionary to store cumulative absolute errors for each model\n",
    "cumulative_errors = {}\n",
    "\n",
    "# Iterate through each model's test predictions in the results_dict\n",
    "for model_name, info in results_dict.items():\n",
    "    test_pred_df = info.get(\"test_pred_df\")\n",
    "    if test_pred_df is not None and not test_pred_df.empty:\n",
    "        # Ensure the Date column is datetime format\n",
    "        test_pred_df[\"Date\"] = pd.to_datetime(test_pred_df[\"Date\"])\n",
    "        # Merge the model's predictions with the groundtruth data on Date\n",
    "        df_merge = pd.merge(test_pred_df, df_groundtruth_test, on=\"Date\", how=\"inner\")\n",
    "        if not df_merge.empty:\n",
    "            # Calculate the absolute error between the predicted unscaled price and the actual price\n",
    "            df_merge[\"Abs_Error\"] = np.abs(df_merge[\"Pred_Price_unscaled\"] - df_merge[\"Price\"])\n",
    "            # Sum the absolute errors over the test period to get cumulative error\n",
    "            cumulative_error = df_merge[\"Abs_Error\"].sum()\n",
    "            cumulative_errors[model_name] = cumulative_error\n",
    "            print(f\"Model: {model_name} - Cumulative Absolute Error: {cumulative_error:.4f}\")\n",
    "        else:\n",
    "            print(f\"Model: {model_name} - No matching dates with groundtruth for the test period.\")\n",
    "    else:\n",
    "        print(f\"Model: {model_name} - No test predictions available.\")\n",
    "\n",
    "# Plot the cumulative absolute errors as a bar chart\n",
    "if cumulative_errors:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    model_names = list(cumulative_errors.keys())\n",
    "    errors = [cumulative_errors[m] for m in model_names]\n",
    "    plt.bar(model_names, errors, color='skyblue')\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"Cumulative Absolute Error\")\n",
    "    plt.title(\"Cumulative Absolute Error of Model Predictions vs. Groundtruth (Test Period)\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Additional Cell: Daily Difference Diagram (Predicted - Actual)\n",
    "\n",
    "# %% [code]\n",
    "# Ensure groundtruth test data is available\n",
    "if 'df_groundtruth_test' not in globals():\n",
    "    df_groundtruth = pd.read_csv(\"Silver Futures Historical Data_Complete.csv\")\n",
    "    df_groundtruth.drop(columns=[\"Vol.\", \"Change %\"], errors=\"ignore\", inplace=True)\n",
    "    df_groundtruth['Date'] = pd.to_datetime(df_groundtruth['Date'])\n",
    "    df_groundtruth.sort_values(\"Date\", inplace=True)\n",
    "    df_groundtruth.reset_index(drop=True, inplace=True)\n",
    "    df_groundtruth_test = df_groundtruth[df_groundtruth[\"Date\"] >= test_cutoff][[\"Date\", \"Price\"]].copy()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Use a consistent color mapping for clarity\n",
    "model_colors = {\n",
    "    \"LSTM\": \"red\",\n",
    "    \"GRU\": \"blue\",\n",
    "    \"RNN\": \"green\",\n",
    "    \"CNN\": \"orange\",\n",
    "    \"Transformer\": \"cyan\",\n",
    "    \"N-BEATS\": \"brown\",\n",
    "    \"N-HITS\": \"pink\",\n",
    "    \"SVM\": \"olive\",\n",
    "    \"Boost\": \"purple\"\n",
    "}\n",
    "\n",
    "# Iterate through each model in results_dict\n",
    "for model_name, info in results_dict.items():\n",
    "    test_pred_df = info.get(\"test_pred_df\")\n",
    "    if test_pred_df is None or test_pred_df.empty:\n",
    "        continue\n",
    "    # Ensure Date column is datetime format\n",
    "    test_pred_df[\"Date\"] = pd.to_datetime(test_pred_df[\"Date\"])\n",
    "    # Merge with groundtruth based on Date\n",
    "    df_merge = pd.merge(test_pred_df, df_groundtruth_test, on=\"Date\", how=\"inner\")\n",
    "    if df_merge.empty:\n",
    "        continue\n",
    "    df_merge.sort_values(\"Date\", inplace=True)\n",
    "    # Calculate daily difference: predicted unscaled price minus actual price\n",
    "    df_merge[\"Difference\"] = df_merge[\"Pred_Price_unscaled\"] - df_merge[\"Price\"]\n",
    "    color = model_colors.get(model_name, None)\n",
    "    plt.plot(df_merge[\"Date\"], df_merge[\"Difference\"], label=model_name, color=color)\n",
    "\n",
    "# Draw a horizontal line at zero difference\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=1, label=\"Zero Difference\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Daily Difference (Predicted - Actual)\")\n",
    "plt.title(\"Daily Differences between Model Predictions and Actual Prices\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Additional Cell: Model Selection Based on Trend Matching & Confidence Score\n",
    "\n",
    "# %% [code]\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load groundtruth test data (if not already loaded)\n",
    "df_groundtruth = pd.read_csv(\"Silver Futures Historical Data_Complete.csv\")\n",
    "df_groundtruth.drop(columns=[\"Vol.\", \"Change %\"], errors=\"ignore\", inplace=True)\n",
    "df_groundtruth['Date'] = pd.to_datetime(df_groundtruth['Date'])\n",
    "df_groundtruth.sort_values(\"Date\", inplace=True)\n",
    "df_groundtruth.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter to test period (dates >= test_cutoff)\n",
    "df_groundtruth_test = df_groundtruth[df_groundtruth[\"Date\"] >= test_cutoff][[\"Date\", \"Price\"]].copy()\n",
    "\n",
    "# Dictionary to store metrics for each model\n",
    "model_metrics = {}\n",
    "\n",
    "# Iterate through each model in results_dict\n",
    "for model_name, info in results_dict.items():\n",
    "    test_pred_df = info.get(\"test_pred_df\")\n",
    "    if test_pred_df is None or test_pred_df.empty:\n",
    "        print(f\"Model: {model_name} - No test predictions available.\")\n",
    "        continue\n",
    "    # Ensure Date column is datetime type\n",
    "    test_pred_df[\"Date\"] = pd.to_datetime(test_pred_df[\"Date\"])\n",
    "    # Merge model predictions with groundtruth data on Date\n",
    "    df_merge = pd.merge(test_pred_df, df_groundtruth_test, on=\"Date\", how=\"inner\")\n",
    "    if df_merge.empty:\n",
    "        print(f\"Model: {model_name} - No matching dates with groundtruth for the test period.\")\n",
    "        continue\n",
    "    # Sort by Date to ensure proper chronological order\n",
    "    df_merge.sort_values(\"Date\", inplace=True)\n",
    "    \n",
    "    # --- Metric 1: Cumulative Absolute Error ---\n",
    "    df_merge[\"Abs_Error\"] = np.abs(df_merge[\"Pred_Price_unscaled\"] - df_merge[\"Price\"])\n",
    "    cumulative_error = df_merge[\"Abs_Error\"].sum()\n",
    "    \n",
    "    # --- Metric 2: Trend Matching Rate ---\n",
    "    # Calculate day-to-day differences for actual and predicted prices\n",
    "    df_merge[\"Actual_Diff\"] = df_merge[\"Price\"].diff()\n",
    "    df_merge[\"Pred_Diff\"] = df_merge[\"Pred_Price_unscaled\"].diff()\n",
    "    # Exclude the first day (NaN in diff)\n",
    "    df_valid = df_merge.dropna(subset=[\"Actual_Diff\", \"Pred_Diff\"]).copy()\n",
    "    \n",
    "    # Define a helper to determine if the trend signs match.\n",
    "    # We treat a day as matching if both differences have the same sign,\n",
    "    # or if both are zero.\n",
    "    def sign_match(a, b):\n",
    "        if a == 0 and b == 0:\n",
    "            return 1\n",
    "        return 1 if np.sign(a) == np.sign(b) else 0\n",
    "    \n",
    "    df_valid[\"Trend_Match\"] = df_valid.apply(lambda row: sign_match(row[\"Actual_Diff\"], row[\"Pred_Diff\"]), axis=1)\n",
    "    if len(df_valid) > 0:\n",
    "        trend_matching_rate = df_valid[\"Trend_Match\"].mean()  # fraction of days matching trend\n",
    "    else:\n",
    "        trend_matching_rate = 0.0\n",
    "    \n",
    "    # Store the computed metrics for this model\n",
    "    model_metrics[model_name] = {\n",
    "        \"cumulative_error\": cumulative_error,\n",
    "        \"trend_matching_rate\": trend_matching_rate\n",
    "    }\n",
    "\n",
    "# --- Normalize the Cumulative Error ---\n",
    "# Lower cumulative error is better; so we convert errors into a normalized score (0 to 1),\n",
    "# where 1 indicates the best (lowest error) among the models.\n",
    "if model_metrics:\n",
    "    cum_errors = [metrics[\"cumulative_error\"] for metrics in model_metrics.values()]\n",
    "    min_error = min(cum_errors)\n",
    "    max_error = max(cum_errors)\n",
    "    for model_name, metrics in model_metrics.items():\n",
    "        if max_error > min_error:\n",
    "            normalized_error_score = 1 - ((metrics[\"cumulative_error\"] - min_error) / (max_error - min_error))\n",
    "        else:\n",
    "            normalized_error_score = 1  # all models have identical errors\n",
    "        model_metrics[model_name][\"normalized_error_score\"] = normalized_error_score\n",
    "        \n",
    "        # --- Combined Confidence Score ---\n",
    "        # We weight trend matching at 70% and normalized error at 30%.\n",
    "        combined_confidence = 0.7 * metrics[\"trend_matching_rate\"] + 0.3 * normalized_error_score\n",
    "        model_metrics[model_name][\"combined_confidence\"] = combined_confidence\n",
    "\n",
    "# Convert the metrics dictionary into a DataFrame for easier viewing\n",
    "df_metrics = pd.DataFrame.from_dict(model_metrics, orient=\"index\")\n",
    "df_metrics.reset_index(inplace=True)\n",
    "df_metrics.rename(columns={\"index\": \"Model\"}, inplace=True)\n",
    "\n",
    "#print(\"Model Metrics for Trend Matching & Confidence:\")\n",
    "#print(df_metrics)\n",
    "\n",
    "# --- Select Top 5 Models Based on Combined Confidence ---\n",
    "top5_models = df_metrics.sort_values(\"combined_confidence\", ascending=False).head(5)\n",
    "#print(\"\\nTop 5 Selected Models:\")\n",
    "#print(top5_models)\n",
    "\n",
    "# --- Plot Combined Confidence Scores ---\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(df_metrics[\"Model\"], df_metrics[\"combined_confidence\"], color='skyblue')\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Combined Confidence Score\")\n",
    "plt.title(\"Combined Confidence Scores for All Models\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Evidence Summary ---\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Additional Cell: Re-normalize Confidence Scores for Selected Models\n",
    "\n",
    "# %% [code]\n",
    "# Assuming 'top5_models' DataFrame is already computed from the previous cell.\n",
    "if not top5_models.empty:\n",
    "    # Re-normalize the combined confidence scores so the sum equals 1.\n",
    "    total_confidence = top5_models[\"combined_confidence\"].sum()\n",
    "    top5_models[\"normalized_confidence\"] = top5_models[\"combined_confidence\"] / total_confidence\n",
    "    print(\"Normalized Confidence Scores for Top 5 Models (Sum = 1):\")\n",
    "    print(top5_models[[\"Model\", \"normalized_confidence\"]])\n",
    "    \n",
    "    # Prepare a dictionary for later use (e.g., in polynomial fitting routines)\n",
    "    selected_models_confidence = dict(zip(top5_models[\"Model\"], top5_models[\"normalized_confidence\"]))\n",
    "    print(\"\\nSelected models and their normalized confidence scores (ready for later polynomial fitting):\")\n",
    "    print(selected_models_confidence)\n",
    "else:\n",
    "    print(\"No top models available to normalize confidence scores.\")\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore all warning messages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# -------------------------------\n",
    "# Preliminary Setup\n",
    "# -------------------------------\n",
    "# Define test_cutoff if not already defined\n",
    "try:\n",
    "    test_cutoff\n",
    "except NameError:\n",
    "    test_cutoff = pd.to_datetime(\"2025-01-02\")\n",
    "\n",
    "# Load complete CSV and define df_test_actual if not already defined\n",
    "try:\n",
    "    df_test_actual\n",
    "except NameError:\n",
    "    df_complete = pd.read_csv(\"Silver Futures Historical Data_Complete.csv\")\n",
    "    df_complete.drop(columns=[\"Vol.\", \"Change %\"], errors=\"ignore\", inplace=True)\n",
    "    df_complete['Date'] = pd.to_datetime(df_complete['Date'])\n",
    "    df_complete.sort_values(\"Date\", inplace=True)\n",
    "    df_complete.reset_index(drop=True, inplace=True)\n",
    "    # Filter to test period (dates >= test_cutoff)\n",
    "    df_test_actual = df_complete[df_complete[\"Date\"] >= test_cutoff].copy()\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Gather Test Data: Actual vs. Top-5 Model Predictions\n",
    "# -------------------------------\n",
    "# Create a DataFrame from the groundtruth test data\n",
    "df_test_poly = df_test_actual[[\"Date\", \"Price\"]].rename(columns={\"Price\": \"Actual\"}).copy()\n",
    "df_test_poly.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "# Use the best models from your prior selection.\n",
    "# If you previously computed normalized confidences and stored them in selected_models_confidence,\n",
    "# then we can use its keys. Otherwise, fall back to all models in results_dict.\n",
    "try:\n",
    "    best_model_names = list(selected_models_confidence.keys())\n",
    "except NameError:\n",
    "    best_model_names = list(results_dict.keys()) if \"results_dict\" in globals() else []\n",
    "\n",
    "# Merge each model's predictions (assumed to be in the \"Pred_Price_unscaled\" column) into df_test_poly.\n",
    "for model_name in best_model_names:\n",
    "    if \"results_dict\" in globals():\n",
    "        pred_df = results_dict[model_name][\"test_pred_df\"]\n",
    "    else:\n",
    "        pred_df = None\n",
    "    if pred_df is not None and not pred_df.empty:\n",
    "        df_test_poly = pd.merge(\n",
    "            df_test_poly,\n",
    "            pred_df[[\"Date\", \"Pred_Price_unscaled\"]],\n",
    "            on=\"Date\", how=\"left\"\n",
    "        ).rename(columns={\"Pred_Price_unscaled\": model_name})\n",
    "\n",
    "# Drop rows with missing data in any column\n",
    "df_test_poly.dropna(axis=0, how=\"any\", inplace=True)\n",
    "df_test_poly.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Polynomial-fitting test DataFrame columns:\", df_test_poly.columns.tolist())\n",
    "print(\"Number of rows in polynomial-fitting test DataFrame:\", len(df_test_poly))\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Create Train/Validation Masks (Every Second Day for Validation)\n",
    "# -------------------------------\n",
    "n_points = len(df_test_poly)\n",
    "indices = np.arange(n_points)\n",
    "# Use even-indexed rows for training and odd-indexed rows for validation.\n",
    "training_mask = (indices % 2 == 0)\n",
    "validation_mask = ~training_mask\n",
    "\n",
    "# Convert dates to a numeric axis for polynomial fitting.\n",
    "df_test_poly[\"mdates_num\"] = mdates.date2num(df_test_poly[\"Date\"])\n",
    "\n",
    "# -------------------------------\n",
    "# 3) For Each Column, Select Best Polynomial Degree (1 to 35)\n",
    "# -------------------------------\n",
    "def format_polynomial(coeffs):\n",
    "    \"\"\"\n",
    "    Convert polynomial coefficients (highest degree first)\n",
    "    into a readable expression.\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    degree = len(coeffs) - 1\n",
    "    for i, c in enumerate(coeffs):\n",
    "        power = degree - i\n",
    "        if power > 1:\n",
    "            terms.append(f\"{c:+.4e}*x^{power}\")\n",
    "        elif power == 1:\n",
    "            terms.append(f\"{c:+.4e}*x\")\n",
    "        else:\n",
    "            terms.append(f\"{c:+.4e}\")\n",
    "    expr = \"\".join(terms).replace(\"+-\", \"-\")\n",
    "    if expr.startswith(\"+\"):\n",
    "        expr = expr[1:]\n",
    "    return expr\n",
    "\n",
    "# Dictionary to store best-fit info for each column.\n",
    "best_poly_info = {}\n",
    "# Consider polynomial degrees from 1 to 65.\n",
    "poly_degree_candidates = range(1, 65)\n",
    "# Columns to fit: \"Actual\" and each model's column.\n",
    "columns_to_fit = [\"Actual\"] + best_model_names\n",
    "\n",
    "for col in columns_to_fit:\n",
    "    x_train = df_test_poly.loc[training_mask, \"mdates_num\"].values\n",
    "    y_train = df_test_poly.loc[training_mask, col].values\n",
    "    x_val = df_test_poly.loc[validation_mask, \"mdates_num\"].values\n",
    "    y_val = df_test_poly.loc[validation_mask, col].values\n",
    "    \n",
    "    best_val_mse = float(\"inf\")\n",
    "    best_degree = None\n",
    "    best_coeffs = None\n",
    "    for d in poly_degree_candidates:\n",
    "        coeffs = np.polyfit(x_train, y_train, d)\n",
    "        y_val_pred = np.polyval(coeffs, x_val)\n",
    "        val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "        if val_mse < best_val_mse:\n",
    "            best_val_mse = val_mse\n",
    "            best_degree = d\n",
    "            best_coeffs = coeffs\n",
    "    best_poly_info[col] = {\"degree\": best_degree, \"coeffs\": best_coeffs, \"val_mse\": best_val_mse}\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Print Each Column's Best Polynomial Information\n",
    "# -------------------------------\n",
    "print(\"\\n=== Best Polynomial Orders (Test Window) ===\")\n",
    "for col in columns_to_fit:\n",
    "    info = best_poly_info[col]\n",
    "    deg = info[\"degree\"]\n",
    "    mse = info[\"val_mse\"]\n",
    "    coeffs = info[\"coeffs\"]\n",
    "    expr = format_polynomial(coeffs)\n",
    "    print(f\"[{col}] -> Best Degree: {deg}, Val MSE = {mse:.6f}\")\n",
    "    print(f\"     Polynomial Expression: y = {expr}\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5) Joint Plot: Overlay Best-Fitting Polynomial Curves for All Columns\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Plot the actual datapoints with classification:\n",
    "plt.scatter(df_test_poly.loc[training_mask, \"Date\"], df_test_poly.loc[training_mask, \"Actual\"],\n",
    "            color=\"black\", marker=\"o\", s=50, label=\"Actual Training Data\", zorder=5)\n",
    "plt.scatter(df_test_poly.loc[validation_mask, \"Date\"], df_test_poly.loc[validation_mask, \"Actual\"],\n",
    "            color=\"gray\", marker=\"x\", s=50, label=\"Actual Validation Data\", zorder=5)\n",
    "\n",
    "# Define colors for polynomial curves (for Actual and each model).\n",
    "colors = {\"Actual\": \"black\"}\n",
    "default_colors = [\"purple\", \"olive\", \"blue\", \"brown\", \"red\"]\n",
    "for i, model_name in enumerate(best_model_names):\n",
    "    colors[model_name] = default_colors[i % len(default_colors)]\n",
    "\n",
    "# Create a dense x-axis for plotting the polynomial curves.\n",
    "x_dense = np.linspace(df_test_poly[\"mdates_num\"].min(), df_test_poly[\"mdates_num\"].max(), 300)\n",
    "x_dense_dates = [mdates.num2date(xx) for xx in x_dense]\n",
    "\n",
    "# For each column, evaluate its best-fit polynomial and plot it.\n",
    "for col in columns_to_fit:\n",
    "    info = best_poly_info[col]\n",
    "    coeffs = info[\"coeffs\"]\n",
    "    y_dense = np.polyval(coeffs, x_dense)\n",
    "    plt.plot(x_dense_dates, y_dense, color=colors[col], linewidth=2,\n",
    "             label=f\"{col} Poly (Deg {info['degree']})\")\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Best Polynomial Fits (Test Window): Actual vs. Model Predictions of Silver Futures\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 6) (Optional) Joint Plot of Derivative Curves & Print Their Expressions\n",
    "# -------------------------------\n",
    "def derivative_expression(coeffs):\n",
    "    d_coeffs = np.polyder(coeffs)\n",
    "    return format_polynomial(d_coeffs)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for col in columns_to_fit:\n",
    "    info = best_poly_info[col]\n",
    "    coeffs = info[\"coeffs\"]\n",
    "    d_coeffs = np.polyder(coeffs)\n",
    "    d_coeffs = np.polyder(coeffs)\n",
    "    y_deriv = np.polyval(d_coeffs, x_dense)\n",
    "    plt.plot(x_dense_dates, y_deriv, color=colors[col], linewidth=2,\n",
    "             label=f\"{col} Deriv (Deg {info['degree']-1})\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"d(Price)/d(Time)\")\n",
    "plt.title(\"Polynomial Derivative Curves of Siver Futures (Test Window)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Derivative Polynomial Expressions ===\")\n",
    "for col in columns_to_fit:\n",
    "    d_expr = derivative_expression(best_poly_info[col][\"coeffs\"])\n",
    "    print(f\"[{col}] (Degree {best_poly_info[col]['degree']}) -> derivative: {d_expr}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
